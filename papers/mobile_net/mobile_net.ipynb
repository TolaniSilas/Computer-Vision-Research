{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import the neccessary libraries.\nimport os\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as nn_func\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import v2 as transforms_v2\nfrom torchvision import datasets\nimport torch.optim as optim\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-26T23:17:42.214236Z","iopub.execute_input":"2024-08-26T23:17:42.214622Z","iopub.status.idle":"2024-08-26T23:17:42.220293Z","shell.execute_reply.started":"2024-08-26T23:17:42.214585Z","shell.execute_reply":"2024-08-26T23:17:42.219056Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Implementation of the MobileNet architecture from scracth. ","metadata":{}},{"cell_type":"code","source":"class DepthSeptConvBlock(nn.Module):\n    \"\"\"\n    Depthwise Separable Convolution Block.\n\n    This block performs depthwise separable convolutions, which consist of:\n    1. A depthwise convolution: Applies a separate 3x3 convolutional filter for each input channel.\n    2. A pointwise convolution: Applies a 1x1 convolution to mix the features from the depthwise convolution.\n\n    The block includes batch normalization and ReLU activation applied after each convolution.\n\n    Args:\n        in_channels (int): Number of input channels.\n        out_channels (int): Number of output channels.\n        stride (int, optional): Stride of the depthwise convolution. Default is 1.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n\n        # Depthwise convolution followed by batch normalization and ReLU activation.\n        self.depthwise_layer = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels,\n                      out_channels=in_channels,\n                      kernel_size=3,\n                      stride=stride,\n                      groups=in_channels,\n                      padding=1),\n            nn.BatchNorm2d(num_features=in_channels),\n            nn.ReLU(inplace=True)\n        )\n\n        # Pointwise convolution followed by batch normalization and ReLU activation.\n        self.pointwise_layer = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels,\n                      out_channels=out_channels,\n                      kernel_size=1,\n                      stride=1),\n            nn.BatchNorm2d(num_features=out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass through the Depthwise Separable Convolution Block.\n\n        Args:\n            x (torch.Tensor): Input tensor with shape (batch_size, in_channels, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor with shape (batch_size, out_channels, height, width).\n        \"\"\"\n        x = self.depthwise_layer(x)\n        x = self.pointwise_layer(x)\n\n        return x\n\nprint(\"The Depthwise Separable Convolutions block defined!\")","metadata":{"execution":{"iopub.status.busy":"2024-08-26T22:00:04.812620Z","iopub.execute_input":"2024-08-26T22:00:04.813565Z","iopub.status.idle":"2024-08-26T22:00:04.822848Z","shell.execute_reply.started":"2024-08-26T22:00:04.813524Z","shell.execute_reply":"2024-08-26T22:00:04.821944Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"The Depthwise Separable Convolutions block defined!\n","output_type":"stream"}]},{"cell_type":"code","source":"class MobileNet(nn.Module):\n    \"\"\"\n    MobileNet Architecture from the paper \"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\"\n\n\n    MobileNet is a lightweight deep learning model designed for efficient inference on mobile and edge devices.\n    It uses depthwise separable convolutions to reduce the number of parameters and computational complexity.\n\n    Args:\n        out_channels (int, optional): Number of output channels for the initial convolution layer. Default is 32.\n        output_size (int, optional): Size of the output features before the fully connected layer. Default is 1024.\n        num_classes (int, optional): Number of classes for classification. Default is 10.\n    \"\"\"\n\n    def __init__(self, out_channels=32, output_size=1024, num_classes=10):\n        super().__init__()\n\n        # Initial convolutional layer followed by batch normalization and ReLU activation.\n        self.input_conv = nn.Sequential(\n            nn.Conv2d(in_channels=3,\n                      kernel_size=3,\n                      out_channels=out_channels,\n                      stride=2,\n                      padding=1),\n            nn.BatchNorm2d(num_features=out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n        # MobileNet core: sequence of depthwise separable convolution blocks.\n        self.mobile_net = nn.Sequential(\n            DepthSeptConvBlock(in_channels=32, out_channels=64, stride=1),\n            DepthSeptConvBlock(in_channels=64, out_channels=128, stride=2),\n            DepthSeptConvBlock(in_channels=128, out_channels=128, stride=1),\n            DepthSeptConvBlock(in_channels=128, out_channels=256, stride=2),\n            DepthSeptConvBlock(in_channels=256, out_channels=256, stride=1),\n            DepthSeptConvBlock(in_channels=256, out_channels=512, stride=2),\n\n            # Repeated (5x 512) Depthwise Separable Convolution Blocks.\n            DepthSeptConvBlock(in_channels=512, out_channels=512, stride=1),\n            DepthSeptConvBlock(in_channels=512, out_channels=512, stride=1),\n            DepthSeptConvBlock(in_channels=512, out_channels=512, stride=1),\n            DepthSeptConvBlock(in_channels=512, out_channels=512, stride=1),\n            DepthSeptConvBlock(in_channels=512, out_channels=512, stride=1),\n\n            # Additional Depthwise Separable Convolutions followed by Average Pooling.\n            DepthSeptConvBlock(in_channels=512, out_channels=1024, stride=2),\n            DepthSeptConvBlock(in_channels=1024, out_channels=1024, stride=2),\n\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n\n        # Fully connected layer for classification.\n        self.fc = nn.Sequential(\n            nn.Linear(in_features=output_size, out_features=num_classes)\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass through the MobileNet network.\n\n        Args:\n            x (torch.Tensor): Input tensor with shape (batch_size, 3, height, width).\n\n        Returns:\n            torch.Tensor: Output tensor with shape (batch_size, num_classes), representing the class scores.\n        \"\"\"\n        x = self.input_conv(x)\n        x = self.mobile_net(x)\n\n        # Flatten the output before the fully connected layer.\n        x = x.view(-1, 1024)\n        x = self.fc(x)\n        x = nn_func.softmax(x, dim=1)\n\n        return x\n\nprint(\"MobileNet architecture with depthwise separable convolutions defined!\")","metadata":{"execution":{"iopub.status.busy":"2024-08-26T22:00:11.508964Z","iopub.execute_input":"2024-08-26T22:00:11.509710Z","iopub.status.idle":"2024-08-26T22:00:11.523238Z","shell.execute_reply.started":"2024-08-26T22:00:11.509667Z","shell.execute_reply":"2024-08-26T22:00:11.522219Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"MobileNet architecture with depthwise separable convolutions defined!\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.manual_seed(2024)\n\ndef load_MNIST_dataset():\n\n\n    # Define the transformations to be performed.\n    transformations = transforms_v2.Compose([\n        # Resize the images.\n        transforms_v2.Resize((224, 224)),\n\n        # Random horizontal flip with a probability of 0.5\n        transforms_v2.RandomHorizontalFlip(p=0.5),\n\n        # Random rotation of the image by a given angle (degrees)\n        transforms_v2.RandomRotation(degrees=30),\n\n        # Convert images to tensors.\n        transforms_v2.ToTensor(),\n\n        # Normalize the pixel values (in RGB channels).\n        transforms_v2.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n        ])\n\n\n    train_data = datasets.FashionMNIST(\n        root=\"./data\",\n        train=True,\n      transform=transformations,\n      download=True\n        )\n\n    valid_data = datasets.FashionMNIST(\n        root=\"./data\",\n        train=False,\n        transform=transformations,\n        download=True\n        )\n\n    train_loader = DataLoader(train_data, batch_size=64, shuffle=True, num_workers=1)\n    valid_loader = DataLoader(valid_data, batch_size=128, shuffle=False, num_workers=1)\n\n    return train_loader, valid_loader\n\n\ntrain_loader, valid_loader = load_MNIST_dataset()\nprint(\"Data are loaded and are ready to use!\")","metadata":{"execution":{"iopub.status.busy":"2024-08-26T22:00:18.612867Z","iopub.execute_input":"2024-08-26T22:00:18.613687Z","iopub.status.idle":"2024-08-26T22:00:25.656736Z","shell.execute_reply.started":"2024-08-26T22:00:18.613645Z","shell.execute_reply":"2024-08-26T22:00:25.655704Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 26421880/26421880 [00:02<00:00, 10079363.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 29515/29515 [00:00<00:00, 212264.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4422102/4422102 [00:01<00:00, 3994193.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5148/5148 [00:00<00:00, 10657589.83it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n\nData are loaded and are ready to use!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_model(device, model, train_loader, epoch, optimizer):\n\n    # Set the model to training mode.\n    model.train()\n    train_loss = 0\n    print(f\"\\nEpoch: {epoch}\")\n\n    # process the images in batches.\n    for batch_idx, (data, labels) in enumerate(train_loader):\n        # Use the device for hardware acceleration.\n        data = data.to(device)\n        label = labels.to(device)\n\n        # Reset the optimizer.\n        optimizer.zero_grad()\n\n        # Push the data forward through the model layers.\n        output = model(data)\n\n        # Get the loss.\n        loss = loss_criteria(output, label)\n\n        # Keep a running total\n        train_loss += loss.item()\n\n        # Backpropagate\n        loss.backward()\n        optimizer.step()\n     \n        # Print metrics for every 5 batches\n        if batch_idx % 5 == 0:\n            print(f\"Training set [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100 * batch_idx / len(train_loader):.0f}%)] Loss: {loss.item():.5f} \")\n\n    # Return the average loss for each epoch.\n    avg_loss = train_loss / (batch_idx + 1)\n    print(f\"Average Training Loss: {avg_loss:.5f}\")\n\n    print(\"Training Completed!\")\n\n    return avg_loss","metadata":{"execution":{"iopub.status.busy":"2024-08-26T22:00:31.138294Z","iopub.execute_input":"2024-08-26T22:00:31.138636Z","iopub.status.idle":"2024-08-26T22:00:31.146361Z","shell.execute_reply.started":"2024-08-26T22:00:31.138604Z","shell.execute_reply":"2024-08-26T22:00:31.145390Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def validate_model(device, model, val_loader):\n    # Switch the model to evaluation mode to see how the model is performing.\n    model.eval()\n    print(\"Model Validation Starts!\")\n\n    # Initialize the validation loss.\n    val_loss = 0\n    correct = 0\n\n    with torch.no_grad():\n        batch_count = 0\n        for data, target in val_loader:\n            batch_count += 1\n            data, target = data.to(device), target.to(device)\n\n            # Get the predicted class for this  batch.\n            output = model(data)\n\n            # Calculate the loss for this batch.\n            val_loss += loss_criteria(output, target).item()\n\n            # Calculate the accuracy  metrics for this batch.\n            _, predicted = torch.max(output.data, 1)\n            correct += torch.sum(target == predicted).item()\n\n        # Calculate the total accuracy and average loss for each epoch.\n        avg_loss = val_loss / batch_count\n        print(f'Average Validation Loss: {avg_loss:.5f}, Accuracy: {correct}/{len(val_loader.dataset)} ({100 * correct / len(val_loader.dataset):.0f}%)')\n\n        # Return average loss for each epoch.\n        return avg_loss","metadata":{"execution":{"iopub.status.busy":"2024-08-26T22:00:41.532745Z","iopub.execute_input":"2024-08-26T22:00:41.533361Z","iopub.status.idle":"2024-08-26T22:00:41.540754Z","shell.execute_reply.started":"2024-08-26T22:00:41.533318Z","shell.execute_reply":"2024-08-26T22:00:41.539775Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# It is time to use the train function to train and the validatio function to evaluate how the model is performing.\n\ndevice = \"cpu\"\n# Check if GPU is present or avaliable as the hardware accelerator.\nif (torch.cuda.is_available()):\n    device = \"cuda\"\nprint(f\"Training on {device}\")\n\n\n# Create an instance of the model class and allocate it to the device available.\nmodel = MobileNet(num_classes=10).to(device)\n\n# Define the learning rate.\nlearning_rate = 1e-4\n# # Use Adaptive Moment Estimation Optimization to adjust and update the model weights.\n# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Using RMSProp optimizer\noptimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n\n# Specify the loss criteria.\nloss_criteria = nn.CrossEntropyLoss()\n\n# Initialize empty arrays to track metrics.\nepoch_nums, training_loss, validation_loss,  = [], [], []\n\n# Train over 30 epochs.\nepochs = 25\nfor epoch in range(1, epochs +1):\n    train_loss = train_model(device, model, train_loader, epoch, optimizer)\n    val_loss = validate_model(device, model, valid_loader)\n\n    # Append the metrics to the predefined empty arrays.\n    epoch_nums.append(epoch)\n    training_loss.append(train_loss)\n    validation_loss.append(val_loss)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T22:13:51.825129Z","iopub.execute_input":"2024-08-26T22:13:51.826062Z","iopub.status.idle":"2024-08-26T23:14:26.170091Z","shell.execute_reply.started":"2024-08-26T22:13:51.826018Z","shell.execute_reply":"2024-08-26T23:14:26.169083Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Training on cuda\n\nEpoch: 1\nTraining set [0/60000 (0%)] Loss: 2.30418 \nTraining set [320/60000 (1%)] Loss: 2.26698 \nTraining set [640/60000 (1%)] Loss: 2.20818 \nTraining set [960/60000 (2%)] Loss: 2.11573 \nTraining set [1280/60000 (2%)] Loss: 2.13194 \nTraining set [1600/60000 (3%)] Loss: 2.20268 \nTraining set [1920/60000 (3%)] Loss: 2.13192 \nTraining set [2240/60000 (4%)] Loss: 2.06216 \nTraining set [2560/60000 (4%)] Loss: 2.04501 \nTraining set [2880/60000 (5%)] Loss: 1.99907 \nTraining set [3200/60000 (5%)] Loss: 2.04447 \nTraining set [3520/60000 (6%)] Loss: 2.07640 \nTraining set [3840/60000 (6%)] Loss: 2.07297 \nTraining set [4160/60000 (7%)] Loss: 2.03649 \nTraining set [4480/60000 (7%)] Loss: 2.00925 \nTraining set [4800/60000 (8%)] Loss: 2.02753 \nTraining set [5120/60000 (9%)] Loss: 1.95916 \nTraining set [5440/60000 (9%)] Loss: 1.98488 \nTraining set [5760/60000 (10%)] Loss: 2.08801 \nTraining set [6080/60000 (10%)] Loss: 2.01543 \nTraining set [6400/60000 (11%)] Loss: 2.01855 \nTraining set [6720/60000 (11%)] Loss: 1.90927 \nTraining set [7040/60000 (12%)] Loss: 1.92333 \nTraining set [7360/60000 (12%)] Loss: 1.95718 \nTraining set [7680/60000 (13%)] Loss: 1.95640 \nTraining set [8000/60000 (13%)] Loss: 1.95517 \nTraining set [8320/60000 (14%)] Loss: 1.98114 \nTraining set [8640/60000 (14%)] Loss: 1.97343 \nTraining set [8960/60000 (15%)] Loss: 1.87133 \nTraining set [9280/60000 (15%)] Loss: 1.92954 \nTraining set [9600/60000 (16%)] Loss: 1.94407 \nTraining set [9920/60000 (17%)] Loss: 1.92494 \nTraining set [10240/60000 (17%)] Loss: 1.85606 \nTraining set [10560/60000 (18%)] Loss: 2.01064 \nTraining set [10880/60000 (18%)] Loss: 1.90236 \nTraining set [11200/60000 (19%)] Loss: 1.84174 \nTraining set [11520/60000 (19%)] Loss: 1.85251 \nTraining set [11840/60000 (20%)] Loss: 1.89049 \nTraining set [12160/60000 (20%)] Loss: 1.94604 \nTraining set [12480/60000 (21%)] Loss: 1.91745 \nTraining set [12800/60000 (21%)] Loss: 1.75914 \nTraining set [13120/60000 (22%)] Loss: 1.91870 \nTraining set [13440/60000 (22%)] Loss: 1.82556 \nTraining set [13760/60000 (23%)] Loss: 1.79554 \nTraining set [14080/60000 (23%)] Loss: 1.85171 \nTraining set [14400/60000 (24%)] Loss: 1.94805 \nTraining set [14720/60000 (25%)] Loss: 1.80231 \nTraining set [15040/60000 (25%)] Loss: 1.87835 \nTraining set [15360/60000 (26%)] Loss: 1.79078 \nTraining set [15680/60000 (26%)] Loss: 1.74761 \nTraining set [16000/60000 (27%)] Loss: 2.01540 \nTraining set [16320/60000 (27%)] Loss: 1.81564 \nTraining set [16640/60000 (28%)] Loss: 1.79880 \nTraining set [16960/60000 (28%)] Loss: 1.82185 \nTraining set [17280/60000 (29%)] Loss: 1.74851 \nTraining set [17600/60000 (29%)] Loss: 1.90280 \nTraining set [17920/60000 (30%)] Loss: 1.82353 \nTraining set [18240/60000 (30%)] Loss: 1.76332 \nTraining set [18560/60000 (31%)] Loss: 1.82763 \nTraining set [18880/60000 (31%)] Loss: 1.81938 \nTraining set [19200/60000 (32%)] Loss: 1.86901 \nTraining set [19520/60000 (33%)] Loss: 1.82132 \nTraining set [19840/60000 (33%)] Loss: 1.79187 \nTraining set [20160/60000 (34%)] Loss: 1.85507 \nTraining set [20480/60000 (34%)] Loss: 1.81438 \nTraining set [20800/60000 (35%)] Loss: 1.80758 \nTraining set [21120/60000 (35%)] Loss: 1.92750 \nTraining set [21440/60000 (36%)] Loss: 1.80958 \nTraining set [21760/60000 (36%)] Loss: 1.74175 \nTraining set [22080/60000 (37%)] Loss: 1.79812 \nTraining set [22400/60000 (37%)] Loss: 1.88451 \nTraining set [22720/60000 (38%)] Loss: 1.83811 \nTraining set [23040/60000 (38%)] Loss: 1.90660 \nTraining set [23360/60000 (39%)] Loss: 1.80545 \nTraining set [23680/60000 (39%)] Loss: 1.74133 \nTraining set [24000/60000 (40%)] Loss: 1.79025 \nTraining set [24320/60000 (41%)] Loss: 1.80223 \nTraining set [24640/60000 (41%)] Loss: 1.75222 \nTraining set [24960/60000 (42%)] Loss: 1.86680 \nTraining set [25280/60000 (42%)] Loss: 1.82465 \nTraining set [25600/60000 (43%)] Loss: 1.80918 \nTraining set [25920/60000 (43%)] Loss: 1.72112 \nTraining set [26240/60000 (44%)] Loss: 1.80959 \nTraining set [26560/60000 (44%)] Loss: 1.87029 \nTraining set [26880/60000 (45%)] Loss: 1.74975 \nTraining set [27200/60000 (45%)] Loss: 1.77875 \nTraining set [27520/60000 (46%)] Loss: 1.60970 \nTraining set [27840/60000 (46%)] Loss: 1.77518 \nTraining set [28160/60000 (47%)] Loss: 1.82463 \nTraining set [28480/60000 (47%)] Loss: 1.82134 \nTraining set [28800/60000 (48%)] Loss: 1.76888 \nTraining set [29120/60000 (49%)] Loss: 1.77668 \nTraining set [29440/60000 (49%)] Loss: 1.70433 \nTraining set [29760/60000 (50%)] Loss: 1.73167 \nTraining set [30080/60000 (50%)] Loss: 1.75490 \nTraining set [30400/60000 (51%)] Loss: 1.68103 \nTraining set [30720/60000 (51%)] Loss: 1.75547 \nTraining set [31040/60000 (52%)] Loss: 1.70038 \nTraining set [31360/60000 (52%)] Loss: 1.73087 \nTraining set [31680/60000 (53%)] Loss: 1.73474 \nTraining set [32000/60000 (53%)] Loss: 1.67200 \nTraining set [32320/60000 (54%)] Loss: 1.81277 \nTraining set [32640/60000 (54%)] Loss: 1.77620 \nTraining set [32960/60000 (55%)] Loss: 1.76018 \nTraining set [33280/60000 (55%)] Loss: 1.73042 \nTraining set [33600/60000 (56%)] Loss: 1.69045 \nTraining set [33920/60000 (57%)] Loss: 1.71722 \nTraining set [34240/60000 (57%)] Loss: 1.66277 \nTraining set [34560/60000 (58%)] Loss: 1.73850 \nTraining set [34880/60000 (58%)] Loss: 1.74036 \nTraining set [35200/60000 (59%)] Loss: 1.69909 \nTraining set [35520/60000 (59%)] Loss: 1.77433 \nTraining set [35840/60000 (60%)] Loss: 1.71618 \nTraining set [36160/60000 (60%)] Loss: 1.79113 \nTraining set [36480/60000 (61%)] Loss: 1.71220 \nTraining set [36800/60000 (61%)] Loss: 1.74524 \nTraining set [37120/60000 (62%)] Loss: 1.70519 \nTraining set [37440/60000 (62%)] Loss: 1.77125 \nTraining set [37760/60000 (63%)] Loss: 1.70903 \nTraining set [38080/60000 (63%)] Loss: 1.62866 \nTraining set [38400/60000 (64%)] Loss: 1.75089 \nTraining set [38720/60000 (64%)] Loss: 1.72097 \nTraining set [39040/60000 (65%)] Loss: 1.75653 \nTraining set [39360/60000 (66%)] Loss: 1.70815 \nTraining set [39680/60000 (66%)] Loss: 1.70064 \nTraining set [40000/60000 (67%)] Loss: 1.67607 \nTraining set [40320/60000 (67%)] Loss: 1.75986 \nTraining set [40640/60000 (68%)] Loss: 1.74863 \nTraining set [40960/60000 (68%)] Loss: 1.68373 \nTraining set [41280/60000 (69%)] Loss: 1.81986 \nTraining set [41600/60000 (69%)] Loss: 1.79829 \nTraining set [41920/60000 (70%)] Loss: 1.69483 \nTraining set [42240/60000 (70%)] Loss: 1.75952 \nTraining set [42560/60000 (71%)] Loss: 1.72256 \nTraining set [42880/60000 (71%)] Loss: 1.74031 \nTraining set [43200/60000 (72%)] Loss: 1.72824 \nTraining set [43520/60000 (72%)] Loss: 1.73964 \nTraining set [43840/60000 (73%)] Loss: 1.71140 \nTraining set [44160/60000 (74%)] Loss: 1.86510 \nTraining set [44480/60000 (74%)] Loss: 1.76819 \nTraining set [44800/60000 (75%)] Loss: 1.69515 \nTraining set [45120/60000 (75%)] Loss: 1.81245 \nTraining set [45440/60000 (76%)] Loss: 1.74982 \nTraining set [45760/60000 (76%)] Loss: 1.74303 \nTraining set [46080/60000 (77%)] Loss: 1.77237 \nTraining set [46400/60000 (77%)] Loss: 1.71374 \nTraining set [46720/60000 (78%)] Loss: 1.71649 \nTraining set [47040/60000 (78%)] Loss: 1.79430 \nTraining set [47360/60000 (79%)] Loss: 1.69584 \nTraining set [47680/60000 (79%)] Loss: 1.72033 \nTraining set [48000/60000 (80%)] Loss: 1.71119 \nTraining set [48320/60000 (80%)] Loss: 1.67612 \nTraining set [48640/60000 (81%)] Loss: 1.69876 \nTraining set [48960/60000 (82%)] Loss: 1.71083 \nTraining set [49280/60000 (82%)] Loss: 1.70261 \nTraining set [49600/60000 (83%)] Loss: 1.69526 \nTraining set [49920/60000 (83%)] Loss: 1.65577 \nTraining set [50240/60000 (84%)] Loss: 1.67097 \nTraining set [50560/60000 (84%)] Loss: 1.73318 \nTraining set [50880/60000 (85%)] Loss: 1.69432 \nTraining set [51200/60000 (85%)] Loss: 1.65827 \nTraining set [51520/60000 (86%)] Loss: 1.72491 \nTraining set [51840/60000 (86%)] Loss: 1.68479 \nTraining set [52160/60000 (87%)] Loss: 1.67066 \nTraining set [52480/60000 (87%)] Loss: 1.63988 \nTraining set [52800/60000 (88%)] Loss: 1.68918 \nTraining set [53120/60000 (88%)] Loss: 1.66005 \nTraining set [53440/60000 (89%)] Loss: 1.64085 \nTraining set [53760/60000 (90%)] Loss: 1.68361 \nTraining set [54080/60000 (90%)] Loss: 1.76027 \nTraining set [54400/60000 (91%)] Loss: 1.78397 \nTraining set [54720/60000 (91%)] Loss: 1.66592 \nTraining set [55040/60000 (92%)] Loss: 1.69154 \nTraining set [55360/60000 (92%)] Loss: 1.65146 \nTraining set [55680/60000 (93%)] Loss: 1.79266 \nTraining set [56000/60000 (93%)] Loss: 1.71500 \nTraining set [56320/60000 (94%)] Loss: 1.69334 \nTraining set [56640/60000 (94%)] Loss: 1.67965 \nTraining set [56960/60000 (95%)] Loss: 1.64607 \nTraining set [57280/60000 (95%)] Loss: 1.81074 \nTraining set [57600/60000 (96%)] Loss: 1.76722 \nTraining set [57920/60000 (96%)] Loss: 1.71334 \nTraining set [58240/60000 (97%)] Loss: 1.73170 \nTraining set [58560/60000 (98%)] Loss: 1.74433 \nTraining set [58880/60000 (98%)] Loss: 1.75438 \nTraining set [59200/60000 (99%)] Loss: 1.75518 \nTraining set [59520/60000 (99%)] Loss: 1.69560 \nTraining set [59840/60000 (100%)] Loss: 1.69375 \nAverage Training Loss: 1.80577\nTraining Completed!\nModel Validation Starts!\nAverage Validation Loss: 1.73482, Accuracy: 7392/10000 (74%)\n\nEpoch: 2\nTraining set [0/60000 (0%)] Loss: 1.66435 \nTraining set [320/60000 (1%)] Loss: 1.76587 \nTraining set [640/60000 (1%)] Loss: 1.67854 \nTraining set [960/60000 (2%)] Loss: 1.74354 \nTraining set [1280/60000 (2%)] Loss: 1.76626 \nTraining set [1600/60000 (3%)] Loss: 1.77297 \nTraining set [1920/60000 (3%)] Loss: 1.73520 \nTraining set [2240/60000 (4%)] Loss: 1.68138 \nTraining set [2560/60000 (4%)] Loss: 1.63770 \nTraining set [2880/60000 (5%)] Loss: 1.64670 \nTraining set [3200/60000 (5%)] Loss: 1.71772 \nTraining set [3520/60000 (6%)] Loss: 1.74295 \nTraining set [3840/60000 (6%)] Loss: 1.73618 \nTraining set [4160/60000 (7%)] Loss: 1.64164 \nTraining set [4480/60000 (7%)] Loss: 1.73064 \nTraining set [4800/60000 (8%)] Loss: 1.70158 \nTraining set [5120/60000 (9%)] Loss: 1.77356 \nTraining set [5440/60000 (9%)] Loss: 1.67433 \nTraining set [5760/60000 (10%)] Loss: 1.70788 \nTraining set [6080/60000 (10%)] Loss: 1.68518 \nTraining set [6400/60000 (11%)] Loss: 1.70358 \nTraining set [6720/60000 (11%)] Loss: 1.70213 \nTraining set [7040/60000 (12%)] Loss: 1.64520 \nTraining set [7360/60000 (12%)] Loss: 1.66788 \nTraining set [7680/60000 (13%)] Loss: 1.73128 \nTraining set [8000/60000 (13%)] Loss: 1.67645 \nTraining set [8320/60000 (14%)] Loss: 1.63195 \nTraining set [8640/60000 (14%)] Loss: 1.70640 \nTraining set [8960/60000 (15%)] Loss: 1.73631 \nTraining set [9280/60000 (15%)] Loss: 1.71430 \nTraining set [9600/60000 (16%)] Loss: 1.70578 \nTraining set [9920/60000 (17%)] Loss: 1.75086 \nTraining set [10240/60000 (17%)] Loss: 1.65838 \nTraining set [10560/60000 (18%)] Loss: 1.69842 \nTraining set [10880/60000 (18%)] Loss: 1.68626 \nTraining set [11200/60000 (19%)] Loss: 1.73079 \nTraining set [11520/60000 (19%)] Loss: 1.73049 \nTraining set [11840/60000 (20%)] Loss: 1.73047 \nTraining set [12160/60000 (20%)] Loss: 1.66401 \nTraining set [12480/60000 (21%)] Loss: 1.74534 \nTraining set [12800/60000 (21%)] Loss: 1.71069 \nTraining set [13120/60000 (22%)] Loss: 1.70998 \nTraining set [13440/60000 (22%)] Loss: 1.62978 \nTraining set [13760/60000 (23%)] Loss: 1.63021 \nTraining set [14080/60000 (23%)] Loss: 1.70009 \nTraining set [14400/60000 (24%)] Loss: 1.72810 \nTraining set [14720/60000 (25%)] Loss: 1.76628 \nTraining set [15040/60000 (25%)] Loss: 1.59452 \nTraining set [15360/60000 (26%)] Loss: 1.74494 \nTraining set [15680/60000 (26%)] Loss: 1.63994 \nTraining set [16000/60000 (27%)] Loss: 1.67872 \nTraining set [16320/60000 (27%)] Loss: 1.68717 \nTraining set [16640/60000 (28%)] Loss: 1.64831 \nTraining set [16960/60000 (28%)] Loss: 1.72347 \nTraining set [17280/60000 (29%)] Loss: 1.69751 \nTraining set [17600/60000 (29%)] Loss: 1.70486 \nTraining set [17920/60000 (30%)] Loss: 1.62077 \nTraining set [18240/60000 (30%)] Loss: 1.66066 \nTraining set [18560/60000 (31%)] Loss: 1.67265 \nTraining set [18880/60000 (31%)] Loss: 1.72308 \nTraining set [19200/60000 (32%)] Loss: 1.71475 \nTraining set [19520/60000 (33%)] Loss: 1.60113 \nTraining set [19840/60000 (33%)] Loss: 1.67746 \nTraining set [20160/60000 (34%)] Loss: 1.66785 \nTraining set [20480/60000 (34%)] Loss: 1.75979 \nTraining set [20800/60000 (35%)] Loss: 1.64868 \nTraining set [21120/60000 (35%)] Loss: 1.65942 \nTraining set [21440/60000 (36%)] Loss: 1.65980 \nTraining set [21760/60000 (36%)] Loss: 1.65799 \nTraining set [22080/60000 (37%)] Loss: 1.57518 \nTraining set [22400/60000 (37%)] Loss: 1.69957 \nTraining set [22720/60000 (38%)] Loss: 1.75279 \nTraining set [23040/60000 (38%)] Loss: 1.68973 \nTraining set [23360/60000 (39%)] Loss: 1.62090 \nTraining set [23680/60000 (39%)] Loss: 1.60175 \nTraining set [24000/60000 (40%)] Loss: 1.79011 \nTraining set [24320/60000 (41%)] Loss: 1.71191 \nTraining set [24640/60000 (41%)] Loss: 1.72762 \nTraining set [24960/60000 (42%)] Loss: 1.80685 \nTraining set [25280/60000 (42%)] Loss: 1.65164 \nTraining set [25600/60000 (43%)] Loss: 1.57486 \nTraining set [25920/60000 (43%)] Loss: 1.63946 \nTraining set [26240/60000 (44%)] Loss: 1.64152 \nTraining set [26560/60000 (44%)] Loss: 1.61993 \nTraining set [26880/60000 (45%)] Loss: 1.76846 \nTraining set [27200/60000 (45%)] Loss: 1.69162 \nTraining set [27520/60000 (46%)] Loss: 1.72535 \nTraining set [27840/60000 (46%)] Loss: 1.65467 \nTraining set [28160/60000 (47%)] Loss: 1.60988 \nTraining set [28480/60000 (47%)] Loss: 1.65363 \nTraining set [28800/60000 (48%)] Loss: 1.72609 \nTraining set [29120/60000 (49%)] Loss: 1.67227 \nTraining set [29440/60000 (49%)] Loss: 1.71200 \nTraining set [29760/60000 (50%)] Loss: 1.63857 \nTraining set [30080/60000 (50%)] Loss: 1.63840 \nTraining set [30400/60000 (51%)] Loss: 1.67445 \nTraining set [30720/60000 (51%)] Loss: 1.67610 \nTraining set [31040/60000 (52%)] Loss: 1.66726 \nTraining set [31360/60000 (52%)] Loss: 1.67339 \nTraining set [31680/60000 (53%)] Loss: 1.73070 \nTraining set [32000/60000 (53%)] Loss: 1.66276 \nTraining set [32320/60000 (54%)] Loss: 1.60601 \nTraining set [32640/60000 (54%)] Loss: 1.75111 \nTraining set [32960/60000 (55%)] Loss: 1.64788 \nTraining set [33280/60000 (55%)] Loss: 1.68894 \nTraining set [33600/60000 (56%)] Loss: 1.76242 \nTraining set [33920/60000 (57%)] Loss: 1.64167 \nTraining set [34240/60000 (57%)] Loss: 1.63698 \nTraining set [34560/60000 (58%)] Loss: 1.65546 \nTraining set [34880/60000 (58%)] Loss: 1.68071 \nTraining set [35200/60000 (59%)] Loss: 1.74210 \nTraining set [35520/60000 (59%)] Loss: 1.64964 \nTraining set [35840/60000 (60%)] Loss: 1.72783 \nTraining set [36160/60000 (60%)] Loss: 1.62589 \nTraining set [36480/60000 (61%)] Loss: 1.72189 \nTraining set [36800/60000 (61%)] Loss: 1.62903 \nTraining set [37120/60000 (62%)] Loss: 1.70643 \nTraining set [37440/60000 (62%)] Loss: 1.62657 \nTraining set [37760/60000 (63%)] Loss: 1.75306 \nTraining set [38080/60000 (63%)] Loss: 1.67980 \nTraining set [38400/60000 (64%)] Loss: 1.62037 \nTraining set [38720/60000 (64%)] Loss: 1.63951 \nTraining set [39040/60000 (65%)] Loss: 1.72884 \nTraining set [39360/60000 (66%)] Loss: 1.63851 \nTraining set [39680/60000 (66%)] Loss: 1.64060 \nTraining set [40000/60000 (67%)] Loss: 1.64780 \nTraining set [40320/60000 (67%)] Loss: 1.72248 \nTraining set [40640/60000 (68%)] Loss: 1.69846 \nTraining set [40960/60000 (68%)] Loss: 1.67437 \nTraining set [41280/60000 (69%)] Loss: 1.64238 \nTraining set [41600/60000 (69%)] Loss: 1.69941 \nTraining set [41920/60000 (70%)] Loss: 1.64410 \nTraining set [42240/60000 (70%)] Loss: 1.67600 \nTraining set [42560/60000 (71%)] Loss: 1.65274 \nTraining set [42880/60000 (71%)] Loss: 1.63726 \nTraining set [43200/60000 (72%)] Loss: 1.68257 \nTraining set [43520/60000 (72%)] Loss: 1.67232 \nTraining set [43840/60000 (73%)] Loss: 1.66229 \nTraining set [44160/60000 (74%)] Loss: 1.62117 \nTraining set [44480/60000 (74%)] Loss: 1.60724 \nTraining set [44800/60000 (75%)] Loss: 1.66555 \nTraining set [45120/60000 (75%)] Loss: 1.72053 \nTraining set [45440/60000 (76%)] Loss: 1.71493 \nTraining set [45760/60000 (76%)] Loss: 1.67184 \nTraining set [46080/60000 (77%)] Loss: 1.68609 \nTraining set [46400/60000 (77%)] Loss: 1.61619 \nTraining set [46720/60000 (78%)] Loss: 1.64884 \nTraining set [47040/60000 (78%)] Loss: 1.60163 \nTraining set [47360/60000 (79%)] Loss: 1.64858 \nTraining set [47680/60000 (79%)] Loss: 1.65836 \nTraining set [48000/60000 (80%)] Loss: 1.75856 \nTraining set [48320/60000 (80%)] Loss: 1.66686 \nTraining set [48640/60000 (81%)] Loss: 1.68550 \nTraining set [48960/60000 (82%)] Loss: 1.63901 \nTraining set [49280/60000 (82%)] Loss: 1.68223 \nTraining set [49600/60000 (83%)] Loss: 1.66582 \nTraining set [49920/60000 (83%)] Loss: 1.62993 \nTraining set [50240/60000 (84%)] Loss: 1.68259 \nTraining set [50560/60000 (84%)] Loss: 1.71894 \nTraining set [50880/60000 (85%)] Loss: 1.66643 \nTraining set [51200/60000 (85%)] Loss: 1.68051 \nTraining set [51520/60000 (86%)] Loss: 1.55471 \nTraining set [51840/60000 (86%)] Loss: 1.72635 \nTraining set [52160/60000 (87%)] Loss: 1.64799 \nTraining set [52480/60000 (87%)] Loss: 1.67013 \nTraining set [52800/60000 (88%)] Loss: 1.71887 \nTraining set [53120/60000 (88%)] Loss: 1.66851 \nTraining set [53440/60000 (89%)] Loss: 1.73399 \nTraining set [53760/60000 (90%)] Loss: 1.60969 \nTraining set [54080/60000 (90%)] Loss: 1.69500 \nTraining set [54400/60000 (91%)] Loss: 1.67423 \nTraining set [54720/60000 (91%)] Loss: 1.73428 \nTraining set [55040/60000 (92%)] Loss: 1.72868 \nTraining set [55360/60000 (92%)] Loss: 1.63411 \nTraining set [55680/60000 (93%)] Loss: 1.70986 \nTraining set [56000/60000 (93%)] Loss: 1.65186 \nTraining set [56320/60000 (94%)] Loss: 1.69332 \nTraining set [56640/60000 (94%)] Loss: 1.61777 \nTraining set [56960/60000 (95%)] Loss: 1.65960 \nTraining set [57280/60000 (95%)] Loss: 1.65052 \nTraining set [57600/60000 (96%)] Loss: 1.62608 \nTraining set [57920/60000 (96%)] Loss: 1.69913 \nTraining set [58240/60000 (97%)] Loss: 1.72506 \nTraining set [58560/60000 (98%)] Loss: 1.58125 \nTraining set [58880/60000 (98%)] Loss: 1.71055 \nTraining set [59200/60000 (99%)] Loss: 1.65727 \nTraining set [59520/60000 (99%)] Loss: 1.70138 \nTraining set [59840/60000 (100%)] Loss: 1.62938 \nAverage Training Loss: 1.67826\nTraining Completed!\nModel Validation Starts!\nAverage Validation Loss: 1.80065, Accuracy: 6771/10000 (68%)\n\nEpoch: 3\nTraining set [0/60000 (0%)] Loss: 1.60941 \nTraining set [320/60000 (1%)] Loss: 1.60634 \nTraining set [640/60000 (1%)] Loss: 1.65074 \nTraining set [960/60000 (2%)] Loss: 1.71838 \nTraining set [1280/60000 (2%)] Loss: 1.69926 \nTraining set [1600/60000 (3%)] Loss: 1.52693 \nTraining set [1920/60000 (3%)] Loss: 1.68224 \nTraining set [2240/60000 (4%)] Loss: 1.68894 \nTraining set [2560/60000 (4%)] Loss: 1.61546 \nTraining set [2880/60000 (5%)] Loss: 1.77339 \nTraining set [3200/60000 (5%)] Loss: 1.64959 \nTraining set [3520/60000 (6%)] Loss: 1.61370 \nTraining set [3840/60000 (6%)] Loss: 1.65771 \nTraining set [4160/60000 (7%)] Loss: 1.68331 \nTraining set [4480/60000 (7%)] Loss: 1.64896 \nTraining set [4800/60000 (8%)] Loss: 1.65070 \nTraining set [5120/60000 (9%)] Loss: 1.66217 \nTraining set [5440/60000 (9%)] Loss: 1.68035 \nTraining set [5760/60000 (10%)] Loss: 1.60695 \nTraining set [6080/60000 (10%)] Loss: 1.65156 \nTraining set [6400/60000 (11%)] Loss: 1.71394 \nTraining set [6720/60000 (11%)] Loss: 1.64614 \nTraining set [7040/60000 (12%)] Loss: 1.61898 \nTraining set [7360/60000 (12%)] Loss: 1.70424 \nTraining set [7680/60000 (13%)] Loss: 1.66037 \nTraining set [8000/60000 (13%)] Loss: 1.65959 \nTraining set [8320/60000 (14%)] Loss: 1.64667 \nTraining set [8640/60000 (14%)] Loss: 1.59633 \nTraining set [8960/60000 (15%)] Loss: 1.66148 \nTraining set [9280/60000 (15%)] Loss: 1.65463 \nTraining set [9600/60000 (16%)] Loss: 1.62320 \nTraining set [9920/60000 (17%)] Loss: 1.71737 \nTraining set [10240/60000 (17%)] Loss: 1.61960 \nTraining set [10560/60000 (18%)] Loss: 1.61366 \nTraining set [10880/60000 (18%)] Loss: 1.66698 \nTraining set [11200/60000 (19%)] Loss: 1.62176 \nTraining set [11520/60000 (19%)] Loss: 1.59893 \nTraining set [11840/60000 (20%)] Loss: 1.59142 \nTraining set [12160/60000 (20%)] Loss: 1.68332 \nTraining set [12480/60000 (21%)] Loss: 1.60414 \nTraining set [12800/60000 (21%)] Loss: 1.66958 \nTraining set [13120/60000 (22%)] Loss: 1.60652 \nTraining set [13440/60000 (22%)] Loss: 1.63537 \nTraining set [13760/60000 (23%)] Loss: 1.64331 \nTraining set [14080/60000 (23%)] Loss: 1.65506 \nTraining set [14400/60000 (24%)] Loss: 1.73953 \nTraining set [14720/60000 (25%)] Loss: 1.61493 \nTraining set [15040/60000 (25%)] Loss: 1.61557 \nTraining set [15360/60000 (26%)] Loss: 1.58769 \nTraining set [15680/60000 (26%)] Loss: 1.61324 \nTraining set [16000/60000 (27%)] Loss: 1.65122 \nTraining set [16320/60000 (27%)] Loss: 1.61970 \nTraining set [16640/60000 (28%)] Loss: 1.66544 \nTraining set [16960/60000 (28%)] Loss: 1.65314 \nTraining set [17280/60000 (29%)] Loss: 1.60972 \nTraining set [17600/60000 (29%)] Loss: 1.57860 \nTraining set [17920/60000 (30%)] Loss: 1.69832 \nTraining set [18240/60000 (30%)] Loss: 1.66303 \nTraining set [18560/60000 (31%)] Loss: 1.73722 \nTraining set [18880/60000 (31%)] Loss: 1.70320 \nTraining set [19200/60000 (32%)] Loss: 1.68363 \nTraining set [19520/60000 (33%)] Loss: 1.61766 \nTraining set [19840/60000 (33%)] Loss: 1.65436 \nTraining set [20160/60000 (34%)] Loss: 1.66901 \nTraining set [20480/60000 (34%)] Loss: 1.68329 \nTraining set [20800/60000 (35%)] Loss: 1.55413 \nTraining set [21120/60000 (35%)] Loss: 1.62224 \nTraining set [21440/60000 (36%)] Loss: 1.62951 \nTraining set [21760/60000 (36%)] Loss: 1.64685 \nTraining set [22080/60000 (37%)] Loss: 1.64611 \nTraining set [22400/60000 (37%)] Loss: 1.57067 \nTraining set [22720/60000 (38%)] Loss: 1.59339 \nTraining set [23040/60000 (38%)] Loss: 1.57934 \nTraining set [23360/60000 (39%)] Loss: 1.63749 \nTraining set [23680/60000 (39%)] Loss: 1.67594 \nTraining set [24000/60000 (40%)] Loss: 1.65206 \nTraining set [24320/60000 (41%)] Loss: 1.66125 \nTraining set [24640/60000 (41%)] Loss: 1.62688 \nTraining set [24960/60000 (42%)] Loss: 1.65649 \nTraining set [25280/60000 (42%)] Loss: 1.59157 \nTraining set [25600/60000 (43%)] Loss: 1.62743 \nTraining set [25920/60000 (43%)] Loss: 1.61363 \nTraining set [26240/60000 (44%)] Loss: 1.63120 \nTraining set [26560/60000 (44%)] Loss: 1.63068 \nTraining set [26880/60000 (45%)] Loss: 1.66999 \nTraining set [27200/60000 (45%)] Loss: 1.67116 \nTraining set [27520/60000 (46%)] Loss: 1.64194 \nTraining set [27840/60000 (46%)] Loss: 1.54579 \nTraining set [28160/60000 (47%)] Loss: 1.64298 \nTraining set [28480/60000 (47%)] Loss: 1.59565 \nTraining set [28800/60000 (48%)] Loss: 1.61775 \nTraining set [29120/60000 (49%)] Loss: 1.68362 \nTraining set [29440/60000 (49%)] Loss: 1.63900 \nTraining set [29760/60000 (50%)] Loss: 1.61306 \nTraining set [30080/60000 (50%)] Loss: 1.56647 \nTraining set [30400/60000 (51%)] Loss: 1.62335 \nTraining set [30720/60000 (51%)] Loss: 1.66822 \nTraining set [31040/60000 (52%)] Loss: 1.61212 \nTraining set [31360/60000 (52%)] Loss: 1.59901 \nTraining set [31680/60000 (53%)] Loss: 1.67907 \nTraining set [32000/60000 (53%)] Loss: 1.57177 \nTraining set [32320/60000 (54%)] Loss: 1.57088 \nTraining set [32640/60000 (54%)] Loss: 1.55608 \nTraining set [32960/60000 (55%)] Loss: 1.65030 \nTraining set [33280/60000 (55%)] Loss: 1.62994 \nTraining set [33600/60000 (56%)] Loss: 1.63065 \nTraining set [33920/60000 (57%)] Loss: 1.62024 \nTraining set [34240/60000 (57%)] Loss: 1.59215 \nTraining set [34560/60000 (58%)] Loss: 1.60015 \nTraining set [34880/60000 (58%)] Loss: 1.70518 \nTraining set [35200/60000 (59%)] Loss: 1.62735 \nTraining set [35520/60000 (59%)] Loss: 1.59538 \nTraining set [35840/60000 (60%)] Loss: 1.58532 \nTraining set [36160/60000 (60%)] Loss: 1.61634 \nTraining set [36480/60000 (61%)] Loss: 1.65909 \nTraining set [36800/60000 (61%)] Loss: 1.60977 \nTraining set [37120/60000 (62%)] Loss: 1.60454 \nTraining set [37440/60000 (62%)] Loss: 1.74026 \nTraining set [37760/60000 (63%)] Loss: 1.63019 \nTraining set [38080/60000 (63%)] Loss: 1.63540 \nTraining set [38400/60000 (64%)] Loss: 1.59445 \nTraining set [38720/60000 (64%)] Loss: 1.67342 \nTraining set [39040/60000 (65%)] Loss: 1.65845 \nTraining set [39360/60000 (66%)] Loss: 1.67474 \nTraining set [39680/60000 (66%)] Loss: 1.61887 \nTraining set [40000/60000 (67%)] Loss: 1.61679 \nTraining set [40320/60000 (67%)] Loss: 1.62486 \nTraining set [40640/60000 (68%)] Loss: 1.61811 \nTraining set [40960/60000 (68%)] Loss: 1.58477 \nTraining set [41280/60000 (69%)] Loss: 1.65654 \nTraining set [41600/60000 (69%)] Loss: 1.64133 \nTraining set [41920/60000 (70%)] Loss: 1.64566 \nTraining set [42240/60000 (70%)] Loss: 1.60261 \nTraining set [42560/60000 (71%)] Loss: 1.65749 \nTraining set [42880/60000 (71%)] Loss: 1.70872 \nTraining set [43200/60000 (72%)] Loss: 1.67760 \nTraining set [43520/60000 (72%)] Loss: 1.74246 \nTraining set [43840/60000 (73%)] Loss: 1.64432 \nTraining set [44160/60000 (74%)] Loss: 1.62743 \nTraining set [44480/60000 (74%)] Loss: 1.63183 \nTraining set [44800/60000 (75%)] Loss: 1.57227 \nTraining set [45120/60000 (75%)] Loss: 1.64645 \nTraining set [45440/60000 (76%)] Loss: 1.62474 \nTraining set [45760/60000 (76%)] Loss: 1.57952 \nTraining set [46080/60000 (77%)] Loss: 1.56752 \nTraining set [46400/60000 (77%)] Loss: 1.62532 \nTraining set [46720/60000 (78%)] Loss: 1.65921 \nTraining set [47040/60000 (78%)] Loss: 1.57665 \nTraining set [47360/60000 (79%)] Loss: 1.64494 \nTraining set [47680/60000 (79%)] Loss: 1.66002 \nTraining set [48000/60000 (80%)] Loss: 1.54578 \nTraining set [48320/60000 (80%)] Loss: 1.65388 \nTraining set [48640/60000 (81%)] Loss: 1.62626 \nTraining set [48960/60000 (82%)] Loss: 1.67026 \nTraining set [49280/60000 (82%)] Loss: 1.65093 \nTraining set [49600/60000 (83%)] Loss: 1.61372 \nTraining set [49920/60000 (83%)] Loss: 1.64220 \nTraining set [50240/60000 (84%)] Loss: 1.59935 \nTraining set [50560/60000 (84%)] Loss: 1.68163 \nTraining set [50880/60000 (85%)] Loss: 1.60998 \nTraining set [51200/60000 (85%)] Loss: 1.63865 \nTraining set [51520/60000 (86%)] Loss: 1.66278 \nTraining set [51840/60000 (86%)] Loss: 1.61143 \nTraining set [52160/60000 (87%)] Loss: 1.68969 \nTraining set [52480/60000 (87%)] Loss: 1.69799 \nTraining set [52800/60000 (88%)] Loss: 1.57334 \nTraining set [53120/60000 (88%)] Loss: 1.62610 \nTraining set [53440/60000 (89%)] Loss: 1.60082 \nTraining set [53760/60000 (90%)] Loss: 1.67778 \nTraining set [54080/60000 (90%)] Loss: 1.63603 \nTraining set [54400/60000 (91%)] Loss: 1.60404 \nTraining set [54720/60000 (91%)] Loss: 1.73289 \nTraining set [55040/60000 (92%)] Loss: 1.59006 \nTraining set [55360/60000 (92%)] Loss: 1.66525 \nTraining set [55680/60000 (93%)] Loss: 1.59801 \nTraining set [56000/60000 (93%)] Loss: 1.64843 \nTraining set [56320/60000 (94%)] Loss: 1.55903 \nTraining set [56640/60000 (94%)] Loss: 1.51132 \nTraining set [56960/60000 (95%)] Loss: 1.63609 \nTraining set [57280/60000 (95%)] Loss: 1.61933 \nTraining set [57600/60000 (96%)] Loss: 1.60972 \nTraining set [57920/60000 (96%)] Loss: 1.59197 \nTraining set [58240/60000 (97%)] Loss: 1.62909 \nTraining set [58560/60000 (98%)] Loss: 1.67714 \nTraining set [58880/60000 (98%)] Loss: 1.63558 \nTraining set [59200/60000 (99%)] Loss: 1.61788 \nTraining set [59520/60000 (99%)] Loss: 1.64793 \nTraining set [59840/60000 (100%)] Loss: 1.68079 \nAverage Training Loss: 1.63659\nTraining Completed!\nModel Validation Starts!\nAverage Validation Loss: 1.62880, Accuracy: 8361/10000 (84%)\n\nEpoch: 4\nTraining set [0/60000 (0%)] Loss: 1.56012 \nTraining set [320/60000 (1%)] Loss: 1.62382 \nTraining set [640/60000 (1%)] Loss: 1.57811 \nTraining set [960/60000 (2%)] Loss: 1.59543 \nTraining set [1280/60000 (2%)] Loss: 1.67195 \nTraining set [1600/60000 (3%)] Loss: 1.57959 \nTraining set [1920/60000 (3%)] Loss: 1.58869 \nTraining set [2240/60000 (4%)] Loss: 1.66684 \nTraining set [2560/60000 (4%)] Loss: 1.58658 \nTraining set [2880/60000 (5%)] Loss: 1.67988 \nTraining set [3200/60000 (5%)] Loss: 1.61343 \nTraining set [3520/60000 (6%)] Loss: 1.59404 \nTraining set [3840/60000 (6%)] Loss: 1.64050 \nTraining set [4160/60000 (7%)] Loss: 1.62949 \nTraining set [4480/60000 (7%)] Loss: 1.58460 \nTraining set [4800/60000 (8%)] Loss: 1.67946 \nTraining set [5120/60000 (9%)] Loss: 1.59146 \nTraining set [5440/60000 (9%)] Loss: 1.63630 \nTraining set [5760/60000 (10%)] Loss: 1.66754 \nTraining set [6080/60000 (10%)] Loss: 1.68433 \nTraining set [6400/60000 (11%)] Loss: 1.66415 \nTraining set [6720/60000 (11%)] Loss: 1.55278 \nTraining set [7040/60000 (12%)] Loss: 1.64740 \nTraining set [7360/60000 (12%)] Loss: 1.60375 \nTraining set [7680/60000 (13%)] Loss: 1.60187 \nTraining set [8000/60000 (13%)] Loss: 1.61632 \nTraining set [8320/60000 (14%)] Loss: 1.66293 \nTraining set [8640/60000 (14%)] Loss: 1.53792 \nTraining set [8960/60000 (15%)] Loss: 1.60425 \nTraining set [9280/60000 (15%)] Loss: 1.64450 \nTraining set [9600/60000 (16%)] Loss: 1.64987 \nTraining set [9920/60000 (17%)] Loss: 1.63417 \nTraining set [10240/60000 (17%)] Loss: 1.66839 \nTraining set [10560/60000 (18%)] Loss: 1.61832 \nTraining set [10880/60000 (18%)] Loss: 1.66628 \nTraining set [11200/60000 (19%)] Loss: 1.54098 \nTraining set [11520/60000 (19%)] Loss: 1.60318 \nTraining set [11840/60000 (20%)] Loss: 1.64158 \nTraining set [12160/60000 (20%)] Loss: 1.60134 \nTraining set [12480/60000 (21%)] Loss: 1.60233 \nTraining set [12800/60000 (21%)] Loss: 1.68109 \nTraining set [13120/60000 (22%)] Loss: 1.62030 \nTraining set [13440/60000 (22%)] Loss: 1.64930 \nTraining set [13760/60000 (23%)] Loss: 1.67274 \nTraining set [14080/60000 (23%)] Loss: 1.61516 \nTraining set [14400/60000 (24%)] Loss: 1.62952 \nTraining set [14720/60000 (25%)] Loss: 1.65394 \nTraining set [15040/60000 (25%)] Loss: 1.64880 \nTraining set [15360/60000 (26%)] Loss: 1.71595 \nTraining set [15680/60000 (26%)] Loss: 1.56199 \nTraining set [16000/60000 (27%)] Loss: 1.65048 \nTraining set [16320/60000 (27%)] Loss: 1.67320 \nTraining set [16640/60000 (28%)] Loss: 1.60430 \nTraining set [16960/60000 (28%)] Loss: 1.58861 \nTraining set [17280/60000 (29%)] Loss: 1.61324 \nTraining set [17600/60000 (29%)] Loss: 1.62407 \nTraining set [17920/60000 (30%)] Loss: 1.73713 \nTraining set [18240/60000 (30%)] Loss: 1.56137 \nTraining set [18560/60000 (31%)] Loss: 1.63339 \nTraining set [18880/60000 (31%)] Loss: 1.58005 \nTraining set [19200/60000 (32%)] Loss: 1.60972 \nTraining set [19520/60000 (33%)] Loss: 1.65726 \nTraining set [19840/60000 (33%)] Loss: 1.64012 \nTraining set [20160/60000 (34%)] Loss: 1.66729 \nTraining set [20480/60000 (34%)] Loss: 1.62725 \nTraining set [20800/60000 (35%)] Loss: 1.58575 \nTraining set [21120/60000 (35%)] Loss: 1.60932 \nTraining set [21440/60000 (36%)] Loss: 1.63920 \nTraining set [21760/60000 (36%)] Loss: 1.66413 \nTraining set [22080/60000 (37%)] Loss: 1.67187 \nTraining set [22400/60000 (37%)] Loss: 1.54381 \nTraining set [22720/60000 (38%)] Loss: 1.69887 \nTraining set [23040/60000 (38%)] Loss: 1.63403 \nTraining set [23360/60000 (39%)] Loss: 1.65640 \nTraining set [23680/60000 (39%)] Loss: 1.67505 \nTraining set [24000/60000 (40%)] Loss: 1.61572 \nTraining set [24320/60000 (41%)] Loss: 1.63731 \nTraining set [24640/60000 (41%)] Loss: 1.57966 \nTraining set [24960/60000 (42%)] Loss: 1.59620 \nTraining set [25280/60000 (42%)] Loss: 1.56559 \nTraining set [25600/60000 (43%)] Loss: 1.64154 \nTraining set [25920/60000 (43%)] Loss: 1.57140 \nTraining set [26240/60000 (44%)] Loss: 1.65522 \nTraining set [26560/60000 (44%)] Loss: 1.62002 \nTraining set [26880/60000 (45%)] Loss: 1.62222 \nTraining set [27200/60000 (45%)] Loss: 1.65520 \nTraining set [27520/60000 (46%)] Loss: 1.70488 \nTraining set [27840/60000 (46%)] Loss: 1.63246 \nTraining set [28160/60000 (47%)] Loss: 1.58639 \nTraining set [28480/60000 (47%)] Loss: 1.65049 \nTraining set [28800/60000 (48%)] Loss: 1.58536 \nTraining set [29120/60000 (49%)] Loss: 1.60255 \nTraining set [29440/60000 (49%)] Loss: 1.64424 \nTraining set [29760/60000 (50%)] Loss: 1.54470 \nTraining set [30080/60000 (50%)] Loss: 1.65292 \nTraining set [30400/60000 (51%)] Loss: 1.57229 \nTraining set [30720/60000 (51%)] Loss: 1.65047 \nTraining set [31040/60000 (52%)] Loss: 1.61959 \nTraining set [31360/60000 (52%)] Loss: 1.64458 \nTraining set [31680/60000 (53%)] Loss: 1.60836 \nTraining set [32000/60000 (53%)] Loss: 1.61425 \nTraining set [32320/60000 (54%)] Loss: 1.60829 \nTraining set [32640/60000 (54%)] Loss: 1.56253 \nTraining set [32960/60000 (55%)] Loss: 1.62865 \nTraining set [33280/60000 (55%)] Loss: 1.58249 \nTraining set [33600/60000 (56%)] Loss: 1.60335 \nTraining set [33920/60000 (57%)] Loss: 1.60396 \nTraining set [34240/60000 (57%)] Loss: 1.64011 \nTraining set [34560/60000 (58%)] Loss: 1.64201 \nTraining set [34880/60000 (58%)] Loss: 1.62048 \nTraining set [35200/60000 (59%)] Loss: 1.67474 \nTraining set [35520/60000 (59%)] Loss: 1.57467 \nTraining set [35840/60000 (60%)] Loss: 1.63688 \nTraining set [36160/60000 (60%)] Loss: 1.65286 \nTraining set [36480/60000 (61%)] Loss: 1.58161 \nTraining set [36800/60000 (61%)] Loss: 1.54487 \nTraining set [37120/60000 (62%)] Loss: 1.53714 \nTraining set [37440/60000 (62%)] Loss: 1.64794 \nTraining set [37760/60000 (63%)] Loss: 1.61766 \nTraining set [38080/60000 (63%)] Loss: 1.56434 \nTraining set [38400/60000 (64%)] Loss: 1.68547 \nTraining set [38720/60000 (64%)] Loss: 1.59377 \nTraining set [39040/60000 (65%)] Loss: 1.63766 \nTraining set [39360/60000 (66%)] Loss: 1.62486 \nTraining set [39680/60000 (66%)] Loss: 1.69563 \nTraining set [40000/60000 (67%)] Loss: 1.70174 \nTraining set [40320/60000 (67%)] Loss: 1.60462 \nTraining set [40640/60000 (68%)] Loss: 1.60960 \nTraining set [40960/60000 (68%)] Loss: 1.59026 \nTraining set [41280/60000 (69%)] Loss: 1.58799 \nTraining set [41600/60000 (69%)] Loss: 1.63145 \nTraining set [41920/60000 (70%)] Loss: 1.57032 \nTraining set [42240/60000 (70%)] Loss: 1.66311 \nTraining set [42560/60000 (71%)] Loss: 1.65463 \nTraining set [42880/60000 (71%)] Loss: 1.56806 \nTraining set [43200/60000 (72%)] Loss: 1.57667 \nTraining set [43520/60000 (72%)] Loss: 1.57127 \nTraining set [43840/60000 (73%)] Loss: 1.65271 \nTraining set [44160/60000 (74%)] Loss: 1.57809 \nTraining set [44480/60000 (74%)] Loss: 1.59440 \nTraining set [44800/60000 (75%)] Loss: 1.59084 \nTraining set [45120/60000 (75%)] Loss: 1.59108 \nTraining set [45440/60000 (76%)] Loss: 1.59024 \nTraining set [45760/60000 (76%)] Loss: 1.60752 \nTraining set [46080/60000 (77%)] Loss: 1.59532 \nTraining set [46400/60000 (77%)] Loss: 1.66505 \nTraining set [46720/60000 (78%)] Loss: 1.70775 \nTraining set [47040/60000 (78%)] Loss: 1.63840 \nTraining set [47360/60000 (79%)] Loss: 1.60877 \nTraining set [47680/60000 (79%)] Loss: 1.62877 \nTraining set [48000/60000 (80%)] Loss: 1.60957 \nTraining set [48320/60000 (80%)] Loss: 1.58876 \nTraining set [48640/60000 (81%)] Loss: 1.58831 \nTraining set [48960/60000 (82%)] Loss: 1.59331 \nTraining set [49280/60000 (82%)] Loss: 1.63809 \nTraining set [49600/60000 (83%)] Loss: 1.56516 \nTraining set [49920/60000 (83%)] Loss: 1.60481 \nTraining set [50240/60000 (84%)] Loss: 1.63106 \nTraining set [50560/60000 (84%)] Loss: 1.61632 \nTraining set [50880/60000 (85%)] Loss: 1.62132 \nTraining set [51200/60000 (85%)] Loss: 1.72352 \nTraining set [51520/60000 (86%)] Loss: 1.64500 \nTraining set [51840/60000 (86%)] Loss: 1.62233 \nTraining set [52160/60000 (87%)] Loss: 1.59742 \nTraining set [52480/60000 (87%)] Loss: 1.72480 \nTraining set [52800/60000 (88%)] Loss: 1.61121 \nTraining set [53120/60000 (88%)] Loss: 1.60111 \nTraining set [53440/60000 (89%)] Loss: 1.57452 \nTraining set [53760/60000 (90%)] Loss: 1.67898 \nTraining set [54080/60000 (90%)] Loss: 1.62785 \nTraining set [54400/60000 (91%)] Loss: 1.63432 \nTraining set [54720/60000 (91%)] Loss: 1.55726 \nTraining set [55040/60000 (92%)] Loss: 1.62674 \nTraining set [55360/60000 (92%)] Loss: 1.63266 \nTraining set [55680/60000 (93%)] Loss: 1.65779 \nTraining set [56000/60000 (93%)] Loss: 1.60679 \nTraining set [56320/60000 (94%)] Loss: 1.61832 \nTraining set [56640/60000 (94%)] Loss: 1.56344 \nTraining set [56960/60000 (95%)] Loss: 1.66521 \nTraining set [57280/60000 (95%)] Loss: 1.65268 \nTraining set [57600/60000 (96%)] Loss: 1.59210 \nTraining set [57920/60000 (96%)] Loss: 1.61001 \nTraining set [58240/60000 (97%)] Loss: 1.61917 \nTraining set [58560/60000 (98%)] Loss: 1.67192 \nTraining set [58880/60000 (98%)] Loss: 1.62421 \nTraining set [59200/60000 (99%)] Loss: 1.56786 \nTraining set [59520/60000 (99%)] Loss: 1.56066 \nTraining set [59840/60000 (100%)] Loss: 1.53797 \nAverage Training Loss: 1.61860\nTraining Completed!\nModel Validation Starts!\nAverage Validation Loss: 1.61526, Accuracy: 8492/10000 (85%)\n\nEpoch: 5\nTraining set [0/60000 (0%)] Loss: 1.67557 \nTraining set [320/60000 (1%)] Loss: 1.57419 \nTraining set [640/60000 (1%)] Loss: 1.56514 \nTraining set [960/60000 (2%)] Loss: 1.60778 \nTraining set [1280/60000 (2%)] Loss: 1.65004 \nTraining set [1600/60000 (3%)] Loss: 1.66910 \nTraining set [1920/60000 (3%)] Loss: 1.60110 \nTraining set [2240/60000 (4%)] Loss: 1.61767 \nTraining set [2560/60000 (4%)] Loss: 1.66963 \nTraining set [2880/60000 (5%)] Loss: 1.63237 \nTraining set [3200/60000 (5%)] Loss: 1.61866 \nTraining set [3520/60000 (6%)] Loss: 1.54015 \nTraining set [3840/60000 (6%)] Loss: 1.66715 \nTraining set [4160/60000 (7%)] Loss: 1.59544 \nTraining set [4480/60000 (7%)] Loss: 1.54769 \nTraining set [4800/60000 (8%)] Loss: 1.61816 \nTraining set [5120/60000 (9%)] Loss: 1.59031 \nTraining set [5440/60000 (9%)] Loss: 1.56088 \nTraining set [5760/60000 (10%)] Loss: 1.63225 \nTraining set [6080/60000 (10%)] Loss: 1.61941 \nTraining set [6400/60000 (11%)] Loss: 1.58716 \nTraining set [6720/60000 (11%)] Loss: 1.63261 \nTraining set [7040/60000 (12%)] Loss: 1.66603 \nTraining set [7360/60000 (12%)] Loss: 1.63758 \nTraining set [7680/60000 (13%)] Loss: 1.61302 \nTraining set [8000/60000 (13%)] Loss: 1.60862 \nTraining set [8320/60000 (14%)] Loss: 1.54986 \nTraining set [8640/60000 (14%)] Loss: 1.57900 \nTraining set [8960/60000 (15%)] Loss: 1.58128 \nTraining set [9280/60000 (15%)] Loss: 1.59332 \nTraining set [9600/60000 (16%)] Loss: 1.58095 \nTraining set [9920/60000 (17%)] Loss: 1.64579 \nTraining set [10240/60000 (17%)] Loss: 1.55990 \nTraining set [10560/60000 (18%)] Loss: 1.55582 \nTraining set [10880/60000 (18%)] Loss: 1.63921 \nTraining set [11200/60000 (19%)] Loss: 1.63487 \nTraining set [11520/60000 (19%)] Loss: 1.52468 \nTraining set [11840/60000 (20%)] Loss: 1.60299 \nTraining set [12160/60000 (20%)] Loss: 1.70292 \nTraining set [12480/60000 (21%)] Loss: 1.61183 \nTraining set [12800/60000 (21%)] Loss: 1.62082 \nTraining set [13120/60000 (22%)] Loss: 1.60950 \nTraining set [13440/60000 (22%)] Loss: 1.62442 \nTraining set [13760/60000 (23%)] Loss: 1.64216 \nTraining set [14080/60000 (23%)] Loss: 1.56472 \nTraining set [14400/60000 (24%)] Loss: 1.60437 \nTraining set [14720/60000 (25%)] Loss: 1.58031 \nTraining set [15040/60000 (25%)] Loss: 1.67072 \nTraining set [15360/60000 (26%)] Loss: 1.57921 \nTraining set [15680/60000 (26%)] Loss: 1.58096 \nTraining set [16000/60000 (27%)] Loss: 1.67375 \nTraining set [16320/60000 (27%)] Loss: 1.56151 \nTraining set [16640/60000 (28%)] Loss: 1.62318 \nTraining set [16960/60000 (28%)] Loss: 1.59197 \nTraining set [17280/60000 (29%)] Loss: 1.68333 \nTraining set [17600/60000 (29%)] Loss: 1.62450 \nTraining set [17920/60000 (30%)] Loss: 1.58804 \nTraining set [18240/60000 (30%)] Loss: 1.65078 \nTraining set [18560/60000 (31%)] Loss: 1.62325 \nTraining set [18880/60000 (31%)] Loss: 1.58673 \nTraining set [19200/60000 (32%)] Loss: 1.59756 \nTraining set [19520/60000 (33%)] Loss: 1.58328 \nTraining set [19840/60000 (33%)] Loss: 1.63683 \nTraining set [20160/60000 (34%)] Loss: 1.66144 \nTraining set [20480/60000 (34%)] Loss: 1.65881 \nTraining set [20800/60000 (35%)] Loss: 1.58830 \nTraining set [21120/60000 (35%)] Loss: 1.66474 \nTraining set [21440/60000 (36%)] Loss: 1.60604 \nTraining set [21760/60000 (36%)] Loss: 1.67316 \nTraining set [22080/60000 (37%)] Loss: 1.69143 \nTraining set [22400/60000 (37%)] Loss: 1.68793 \nTraining set [22720/60000 (38%)] Loss: 1.59498 \nTraining set [23040/60000 (38%)] Loss: 1.64266 \nTraining set [23360/60000 (39%)] Loss: 1.58265 \nTraining set [23680/60000 (39%)] Loss: 1.60226 \nTraining set [24000/60000 (40%)] Loss: 1.61813 \nTraining set [24320/60000 (41%)] Loss: 1.60126 \nTraining set [24640/60000 (41%)] Loss: 1.63016 \nTraining set [24960/60000 (42%)] Loss: 1.56978 \nTraining set [25280/60000 (42%)] Loss: 1.64301 \nTraining set [25600/60000 (43%)] Loss: 1.66193 \nTraining set [25920/60000 (43%)] Loss: 1.62279 \nTraining set [26240/60000 (44%)] Loss: 1.62317 \nTraining set [26560/60000 (44%)] Loss: 1.58019 \nTraining set [26880/60000 (45%)] Loss: 1.56139 \nTraining set [27200/60000 (45%)] Loss: 1.64108 \nTraining set [27520/60000 (46%)] Loss: 1.54674 \nTraining set [27840/60000 (46%)] Loss: 1.66772 \nTraining set [28160/60000 (47%)] Loss: 1.57260 \nTraining set [28480/60000 (47%)] Loss: 1.64439 \nTraining set [28800/60000 (48%)] Loss: 1.59299 \nTraining set [29120/60000 (49%)] Loss: 1.60280 \nTraining set [29440/60000 (49%)] Loss: 1.61910 \nTraining set [29760/60000 (50%)] Loss: 1.70106 \nTraining set [30080/60000 (50%)] Loss: 1.55812 \nTraining set [30400/60000 (51%)] Loss: 1.61210 \nTraining set [30720/60000 (51%)] Loss: 1.57999 \nTraining set [31040/60000 (52%)] Loss: 1.61213 \nTraining set [31360/60000 (52%)] Loss: 1.56791 \nTraining set [31680/60000 (53%)] Loss: 1.57793 \nTraining set [32000/60000 (53%)] Loss: 1.63224 \nTraining set [32320/60000 (54%)] Loss: 1.60263 \nTraining set [32640/60000 (54%)] Loss: 1.63331 \nTraining set [32960/60000 (55%)] Loss: 1.59746 \nTraining set [33280/60000 (55%)] Loss: 1.59314 \nTraining set [33600/60000 (56%)] Loss: 1.57146 \nTraining set [33920/60000 (57%)] Loss: 1.61161 \nTraining set [34240/60000 (57%)] Loss: 1.59584 \nTraining set [34560/60000 (58%)] Loss: 1.68150 \nTraining set [34880/60000 (58%)] Loss: 1.68894 \nTraining set [35200/60000 (59%)] Loss: 1.59304 \nTraining set [35520/60000 (59%)] Loss: 1.61731 \nTraining set [35840/60000 (60%)] Loss: 1.58348 \nTraining set [36160/60000 (60%)] Loss: 1.59390 \nTraining set [36480/60000 (61%)] Loss: 1.63910 \nTraining set [36800/60000 (61%)] Loss: 1.61129 \nTraining set [37120/60000 (62%)] Loss: 1.58266 \nTraining set [37440/60000 (62%)] Loss: 1.54841 \nTraining set [37760/60000 (63%)] Loss: 1.60053 \nTraining set [38080/60000 (63%)] Loss: 1.60795 \nTraining set [38400/60000 (64%)] Loss: 1.57246 \nTraining set [38720/60000 (64%)] Loss: 1.58203 \nTraining set [39040/60000 (65%)] Loss: 1.59004 \nTraining set [39360/60000 (66%)] Loss: 1.60550 \nTraining set [39680/60000 (66%)] Loss: 1.60130 \nTraining set [40000/60000 (67%)] Loss: 1.63744 \nTraining set [40320/60000 (67%)] Loss: 1.61698 \nTraining set [40640/60000 (68%)] Loss: 1.60722 \nTraining set [40960/60000 (68%)] Loss: 1.60974 \nTraining set [41280/60000 (69%)] Loss: 1.55623 \nTraining set [41600/60000 (69%)] Loss: 1.57446 \nTraining set [41920/60000 (70%)] Loss: 1.63111 \nTraining set [42240/60000 (70%)] Loss: 1.64469 \nTraining set [42560/60000 (71%)] Loss: 1.59450 \nTraining set [42880/60000 (71%)] Loss: 1.60686 \nTraining set [43200/60000 (72%)] Loss: 1.56893 \nTraining set [43520/60000 (72%)] Loss: 1.60289 \nTraining set [43840/60000 (73%)] Loss: 1.56879 \nTraining set [44160/60000 (74%)] Loss: 1.55913 \nTraining set [44480/60000 (74%)] Loss: 1.60115 \nTraining set [44800/60000 (75%)] Loss: 1.54978 \nTraining set [45120/60000 (75%)] Loss: 1.68520 \nTraining set [45440/60000 (76%)] Loss: 1.58798 \nTraining set [45760/60000 (76%)] Loss: 1.63817 \nTraining set [46080/60000 (77%)] Loss: 1.60743 \nTraining set [46400/60000 (77%)] Loss: 1.53570 \nTraining set [46720/60000 (78%)] Loss: 1.59584 \nTraining set [47040/60000 (78%)] Loss: 1.57607 \nTraining set [47360/60000 (79%)] Loss: 1.57029 \nTraining set [47680/60000 (79%)] Loss: 1.64818 \nTraining set [48000/60000 (80%)] Loss: 1.63863 \nTraining set [48320/60000 (80%)] Loss: 1.54616 \nTraining set [48640/60000 (81%)] Loss: 1.67306 \nTraining set [48960/60000 (82%)] Loss: 1.51095 \nTraining set [49280/60000 (82%)] Loss: 1.55239 \nTraining set [49600/60000 (83%)] Loss: 1.56190 \nTraining set [49920/60000 (83%)] Loss: 1.62903 \nTraining set [50240/60000 (84%)] Loss: 1.58213 \nTraining set [50560/60000 (84%)] Loss: 1.57969 \nTraining set [50880/60000 (85%)] Loss: 1.64102 \nTraining set [51200/60000 (85%)] Loss: 1.57382 \nTraining set [51520/60000 (86%)] Loss: 1.58744 \nTraining set [51840/60000 (86%)] Loss: 1.69886 \nTraining set [52160/60000 (87%)] Loss: 1.56579 \nTraining set [52480/60000 (87%)] Loss: 1.61600 \nTraining set [52800/60000 (88%)] Loss: 1.57861 \nTraining set [53120/60000 (88%)] Loss: 1.59712 \nTraining set [53440/60000 (89%)] Loss: 1.60778 \nTraining set [53760/60000 (90%)] Loss: 1.61225 \nTraining set [54080/60000 (90%)] Loss: 1.68102 \nTraining set [54400/60000 (91%)] Loss: 1.62360 \nTraining set [54720/60000 (91%)] Loss: 1.57077 \nTraining set [55040/60000 (92%)] Loss: 1.59448 \nTraining set [55360/60000 (92%)] Loss: 1.62184 \nTraining set [55680/60000 (93%)] Loss: 1.65268 \nTraining set [56000/60000 (93%)] Loss: 1.53820 \nTraining set [56320/60000 (94%)] Loss: 1.60724 \nTraining set [56640/60000 (94%)] Loss: 1.64376 \nTraining set [56960/60000 (95%)] Loss: 1.60993 \nTraining set [57280/60000 (95%)] Loss: 1.63460 \nTraining set [57600/60000 (96%)] Loss: 1.60265 \nTraining set [57920/60000 (96%)] Loss: 1.61002 \nTraining set [58240/60000 (97%)] Loss: 1.55210 \nTraining set [58560/60000 (98%)] Loss: 1.57192 \nTraining set [58880/60000 (98%)] Loss: 1.61090 \nTraining set [59200/60000 (99%)] Loss: 1.58148 \nTraining set [59520/60000 (99%)] Loss: 1.59675 \nTraining set [59840/60000 (100%)] Loss: 1.55413 \nAverage Training Loss: 1.60861\nTraining Completed!\nModel Validation Starts!\nAverage Validation Loss: 1.61353, Accuracy: 8499/10000 (85%)\n\nEpoch: 6\nTraining set [0/60000 (0%)] Loss: 1.67399 \nTraining set [320/60000 (1%)] Loss: 1.58765 \nTraining set [640/60000 (1%)] Loss: 1.59289 \nTraining set [960/60000 (2%)] Loss: 1.60989 \nTraining set [1280/60000 (2%)] Loss: 1.55886 \nTraining set [1600/60000 (3%)] Loss: 1.59497 \nTraining set [1920/60000 (3%)] Loss: 1.56844 \nTraining set [2240/60000 (4%)] Loss: 1.65160 \nTraining set [2560/60000 (4%)] Loss: 1.59295 \nTraining set [2880/60000 (5%)] Loss: 1.54733 \nTraining set [3200/60000 (5%)] Loss: 1.61350 \nTraining set [3520/60000 (6%)] Loss: 1.63881 \nTraining set [3840/60000 (6%)] Loss: 1.58291 \nTraining set [4160/60000 (7%)] Loss: 1.58953 \nTraining set [4480/60000 (7%)] Loss: 1.60370 \nTraining set [4800/60000 (8%)] Loss: 1.58213 \nTraining set [5120/60000 (9%)] Loss: 1.56303 \nTraining set [5440/60000 (9%)] Loss: 1.56685 \nTraining set [5760/60000 (10%)] Loss: 1.59157 \nTraining set [6080/60000 (10%)] Loss: 1.64647 \nTraining set [6400/60000 (11%)] Loss: 1.67438 \nTraining set [6720/60000 (11%)] Loss: 1.63389 \nTraining set [7040/60000 (12%)] Loss: 1.61105 \nTraining set [7360/60000 (12%)] Loss: 1.61822 \nTraining set [7680/60000 (13%)] Loss: 1.62649 \nTraining set [8000/60000 (13%)] Loss: 1.60107 \nTraining set [8320/60000 (14%)] Loss: 1.57977 \nTraining set [8640/60000 (14%)] Loss: 1.58823 \nTraining set [8960/60000 (15%)] Loss: 1.54985 \nTraining set [9280/60000 (15%)] Loss: 1.64407 \nTraining set [9600/60000 (16%)] Loss: 1.59727 \nTraining set [9920/60000 (17%)] Loss: 1.59861 \nTraining set [10240/60000 (17%)] Loss: 1.57434 \nTraining set [10560/60000 (18%)] Loss: 1.62492 \nTraining set [10880/60000 (18%)] Loss: 1.58059 \nTraining set [11200/60000 (19%)] Loss: 1.67813 \nTraining set [11520/60000 (19%)] Loss: 1.58638 \nTraining set [11840/60000 (20%)] Loss: 1.56068 \nTraining set [12160/60000 (20%)] Loss: 1.54569 \nTraining set [12480/60000 (21%)] Loss: 1.52918 \nTraining set [12800/60000 (21%)] Loss: 1.61432 \nTraining set [13120/60000 (22%)] Loss: 1.64740 \nTraining set [13440/60000 (22%)] Loss: 1.58903 \nTraining set [13760/60000 (23%)] Loss: 1.69694 \nTraining set [14080/60000 (23%)] Loss: 1.56475 \nTraining set [14400/60000 (24%)] Loss: 1.58550 \nTraining set [14720/60000 (25%)] Loss: 1.62986 \nTraining set [15040/60000 (25%)] Loss: 1.56330 \nTraining set [15360/60000 (26%)] Loss: 1.58304 \nTraining set [15680/60000 (26%)] Loss: 1.66981 \nTraining set [16000/60000 (27%)] Loss: 1.62969 \nTraining set [16320/60000 (27%)] Loss: 1.64558 \nTraining set [16640/60000 (28%)] Loss: 1.58107 \nTraining set [16960/60000 (28%)] Loss: 1.57420 \nTraining set [17280/60000 (29%)] Loss: 1.60111 \nTraining set [17600/60000 (29%)] Loss: 1.57530 \nTraining set [17920/60000 (30%)] Loss: 1.56136 \nTraining set [18240/60000 (30%)] Loss: 1.57130 \nTraining set [18560/60000 (31%)] Loss: 1.60397 \nTraining set [18880/60000 (31%)] Loss: 1.53884 \nTraining set [19200/60000 (32%)] Loss: 1.63685 \nTraining set [19520/60000 (33%)] Loss: 1.62258 \nTraining set [19840/60000 (33%)] Loss: 1.55118 \nTraining set [20160/60000 (34%)] Loss: 1.57304 \nTraining set [20480/60000 (34%)] Loss: 1.64411 \nTraining set [20800/60000 (35%)] Loss: 1.58968 \nTraining set [21120/60000 (35%)] Loss: 1.57905 \nTraining set [21440/60000 (36%)] Loss: 1.61138 \nTraining set [21760/60000 (36%)] Loss: 1.62127 \nTraining set [22080/60000 (37%)] Loss: 1.55740 \nTraining set [22400/60000 (37%)] Loss: 1.62451 \nTraining set [22720/60000 (38%)] Loss: 1.53627 \nTraining set [23040/60000 (38%)] Loss: 1.51793 \nTraining set [23360/60000 (39%)] Loss: 1.58071 \nTraining set [23680/60000 (39%)] Loss: 1.59166 \nTraining set [24000/60000 (40%)] Loss: 1.57481 \nTraining set [24320/60000 (41%)] Loss: 1.59640 \nTraining set [24640/60000 (41%)] Loss: 1.60785 \nTraining set [24960/60000 (42%)] Loss: 1.60231 \nTraining set [25280/60000 (42%)] Loss: 1.56564 \nTraining set [25600/60000 (43%)] Loss: 1.63856 \nTraining set [25920/60000 (43%)] Loss: 1.59855 \nTraining set [26240/60000 (44%)] Loss: 1.59161 \nTraining set [26560/60000 (44%)] Loss: 1.59676 \nTraining set [26880/60000 (45%)] Loss: 1.62909 \nTraining set [27200/60000 (45%)] Loss: 1.58471 \nTraining set [27520/60000 (46%)] Loss: 1.55493 \nTraining set [27840/60000 (46%)] Loss: 1.64943 \nTraining set [28160/60000 (47%)] Loss: 1.60777 \nTraining set [28480/60000 (47%)] Loss: 1.54574 \nTraining set [28800/60000 (48%)] Loss: 1.61268 \nTraining set [29120/60000 (49%)] Loss: 1.56047 \nTraining set [29440/60000 (49%)] Loss: 1.55164 \nTraining set [29760/60000 (50%)] Loss: 1.55159 \nTraining set [30080/60000 (50%)] Loss: 1.56032 \nTraining set [30400/60000 (51%)] Loss: 1.56794 \nTraining set [30720/60000 (51%)] Loss: 1.57151 \nTraining set [31040/60000 (52%)] Loss: 1.60522 \nTraining set [31360/60000 (52%)] Loss: 1.59834 \nTraining set [31680/60000 (53%)] Loss: 1.62775 \nTraining set [32000/60000 (53%)] Loss: 1.60944 \nTraining set [32320/60000 (54%)] Loss: 1.57837 \nTraining set [32640/60000 (54%)] Loss: 1.58131 \nTraining set [32960/60000 (55%)] Loss: 1.63377 \nTraining set [33280/60000 (55%)] Loss: 1.53779 \nTraining set [33600/60000 (56%)] Loss: 1.66005 \nTraining set [33920/60000 (57%)] Loss: 1.60689 \nTraining set [34240/60000 (57%)] Loss: 1.58139 \nTraining set [34560/60000 (58%)] Loss: 1.57995 \nTraining set [34880/60000 (58%)] Loss: 1.61474 \nTraining set [35200/60000 (59%)] Loss: 1.56820 \nTraining set [35520/60000 (59%)] Loss: 1.56306 \nTraining set [35840/60000 (60%)] Loss: 1.61039 \nTraining set [36160/60000 (60%)] Loss: 1.62520 \nTraining set [36480/60000 (61%)] Loss: 1.61538 \nTraining set [36800/60000 (61%)] Loss: 1.55071 \nTraining set [37120/60000 (62%)] Loss: 1.60074 \nTraining set [37440/60000 (62%)] Loss: 1.55818 \nTraining set [37760/60000 (63%)] Loss: 1.56071 \nTraining set [38080/60000 (63%)] Loss: 1.55948 \nTraining set [38400/60000 (64%)] Loss: 1.57379 \nTraining set [38720/60000 (64%)] Loss: 1.58840 \nTraining set [39040/60000 (65%)] Loss: 1.59279 \nTraining set [39360/60000 (66%)] Loss: 1.63383 \nTraining set [39680/60000 (66%)] Loss: 1.61611 \nTraining set [40000/60000 (67%)] Loss: 1.57135 \nTraining set [40320/60000 (67%)] Loss: 1.60086 \nTraining set [40640/60000 (68%)] Loss: 1.60163 \nTraining set [40960/60000 (68%)] Loss: 1.55282 \nTraining set [41280/60000 (69%)] Loss: 1.52252 \nTraining set [41600/60000 (69%)] Loss: 1.68269 \nTraining set [41920/60000 (70%)] Loss: 1.60869 \nTraining set [42240/60000 (70%)] Loss: 1.61917 \nTraining set [42560/60000 (71%)] Loss: 1.56031 \nTraining set [42880/60000 (71%)] Loss: 1.61665 \nTraining set [43200/60000 (72%)] Loss: 1.61664 \nTraining set [43520/60000 (72%)] Loss: 1.53601 \nTraining set [43840/60000 (73%)] Loss: 1.60912 \nTraining set [44160/60000 (74%)] Loss: 1.55520 \nTraining set [44480/60000 (74%)] Loss: 1.62653 \nTraining set [44800/60000 (75%)] Loss: 1.60078 \nTraining set [45120/60000 (75%)] Loss: 1.62901 \nTraining set [45440/60000 (76%)] Loss: 1.62724 \nTraining set [45760/60000 (76%)] Loss: 1.57452 \nTraining set [46080/60000 (77%)] Loss: 1.61169 \nTraining set [46400/60000 (77%)] Loss: 1.57472 \nTraining set [46720/60000 (78%)] Loss: 1.67447 \nTraining set [47040/60000 (78%)] Loss: 1.63363 \nTraining set [47360/60000 (79%)] Loss: 1.59211 \nTraining set [47680/60000 (79%)] Loss: 1.62255 \nTraining set [48000/60000 (80%)] Loss: 1.53914 \nTraining set [48320/60000 (80%)] Loss: 1.63082 \nTraining set [48640/60000 (81%)] Loss: 1.59953 \nTraining set [48960/60000 (82%)] Loss: 1.62988 \nTraining set [49280/60000 (82%)] Loss: 1.63213 \nTraining set [49600/60000 (83%)] Loss: 1.58485 \nTraining set [49920/60000 (83%)] Loss: 1.61937 \nTraining set [50240/60000 (84%)] Loss: 1.61813 \nTraining set [50560/60000 (84%)] Loss: 1.59854 \nTraining set [50880/60000 (85%)] Loss: 1.56383 \nTraining set [51200/60000 (85%)] Loss: 1.58726 \nTraining set [51520/60000 (86%)] Loss: 1.60845 \nTraining set [51840/60000 (86%)] Loss: 1.64744 \nTraining set [52160/60000 (87%)] Loss: 1.56720 \nTraining set [52480/60000 (87%)] Loss: 1.56226 \nTraining set [52800/60000 (88%)] Loss: 1.69952 \nTraining set [53120/60000 (88%)] Loss: 1.64349 \nTraining set [53440/60000 (89%)] Loss: 1.59557 \nTraining set [53760/60000 (90%)] Loss: 1.60135 \nTraining set [54080/60000 (90%)] Loss: 1.60563 \nTraining set [54400/60000 (91%)] Loss: 1.57242 \nTraining set [54720/60000 (91%)] Loss: 1.56088 \nTraining set [55040/60000 (92%)] Loss: 1.59873 \nTraining set [55360/60000 (92%)] Loss: 1.63440 \nTraining set [55680/60000 (93%)] Loss: 1.60009 \nTraining set [56000/60000 (93%)] Loss: 1.58315 \nTraining set [56320/60000 (94%)] Loss: 1.61352 \nTraining set [56640/60000 (94%)] Loss: 1.63288 \nTraining set [56960/60000 (95%)] Loss: 1.66227 \nTraining set [57280/60000 (95%)] Loss: 1.58808 \nTraining set [57600/60000 (96%)] Loss: 1.59599 \nTraining set [57920/60000 (96%)] Loss: 1.57284 \nTraining set [58240/60000 (97%)] Loss: 1.59276 \nTraining set [58560/60000 (98%)] Loss: 1.58514 \nTraining set [58880/60000 (98%)] Loss: 1.53210 \nTraining set [59200/60000 (99%)] Loss: 1.61593 \nTraining set [59520/60000 (99%)] Loss: 1.64406 \nTraining set [59840/60000 (100%)] Loss: 1.61227 \nAverage Training Loss: 1.59991\nTraining Completed!\nModel Validation Starts!\nAverage Validation Loss: 1.60801, Accuracy: 8555/10000 (86%)\n\nEpoch: 7\nTraining set [0/60000 (0%)] Loss: 1.63344 \nTraining set [320/60000 (1%)] Loss: 1.62694 \nTraining set [640/60000 (1%)] Loss: 1.60549 \nTraining set [960/60000 (2%)] Loss: 1.61330 \nTraining set [1280/60000 (2%)] Loss: 1.56307 \nTraining set [1600/60000 (3%)] Loss: 1.60414 \nTraining set [1920/60000 (3%)] Loss: 1.57913 \nTraining set [2240/60000 (4%)] Loss: 1.59346 \nTraining set [2560/60000 (4%)] Loss: 1.57247 \nTraining set [2880/60000 (5%)] Loss: 1.58026 \nTraining set [3200/60000 (5%)] Loss: 1.58327 \nTraining set [3520/60000 (6%)] Loss: 1.63306 \nTraining set [3840/60000 (6%)] Loss: 1.61475 \nTraining set [4160/60000 (7%)] Loss: 1.62547 \nTraining set [4480/60000 (7%)] Loss: 1.50609 \nTraining set [4800/60000 (8%)] Loss: 1.60427 \nTraining set [5120/60000 (9%)] Loss: 1.56876 \nTraining set [5440/60000 (9%)] Loss: 1.60855 \nTraining set [5760/60000 (10%)] Loss: 1.60751 \nTraining set [6080/60000 (10%)] Loss: 1.62438 \nTraining set [6400/60000 (11%)] Loss: 1.56718 \nTraining set [6720/60000 (11%)] Loss: 1.54208 \nTraining set [7040/60000 (12%)] Loss: 1.60817 \nTraining set [7360/60000 (12%)] Loss: 1.59483 \nTraining set [7680/60000 (13%)] Loss: 1.61005 \nTraining set [8000/60000 (13%)] Loss: 1.57890 \nTraining set [8320/60000 (14%)] Loss: 1.54338 \nTraining set [8640/60000 (14%)] Loss: 1.56639 \nTraining set [8960/60000 (15%)] Loss: 1.59278 \nTraining set [9280/60000 (15%)] Loss: 1.61049 \nTraining set [9600/60000 (16%)] Loss: 1.52884 \nTraining set [9920/60000 (17%)] Loss: 1.57627 \nTraining set [10240/60000 (17%)] Loss: 1.59787 \nTraining set [10560/60000 (18%)] Loss: 1.65231 \nTraining set [10880/60000 (18%)] Loss: 1.56053 \nTraining set [11200/60000 (19%)] Loss: 1.63709 \nTraining set [11520/60000 (19%)] Loss: 1.61727 \nTraining set [11840/60000 (20%)] Loss: 1.59638 \nTraining set [12160/60000 (20%)] Loss: 1.62148 \nTraining set [12480/60000 (21%)] Loss: 1.58089 \nTraining set [12800/60000 (21%)] Loss: 1.56028 \nTraining set [13120/60000 (22%)] Loss: 1.57684 \nTraining set [13440/60000 (22%)] Loss: 1.54551 \nTraining set [13760/60000 (23%)] Loss: 1.65135 \nTraining set [14080/60000 (23%)] Loss: 1.60301 \nTraining set [14400/60000 (24%)] Loss: 1.54051 \nTraining set [14720/60000 (25%)] Loss: 1.60170 \nTraining set [15040/60000 (25%)] Loss: 1.61266 \nTraining set [15360/60000 (26%)] Loss: 1.57280 \nTraining set [15680/60000 (26%)] Loss: 1.63335 \nTraining set [16000/60000 (27%)] Loss: 1.61371 \nTraining set [16320/60000 (27%)] Loss: 1.59963 \nTraining set [16640/60000 (28%)] Loss: 1.61765 \nTraining set [16960/60000 (28%)] Loss: 1.59943 \nTraining set [17280/60000 (29%)] Loss: 1.56457 \nTraining set [17600/60000 (29%)] Loss: 1.59365 \nTraining set [17920/60000 (30%)] Loss: 1.63977 \nTraining set [18240/60000 (30%)] Loss: 1.56811 \nTraining set [18560/60000 (31%)] Loss: 1.63911 \nTraining set [18880/60000 (31%)] Loss: 1.61098 \nTraining set [19200/60000 (32%)] Loss: 1.58038 \nTraining set [19520/60000 (33%)] Loss: 1.55940 \nTraining set [19840/60000 (33%)] Loss: 1.55555 \nTraining set [20160/60000 (34%)] Loss: 1.57436 \nTraining set [20480/60000 (34%)] Loss: 1.58941 \nTraining set [20800/60000 (35%)] Loss: 1.70774 \nTraining set [21120/60000 (35%)] Loss: 1.60862 \nTraining set [21440/60000 (36%)] Loss: 1.61008 \nTraining set [21760/60000 (36%)] Loss: 1.58081 \nTraining set [22080/60000 (37%)] Loss: 1.56333 \nTraining set [22400/60000 (37%)] Loss: 1.52940 \nTraining set [22720/60000 (38%)] Loss: 1.61452 \nTraining set [23040/60000 (38%)] Loss: 1.61287 \nTraining set [23360/60000 (39%)] Loss: 1.50815 \nTraining set [23680/60000 (39%)] Loss: 1.66013 \nTraining set [24000/60000 (40%)] Loss: 1.59238 \nTraining set [24320/60000 (41%)] Loss: 1.59351 \nTraining set [24640/60000 (41%)] Loss: 1.61272 \nTraining set [24960/60000 (42%)] Loss: 1.63474 \nTraining set [25280/60000 (42%)] Loss: 1.65536 \nTraining set [25600/60000 (43%)] Loss: 1.61643 \nTraining set [25920/60000 (43%)] Loss: 1.58485 \nTraining set [26240/60000 (44%)] Loss: 1.59523 \nTraining set [26560/60000 (44%)] Loss: 1.52239 \nTraining set [26880/60000 (45%)] Loss: 1.59045 \nTraining set [27200/60000 (45%)] Loss: 1.50539 \nTraining set [27520/60000 (46%)] Loss: 1.64996 \nTraining set [27840/60000 (46%)] Loss: 1.61571 \nTraining set [28160/60000 (47%)] Loss: 1.54289 \nTraining set [28480/60000 (47%)] Loss: 1.64740 \nTraining set [28800/60000 (48%)] Loss: 1.63746 \nTraining set [29120/60000 (49%)] Loss: 1.50137 \nTraining set [29440/60000 (49%)] Loss: 1.63834 \nTraining set [29760/60000 (50%)] Loss: 1.59467 \nTraining set [30080/60000 (50%)] Loss: 1.55316 \nTraining set [30400/60000 (51%)] Loss: 1.68790 \nTraining set [30720/60000 (51%)] Loss: 1.66366 \nTraining set [31040/60000 (52%)] Loss: 1.64178 \nTraining set [31360/60000 (52%)] Loss: 1.59964 \nTraining set [31680/60000 (53%)] Loss: 1.58796 \nTraining set [32000/60000 (53%)] Loss: 1.56246 \nTraining set [32320/60000 (54%)] Loss: 1.59346 \nTraining set [32640/60000 (54%)] Loss: 1.58764 \nTraining set [32960/60000 (55%)] Loss: 1.57044 \nTraining set [33280/60000 (55%)] Loss: 1.58935 \nTraining set [33600/60000 (56%)] Loss: 1.58386 \nTraining set [33920/60000 (57%)] Loss: 1.58144 \nTraining set [34240/60000 (57%)] Loss: 1.55914 \nTraining set [34560/60000 (58%)] Loss: 1.51724 \nTraining set [34880/60000 (58%)] Loss: 1.58853 \nTraining set [35200/60000 (59%)] Loss: 1.64422 \nTraining set [35520/60000 (59%)] Loss: 1.55726 \nTraining set [35840/60000 (60%)] Loss: 1.55511 \nTraining set [36160/60000 (60%)] Loss: 1.61922 \nTraining set [36480/60000 (61%)] Loss: 1.50637 \nTraining set [36800/60000 (61%)] Loss: 1.61412 \nTraining set [37120/60000 (62%)] Loss: 1.56485 \nTraining set [37440/60000 (62%)] Loss: 1.59903 \nTraining set [37760/60000 (63%)] Loss: 1.60322 \nTraining set [38080/60000 (63%)] Loss: 1.58539 \nTraining set [38400/60000 (64%)] Loss: 1.54753 \nTraining set [38720/60000 (64%)] Loss: 1.56595 \nTraining set [39040/60000 (65%)] Loss: 1.60785 \nTraining set [39360/60000 (66%)] Loss: 1.65211 \nTraining set [39680/60000 (66%)] Loss: 1.60517 \nTraining set [40000/60000 (67%)] Loss: 1.54694 \nTraining set [40320/60000 (67%)] Loss: 1.59141 \nTraining set [40640/60000 (68%)] Loss: 1.60457 \nTraining set [40960/60000 (68%)] Loss: 1.63555 \nTraining set [41280/60000 (69%)] Loss: 1.60817 \nTraining set [41600/60000 (69%)] Loss: 1.54639 \nTraining set [41920/60000 (70%)] Loss: 1.59550 \nTraining set [42240/60000 (70%)] Loss: 1.60025 \nTraining set [42560/60000 (71%)] Loss: 1.55395 \nTraining set [42880/60000 (71%)] Loss: 1.70987 \nTraining set [43200/60000 (72%)] Loss: 1.64106 \nTraining set [43520/60000 (72%)] Loss: 1.67131 \nTraining set [43840/60000 (73%)] Loss: 1.56716 \nTraining set [44160/60000 (74%)] Loss: 1.68041 \nTraining set [44480/60000 (74%)] Loss: 1.69894 \nTraining set [44800/60000 (75%)] Loss: 1.57069 \nTraining set [45120/60000 (75%)] Loss: 1.55254 \nTraining set [45440/60000 (76%)] Loss: 1.55238 \nTraining set [45760/60000 (76%)] Loss: 1.51172 \nTraining set [46080/60000 (77%)] Loss: 1.51505 \nTraining set [46400/60000 (77%)] Loss: 1.64245 \nTraining set [46720/60000 (78%)] Loss: 1.53704 \nTraining set [47040/60000 (78%)] Loss: 1.62004 \nTraining set [47360/60000 (79%)] Loss: 1.59871 \nTraining set [47680/60000 (79%)] Loss: 1.57240 \nTraining set [48000/60000 (80%)] Loss: 1.59857 \nTraining set [48320/60000 (80%)] Loss: 1.52616 \nTraining set [48640/60000 (81%)] Loss: 1.60544 \nTraining set [48960/60000 (82%)] Loss: 1.63415 \nTraining set [49280/60000 (82%)] Loss: 1.57043 \nTraining set [49600/60000 (83%)] Loss: 1.56615 \nTraining set [49920/60000 (83%)] Loss: 1.65619 \nTraining set [50240/60000 (84%)] Loss: 1.57090 \nTraining set [50560/60000 (84%)] Loss: 1.64249 \nTraining set [50880/60000 (85%)] Loss: 1.63222 \nTraining set [51200/60000 (85%)] Loss: 1.54529 \nTraining set [51520/60000 (86%)] Loss: 1.56613 \nTraining set [51840/60000 (86%)] Loss: 1.59189 \nTraining set [52160/60000 (87%)] Loss: 1.62721 \nTraining set [52480/60000 (87%)] Loss: 1.50800 \nTraining set [52800/60000 (88%)] Loss: 1.57565 \nTraining set [53120/60000 (88%)] Loss: 1.62178 \nTraining set [53440/60000 (89%)] Loss: 1.57533 \nTraining set [53760/60000 (90%)] Loss: 1.58006 \nTraining set [54080/60000 (90%)] Loss: 1.58786 \nTraining set [54400/60000 (91%)] Loss: 1.60790 \nTraining set [54720/60000 (91%)] Loss: 1.54778 \nTraining set [55040/60000 (92%)] Loss: 1.65058 \nTraining set [55360/60000 (92%)] Loss: 1.55702 \nTraining set [55680/60000 (93%)] Loss: 1.55026 \nTraining set [56000/60000 (93%)] Loss: 1.64554 \nTraining set [56320/60000 (94%)] Loss: 1.57711 \nTraining set [56640/60000 (94%)] Loss: 1.56335 \nTraining set [56960/60000 (95%)] Loss: 1.61387 \nTraining set [57280/60000 (95%)] Loss: 1.60386 \nTraining set [57600/60000 (96%)] Loss: 1.56820 \nTraining set [57920/60000 (96%)] Loss: 1.61958 \nTraining set [58240/60000 (97%)] Loss: 1.53544 \nTraining set [58560/60000 (98%)] Loss: 1.58974 \nTraining set [58880/60000 (98%)] Loss: 1.57611 \nTraining set [59200/60000 (99%)] Loss: 1.62054 \nTraining set [59520/60000 (99%)] Loss: 1.52151 \nTraining set [59840/60000 (100%)] Loss: 1.62641 \nAverage Training Loss: 1.59419\nTraining Completed!\nModel Validation Starts!\nAverage Validation Loss: 1.60600, Accuracy: 8571/10000 (86%)\n\nEpoch: 8\nTraining set [0/60000 (0%)] Loss: 1.57440 \nTraining set [320/60000 (1%)] Loss: 1.63025 \nTraining set [640/60000 (1%)] Loss: 1.61588 \nTraining set [960/60000 (2%)] Loss: 1.64137 \nTraining set [1280/60000 (2%)] Loss: 1.64796 \nTraining set [1600/60000 (3%)] Loss: 1.64948 \nTraining set [1920/60000 (3%)] Loss: 1.58588 \nTraining set [2240/60000 (4%)] Loss: 1.59918 \nTraining set [2560/60000 (4%)] Loss: 1.49990 \nTraining set [2880/60000 (5%)] Loss: 1.59414 \nTraining set [3200/60000 (5%)] Loss: 1.57252 \nTraining set [3520/60000 (6%)] Loss: 1.56907 \nTraining set [3840/60000 (6%)] Loss: 1.66059 \nTraining set [4160/60000 (7%)] Loss: 1.57784 \nTraining set [4480/60000 (7%)] Loss: 1.60901 \nTraining set [4800/60000 (8%)] Loss: 1.51234 \nTraining set [5120/60000 (9%)] Loss: 1.61940 \nTraining set [5440/60000 (9%)] Loss: 1.63518 \nTraining set [5760/60000 (10%)] Loss: 1.61368 \nTraining set [6080/60000 (10%)] Loss: 1.57152 \nTraining set [6400/60000 (11%)] Loss: 1.53938 \nTraining set [6720/60000 (11%)] Loss: 1.55954 \nTraining set [7040/60000 (12%)] Loss: 1.62216 \nTraining set [7360/60000 (12%)] Loss: 1.57100 \nTraining set [7680/60000 (13%)] Loss: 1.62200 \nTraining set [8000/60000 (13%)] Loss: 1.51723 \nTraining set [8320/60000 (14%)] Loss: 1.55521 \nTraining set [8640/60000 (14%)] Loss: 1.58297 \nTraining set [8960/60000 (15%)] Loss: 1.60171 \nTraining set [9280/60000 (15%)] Loss: 1.54953 \nTraining set [9600/60000 (16%)] Loss: 1.56859 \nTraining set [9920/60000 (17%)] Loss: 1.62969 \nTraining set [10240/60000 (17%)] Loss: 1.67001 \nTraining set [10560/60000 (18%)] Loss: 1.58814 \nTraining set [10880/60000 (18%)] Loss: 1.63645 \nTraining set [11200/60000 (19%)] Loss: 1.61659 \nTraining set [11520/60000 (19%)] Loss: 1.62671 \nTraining set [11840/60000 (20%)] Loss: 1.55084 \nTraining set [12160/60000 (20%)] Loss: 1.60297 \nTraining set [12480/60000 (21%)] Loss: 1.61673 \nTraining set [12800/60000 (21%)] Loss: 1.62557 \nTraining set [13120/60000 (22%)] Loss: 1.52232 \nTraining set [13440/60000 (22%)] Loss: 1.60379 \nTraining set [13760/60000 (23%)] Loss: 1.58945 \nTraining set [14080/60000 (23%)] Loss: 1.62202 \nTraining set [14400/60000 (24%)] Loss: 1.56918 \nTraining set [14720/60000 (25%)] Loss: 1.53597 \nTraining set [15040/60000 (25%)] Loss: 1.55347 \nTraining set [15360/60000 (26%)] Loss: 1.55148 \nTraining set [15680/60000 (26%)] Loss: 1.57460 \nTraining set [16000/60000 (27%)] Loss: 1.57307 \nTraining set [16320/60000 (27%)] Loss: 1.55872 \nTraining set [16640/60000 (28%)] Loss: 1.59114 \nTraining set [16960/60000 (28%)] Loss: 1.61556 \nTraining set [17280/60000 (29%)] Loss: 1.53365 \nTraining set [17600/60000 (29%)] Loss: 1.57259 \nTraining set [17920/60000 (30%)] Loss: 1.52401 \nTraining set [18240/60000 (30%)] Loss: 1.61471 \nTraining set [18560/60000 (31%)] Loss: 1.62754 \nTraining set [18880/60000 (31%)] Loss: 1.58237 \nTraining set [19200/60000 (32%)] Loss: 1.64484 \nTraining set [19520/60000 (33%)] Loss: 1.56799 \nTraining set [19840/60000 (33%)] Loss: 1.58803 \nTraining set [20160/60000 (34%)] Loss: 1.52545 \nTraining set [20480/60000 (34%)] Loss: 1.56368 \nTraining set [20800/60000 (35%)] Loss: 1.56357 \nTraining set [21120/60000 (35%)] Loss: 1.57905 \nTraining set [21440/60000 (36%)] Loss: 1.60080 \nTraining set [21760/60000 (36%)] Loss: 1.56355 \nTraining set [22080/60000 (37%)] Loss: 1.58525 \nTraining set [22400/60000 (37%)] Loss: 1.53127 \nTraining set [22720/60000 (38%)] Loss: 1.56401 \nTraining set [23040/60000 (38%)] Loss: 1.61481 \nTraining set [23360/60000 (39%)] Loss: 1.56459 \nTraining set [23680/60000 (39%)] Loss: 1.58551 \nTraining set [24000/60000 (40%)] Loss: 1.57867 \nTraining set [24320/60000 (41%)] Loss: 1.71805 \nTraining set [24640/60000 (41%)] Loss: 1.58224 \nTraining set [24960/60000 (42%)] Loss: 1.54843 \nTraining set [25280/60000 (42%)] Loss: 1.59183 \nTraining set [25600/60000 (43%)] Loss: 1.55145 \nTraining set [25920/60000 (43%)] Loss: 1.57938 \nTraining set [26240/60000 (44%)] Loss: 1.59747 \nTraining set [26560/60000 (44%)] Loss: 1.67763 \nTraining set [26880/60000 (45%)] Loss: 1.57458 \nTraining set [27200/60000 (45%)] Loss: 1.55595 \nTraining set [27520/60000 (46%)] Loss: 1.57246 \nTraining set [27840/60000 (46%)] Loss: 1.53958 \nTraining set [28160/60000 (47%)] Loss: 1.57840 \nTraining set [28480/60000 (47%)] Loss: 1.57341 \nTraining set [28800/60000 (48%)] Loss: 1.51016 \nTraining set [29120/60000 (49%)] Loss: 1.67586 \nTraining set [29440/60000 (49%)] Loss: 1.64913 \nTraining set [29760/60000 (50%)] Loss: 1.59967 \nTraining set [30080/60000 (50%)] Loss: 1.58432 \nTraining set [30400/60000 (51%)] Loss: 1.65364 \nTraining set [30720/60000 (51%)] Loss: 1.62880 \nTraining set [31040/60000 (52%)] Loss: 1.60201 \nTraining set [31360/60000 (52%)] Loss: 1.57070 \nTraining set [31680/60000 (53%)] Loss: 1.55943 \nTraining set [32000/60000 (53%)] Loss: 1.59315 \nTraining set [32320/60000 (54%)] Loss: 1.58719 \nTraining set [32640/60000 (54%)] Loss: 1.58189 \nTraining set [32960/60000 (55%)] Loss: 1.62743 \nTraining set [33280/60000 (55%)] Loss: 1.58391 \nTraining set [33600/60000 (56%)] Loss: 1.57349 \nTraining set [33920/60000 (57%)] Loss: 1.62004 \nTraining set [34240/60000 (57%)] Loss: 1.59378 \nTraining set [34560/60000 (58%)] Loss: 1.58079 \nTraining set [34880/60000 (58%)] Loss: 1.56679 \nTraining set [35200/60000 (59%)] Loss: 1.60334 \nTraining set [35520/60000 (59%)] Loss: 1.54151 \nTraining set [35840/60000 (60%)] Loss: 1.65182 \nTraining set [36160/60000 (60%)] Loss: 1.60423 \nTraining set [36480/60000 (61%)] Loss: 1.60784 \nTraining set [36800/60000 (61%)] Loss: 1.55969 \nTraining set [37120/60000 (62%)] Loss: 1.57258 \nTraining set [37440/60000 (62%)] Loss: 1.62098 \nTraining set [37760/60000 (63%)] Loss: 1.63876 \nTraining set [38080/60000 (63%)] Loss: 1.63448 \nTraining set [38400/60000 (64%)] Loss: 1.55834 \nTraining set [38720/60000 (64%)] Loss: 1.57674 \nTraining set [39040/60000 (65%)] Loss: 1.62522 \nTraining set [39360/60000 (66%)] Loss: 1.58209 \nTraining set [39680/60000 (66%)] Loss: 1.52570 \nTraining set [40000/60000 (67%)] Loss: 1.59182 \nTraining set [40320/60000 (67%)] Loss: 1.56890 \nTraining set [40640/60000 (68%)] Loss: 1.58442 \nTraining set [40960/60000 (68%)] Loss: 1.55782 \nTraining set [41280/60000 (69%)] Loss: 1.61617 \nTraining set [41600/60000 (69%)] Loss: 1.54731 \nTraining set [41920/60000 (70%)] Loss: 1.53261 \nTraining set [42240/60000 (70%)] Loss: 1.59604 \nTraining set [42560/60000 (71%)] Loss: 1.55171 \nTraining set [42880/60000 (71%)] Loss: 1.50721 \nTraining set [43200/60000 (72%)] Loss: 1.58788 \nTraining set [43520/60000 (72%)] Loss: 1.54428 \nTraining set [43840/60000 (73%)] Loss: 1.62276 \nTraining set [44160/60000 (74%)] Loss: 1.53020 \nTraining set [44480/60000 (74%)] Loss: 1.61626 \nTraining set [44800/60000 (75%)] Loss: 1.57081 \nTraining set [45120/60000 (75%)] Loss: 1.56255 \nTraining set [45440/60000 (76%)] Loss: 1.63386 \nTraining set [45760/60000 (76%)] Loss: 1.58306 \nTraining set [46080/60000 (77%)] Loss: 1.65719 \nTraining set [46400/60000 (77%)] Loss: 1.57235 \nTraining set [46720/60000 (78%)] Loss: 1.56465 \nTraining set [47040/60000 (78%)] Loss: 1.54497 \nTraining set [47360/60000 (79%)] Loss: 1.58830 \nTraining set [47680/60000 (79%)] Loss: 1.50439 \nTraining set [48000/60000 (80%)] Loss: 1.58913 \nTraining set [48320/60000 (80%)] Loss: 1.57414 \nTraining set [48640/60000 (81%)] Loss: 1.58520 \nTraining set [48960/60000 (82%)] Loss: 1.56501 \nTraining set [49280/60000 (82%)] Loss: 1.58497 \nTraining set [49600/60000 (83%)] Loss: 1.50248 \nTraining set [49920/60000 (83%)] Loss: 1.52983 \nTraining set [50240/60000 (84%)] Loss: 1.57104 \nTraining set [50560/60000 (84%)] Loss: 1.58732 \nTraining set [50880/60000 (85%)] Loss: 1.62707 \nTraining set [51200/60000 (85%)] Loss: 1.51028 \nTraining set [51520/60000 (86%)] Loss: 1.55163 \nTraining set [51840/60000 (86%)] Loss: 1.55343 \nTraining set [52160/60000 (87%)] Loss: 1.62483 \nTraining set [52480/60000 (87%)] Loss: 1.59448 \nTraining set [52800/60000 (88%)] Loss: 1.53087 \nTraining set [53120/60000 (88%)] Loss: 1.60175 \nTraining set [53440/60000 (89%)] Loss: 1.57468 \nTraining set [53760/60000 (90%)] Loss: 1.61382 \nTraining set [54080/60000 (90%)] Loss: 1.60965 \nTraining set [54400/60000 (91%)] Loss: 1.56831 \nTraining set [54720/60000 (91%)] Loss: 1.59639 \nTraining set [55040/60000 (92%)] Loss: 1.57892 \nTraining set [55360/60000 (92%)] Loss: 1.60809 \nTraining set [55680/60000 (93%)] Loss: 1.53515 \nTraining set [56000/60000 (93%)] Loss: 1.58321 \nTraining set [56320/60000 (94%)] Loss: 1.60879 \nTraining set [56640/60000 (94%)] Loss: 1.58415 \nTraining set [56960/60000 (95%)] Loss: 1.65561 \nTraining set [57280/60000 (95%)] Loss: 1.64499 \nTraining set [57600/60000 (96%)] Loss: 1.57796 \nTraining set [57920/60000 (96%)] Loss: 1.58167 \nTraining set [58240/60000 (97%)] Loss: 1.57320 \nTraining set [58560/60000 (98%)] Loss: 1.59969 \nTraining set [58880/60000 (98%)] Loss: 1.55335 \nTraining set [59200/60000 (99%)] Loss: 1.59870 \nTraining set [59520/60000 (99%)] Loss: 1.54818 \nTraining set [59840/60000 (100%)] Loss: 1.55431 \nAverage Training Loss: 1.58735\nTraining Completed!\nModel Validation Starts!\nAverage Validation Loss: 1.59659, Accuracy: 8666/10000 (87%)\n\nEpoch: 9\nTraining set [0/60000 (0%)] Loss: 1.55792 \nTraining set [320/60000 (1%)] Loss: 1.57283 \nTraining set [640/60000 (1%)] Loss: 1.57682 \nTraining set [960/60000 (2%)] Loss: 1.59060 \nTraining set [1280/60000 (2%)] Loss: 1.53926 \nTraining set [1600/60000 (3%)] Loss: 1.55219 \nTraining set [1920/60000 (3%)] Loss: 1.61620 \nTraining set [2240/60000 (4%)] Loss: 1.60093 \nTraining set [2560/60000 (4%)] Loss: 1.53332 \nTraining set [2880/60000 (5%)] Loss: 1.56524 \nTraining set [3200/60000 (5%)] Loss: 1.55758 \nTraining set [3520/60000 (6%)] Loss: 1.61731 \nTraining set [3840/60000 (6%)] Loss: 1.59939 \nTraining set [4160/60000 (7%)] Loss: 1.51414 \nTraining set [4480/60000 (7%)] Loss: 1.59718 \nTraining set [4800/60000 (8%)] Loss: 1.64063 \nTraining set [5120/60000 (9%)] Loss: 1.53539 \nTraining set [5440/60000 (9%)] Loss: 1.56320 \nTraining set [5760/60000 (10%)] Loss: 1.59382 \nTraining set [6080/60000 (10%)] Loss: 1.56550 \nTraining set [6400/60000 (11%)] Loss: 1.57678 \nTraining set [6720/60000 (11%)] Loss: 1.55355 \nTraining set [7040/60000 (12%)] Loss: 1.56558 \nTraining set [7360/60000 (12%)] Loss: 1.57797 \nTraining set [7680/60000 (13%)] Loss: 1.62254 \nTraining set [8000/60000 (13%)] Loss: 1.66561 \nTraining set [8320/60000 (14%)] Loss: 1.54971 \nTraining set [8640/60000 (14%)] Loss: 1.63341 \nTraining set [8960/60000 (15%)] Loss: 1.55182 \nTraining set [9280/60000 (15%)] Loss: 1.58662 \nTraining set [9600/60000 (16%)] Loss: 1.57861 \nTraining set [9920/60000 (17%)] Loss: 1.64211 \nTraining set [10240/60000 (17%)] Loss: 1.54141 \nTraining set [10560/60000 (18%)] Loss: 1.53578 \nTraining set [10880/60000 (18%)] Loss: 1.52105 \nTraining set [11200/60000 (19%)] Loss: 1.51755 \nTraining set [11520/60000 (19%)] Loss: 1.54083 \nTraining set [11840/60000 (20%)] Loss: 1.60084 \nTraining set [12160/60000 (20%)] Loss: 1.56466 \nTraining set [12480/60000 (21%)] Loss: 1.58926 \nTraining set [12800/60000 (21%)] Loss: 1.58998 \nTraining set [13120/60000 (22%)] Loss: 1.61238 \nTraining set [13440/60000 (22%)] Loss: 1.66360 \nTraining set [13760/60000 (23%)] Loss: 1.70231 \nTraining set [14080/60000 (23%)] Loss: 1.65056 \nTraining set [14400/60000 (24%)] Loss: 1.60833 \nTraining set [14720/60000 (25%)] Loss: 1.56783 \nTraining set [15040/60000 (25%)] Loss: 1.63626 \nTraining set [15360/60000 (26%)] Loss: 1.57799 \nTraining set [15680/60000 (26%)] Loss: 1.59012 \nTraining set [16000/60000 (27%)] Loss: 1.56769 \nTraining set [16320/60000 (27%)] Loss: 1.59095 \nTraining set [16640/60000 (28%)] Loss: 1.59569 \nTraining set [16960/60000 (28%)] Loss: 1.60511 \nTraining set [17280/60000 (29%)] Loss: 1.52055 \nTraining set [17600/60000 (29%)] Loss: 1.57129 \nTraining set [17920/60000 (30%)] Loss: 1.59168 \nTraining set [18240/60000 (30%)] Loss: 1.61603 \nTraining set [18560/60000 (31%)] Loss: 1.64280 \nTraining set [18880/60000 (31%)] Loss: 1.57214 \nTraining set [19200/60000 (32%)] Loss: 1.58057 \nTraining set [19520/60000 (33%)] Loss: 1.52670 \nTraining set [19840/60000 (33%)] Loss: 1.53685 \nTraining set [20160/60000 (34%)] Loss: 1.58306 \nTraining set [20480/60000 (34%)] Loss: 1.59172 \nTraining set [20800/60000 (35%)] Loss: 1.50695 \nTraining set [21120/60000 (35%)] Loss: 1.55186 \nTraining set [21440/60000 (36%)] Loss: 1.58376 \nTraining set [21760/60000 (36%)] Loss: 1.60130 \nTraining set [22080/60000 (37%)] Loss: 1.64900 \nTraining set [22400/60000 (37%)] Loss: 1.67178 \nTraining set [22720/60000 (38%)] Loss: 1.56744 \nTraining set [23040/60000 (38%)] Loss: 1.53177 \nTraining set [23360/60000 (39%)] Loss: 1.57680 \nTraining set [23680/60000 (39%)] Loss: 1.63330 \nTraining set [24000/60000 (40%)] Loss: 1.53504 \nTraining set [24320/60000 (41%)] Loss: 1.59802 \nTraining set [24640/60000 (41%)] Loss: 1.60846 \nTraining set [24960/60000 (42%)] Loss: 1.52559 \nTraining set [25280/60000 (42%)] Loss: 1.56283 \nTraining set [25600/60000 (43%)] Loss: 1.57749 \nTraining set [25920/60000 (43%)] Loss: 1.48622 \nTraining set [26240/60000 (44%)] Loss: 1.54988 \nTraining set [26560/60000 (44%)] Loss: 1.52083 \nTraining set [26880/60000 (45%)] Loss: 1.55252 \nTraining set [27200/60000 (45%)] Loss: 1.59855 \nTraining set [27520/60000 (46%)] Loss: 1.63086 \nTraining set [27840/60000 (46%)] Loss: 1.59708 \nTraining set [28160/60000 (47%)] Loss: 1.60525 \nTraining set [28480/60000 (47%)] Loss: 1.63298 \nTraining set [28800/60000 (48%)] Loss: 1.59949 \nTraining set [29120/60000 (49%)] Loss: 1.54393 \nTraining set [29440/60000 (49%)] Loss: 1.50015 \nTraining set [29760/60000 (50%)] Loss: 1.55924 \nTraining set [30080/60000 (50%)] Loss: 1.54654 \nTraining set [30400/60000 (51%)] Loss: 1.54957 \nTraining set [30720/60000 (51%)] Loss: 1.58233 \nTraining set [31040/60000 (52%)] Loss: 1.62056 \nTraining set [31360/60000 (52%)] Loss: 1.59869 \nTraining set [31680/60000 (53%)] Loss: 1.52290 \nTraining set [32000/60000 (53%)] Loss: 1.56789 \nTraining set [32320/60000 (54%)] Loss: 1.62671 \nTraining set [32640/60000 (54%)] Loss: 1.56794 \nTraining set [32960/60000 (55%)] Loss: 1.56422 \nTraining set [33280/60000 (55%)] Loss: 1.62088 \nTraining set [33600/60000 (56%)] Loss: 1.64110 \nTraining set [33920/60000 (57%)] Loss: 1.59583 \nTraining set [34240/60000 (57%)] Loss: 1.53139 \nTraining set [34560/60000 (58%)] Loss: 1.57702 \nTraining set [34880/60000 (58%)] Loss: 1.61444 \nTraining set [35200/60000 (59%)] Loss: 1.52107 \nTraining set [35520/60000 (59%)] Loss: 1.59315 \nTraining set [35840/60000 (60%)] Loss: 1.57864 \nTraining set [36160/60000 (60%)] Loss: 1.59321 \nTraining set [36480/60000 (61%)] Loss: 1.54983 \nTraining set [36800/60000 (61%)] Loss: 1.61830 \nTraining set [37120/60000 (62%)] Loss: 1.60494 \nTraining set [37440/60000 (62%)] Loss: 1.60683 \nTraining set [37760/60000 (63%)] Loss: 1.61657 \nTraining set [38080/60000 (63%)] Loss: 1.60358 \nTraining set [38400/60000 (64%)] Loss: 1.58452 \nTraining set [38720/60000 (64%)] Loss: 1.56094 \nTraining set [39040/60000 (65%)] Loss: 1.53886 \nTraining set [39360/60000 (66%)] Loss: 1.57025 \nTraining set [39680/60000 (66%)] Loss: 1.56886 \nTraining set [40000/60000 (67%)] Loss: 1.58327 \nTraining set [40320/60000 (67%)] Loss: 1.54406 \nTraining set [40640/60000 (68%)] Loss: 1.57583 \nTraining set [40960/60000 (68%)] Loss: 1.61371 \nTraining set [41280/60000 (69%)] Loss: 1.60208 \nTraining set [41600/60000 (69%)] Loss: 1.59872 \nTraining set [41920/60000 (70%)] Loss: 1.64804 \nTraining set [42240/60000 (70%)] Loss: 1.55976 \nTraining set [42560/60000 (71%)] Loss: 1.60297 \nTraining set [42880/60000 (71%)] Loss: 1.55172 \nTraining set [43200/60000 (72%)] Loss: 1.58234 \nTraining set [43520/60000 (72%)] Loss: 1.56189 \nTraining set [43840/60000 (73%)] Loss: 1.54962 \nTraining set [44160/60000 (74%)] Loss: 1.55322 \nTraining set [44480/60000 (74%)] Loss: 1.53441 \nTraining set [44800/60000 (75%)] Loss: 1.52887 \nTraining set [45120/60000 (75%)] Loss: 1.52255 \nTraining set [45440/60000 (76%)] Loss: 1.60875 \nTraining set [45760/60000 (76%)] Loss: 1.58854 \nTraining set [46080/60000 (77%)] Loss: 1.57136 \nTraining set [46400/60000 (77%)] Loss: 1.59378 \nTraining set [46720/60000 (78%)] Loss: 1.65780 \nTraining set [47040/60000 (78%)] Loss: 1.54640 \nTraining set [47360/60000 (79%)] Loss: 1.53300 \nTraining set [47680/60000 (79%)] Loss: 1.57498 \nTraining set [48000/60000 (80%)] Loss: 1.58714 \nTraining set [48320/60000 (80%)] Loss: 1.57804 \nTraining set [48640/60000 (81%)] Loss: 1.56576 \nTraining set [48960/60000 (82%)] Loss: 1.65899 \nTraining set [49280/60000 (82%)] Loss: 1.65064 \nTraining set [49600/60000 (83%)] Loss: 1.60734 \nTraining set [49920/60000 (83%)] Loss: 1.55175 \nTraining set [50240/60000 (84%)] Loss: 1.59628 \nTraining set [50560/60000 (84%)] Loss: 1.57412 \nTraining set [50880/60000 (85%)] Loss: 1.54153 \nTraining set [51200/60000 (85%)] Loss: 1.55213 \nTraining set [51520/60000 (86%)] Loss: 1.53212 \nTraining set [51840/60000 (86%)] Loss: 1.55331 \nTraining set [52160/60000 (87%)] Loss: 1.57783 \nTraining set [52480/60000 (87%)] Loss: 1.60006 \nTraining set [52800/60000 (88%)] Loss: 1.61006 \nTraining set [53120/60000 (88%)] Loss: 1.58484 \nTraining set [53440/60000 (89%)] Loss: 1.56952 \nTraining set [53760/60000 (90%)] Loss: 1.54634 \nTraining set [54080/60000 (90%)] Loss: 1.59183 \nTraining set [54400/60000 (91%)] Loss: 1.52306 \nTraining set [54720/60000 (91%)] Loss: 1.56278 \nTraining set [55040/60000 (92%)] Loss: 1.58392 \nTraining set [55360/60000 (92%)] Loss: 1.57238 \nTraining set [55680/60000 (93%)] Loss: 1.63629 \nTraining set [56000/60000 (93%)] Loss: 1.60958 \nTraining set [56320/60000 (94%)] Loss: 1.59278 \nTraining set [56640/60000 (94%)] Loss: 1.60914 \nTraining set [56960/60000 (95%)] Loss: 1.55797 \nTraining set [57280/60000 (95%)] Loss: 1.54518 \nTraining set [57600/60000 (96%)] Loss: 1.61094 \nTraining set [57920/60000 (96%)] Loss: 1.50179 \nTraining set [58240/60000 (97%)] Loss: 1.58960 \nTraining set [58560/60000 (98%)] Loss: 1.56267 \nTraining set [58880/60000 (98%)] Loss: 1.51004 \nTraining set [59200/60000 (99%)] Loss: 1.56907 \nTraining set [59520/60000 (99%)] Loss: 1.54810 \nTraining set [59840/60000 (100%)] Loss: 1.58058 \nAverage Training Loss: 1.58213\nTraining Completed!\nModel Validation Starts!\nAverage Validation Loss: 1.59639, Accuracy: 8677/10000 (87%)\n\nEpoch: 10\nTraining set [0/60000 (0%)] Loss: 1.56072 \nTraining set [320/60000 (1%)] Loss: 1.51546 \nTraining set [640/60000 (1%)] Loss: 1.51543 \nTraining set [960/60000 (2%)] Loss: 1.54063 \nTraining set [1280/60000 (2%)] Loss: 1.52066 \nTraining set [1600/60000 (3%)] Loss: 1.57265 \nTraining set [1920/60000 (3%)] Loss: 1.68290 \nTraining set [2240/60000 (4%)] Loss: 1.56174 \nTraining set [2560/60000 (4%)] Loss: 1.64169 \nTraining set [2880/60000 (5%)] Loss: 1.59457 \nTraining set [3200/60000 (5%)] Loss: 1.55487 \nTraining set [3520/60000 (6%)] Loss: 1.58011 \nTraining set [3840/60000 (6%)] Loss: 1.57980 \nTraining set [4160/60000 (7%)] Loss: 1.64311 \nTraining set [4480/60000 (7%)] Loss: 1.63408 \nTraining set [4800/60000 (8%)] Loss: 1.60631 \nTraining set [5120/60000 (9%)] Loss: 1.53512 \nTraining set [5440/60000 (9%)] Loss: 1.57406 \nTraining set [5760/60000 (10%)] Loss: 1.52546 \nTraining set [6080/60000 (10%)] Loss: 1.60586 \nTraining set [6400/60000 (11%)] Loss: 1.58808 \nTraining set [6720/60000 (11%)] Loss: 1.62314 \nTraining set [7040/60000 (12%)] Loss: 1.57339 \nTraining set [7360/60000 (12%)] Loss: 1.53536 \nTraining set [7680/60000 (13%)] Loss: 1.59257 \nTraining set [8000/60000 (13%)] Loss: 1.55393 \nTraining set [8320/60000 (14%)] Loss: 1.57799 \nTraining set [8640/60000 (14%)] Loss: 1.61950 \nTraining set [8960/60000 (15%)] Loss: 1.59116 \nTraining set [9280/60000 (15%)] Loss: 1.51075 \nTraining set [9600/60000 (16%)] Loss: 1.65314 \nTraining set [9920/60000 (17%)] Loss: 1.56786 \nTraining set [10240/60000 (17%)] Loss: 1.63213 \nTraining set [10560/60000 (18%)] Loss: 1.59348 \nTraining set [10880/60000 (18%)] Loss: 1.60330 \nTraining set [11200/60000 (19%)] Loss: 1.60157 \nTraining set [11520/60000 (19%)] Loss: 1.54361 \nTraining set [11840/60000 (20%)] Loss: 1.61585 \nTraining set [12160/60000 (20%)] Loss: 1.56408 \nTraining set [12480/60000 (21%)] Loss: 1.59512 \nTraining set [12800/60000 (21%)] Loss: 1.54724 \nTraining set [13120/60000 (22%)] Loss: 1.54957 \nTraining set [13440/60000 (22%)] Loss: 1.59924 \nTraining set [13760/60000 (23%)] Loss: 1.57093 \nTraining set [14080/60000 (23%)] Loss: 1.62767 \nTraining set [14400/60000 (24%)] Loss: 1.69182 \nTraining set [14720/60000 (25%)] Loss: 1.58426 \nTraining set [15040/60000 (25%)] Loss: 1.59807 \nTraining set [15360/60000 (26%)] Loss: 1.60433 \nTraining set [15680/60000 (26%)] Loss: 1.54922 \nTraining set [16000/60000 (27%)] Loss: 1.54614 \nTraining set [16320/60000 (27%)] Loss: 1.63235 \nTraining set [16640/60000 (28%)] Loss: 1.59309 \nTraining set [16960/60000 (28%)] Loss: 1.56787 \nTraining set [17280/60000 (29%)] Loss: 1.64496 \nTraining set [17600/60000 (29%)] Loss: 1.60869 \nTraining set [17920/60000 (30%)] Loss: 1.58080 \nTraining set [18240/60000 (30%)] Loss: 1.58640 \nTraining set [18560/60000 (31%)] Loss: 1.51394 \nTraining set [18880/60000 (31%)] Loss: 1.60902 \nTraining set [19200/60000 (32%)] Loss: 1.51763 \nTraining set [19520/60000 (33%)] Loss: 1.62102 \nTraining set [19840/60000 (33%)] Loss: 1.56669 \nTraining set [20160/60000 (34%)] Loss: 1.56934 \nTraining set [20480/60000 (34%)] Loss: 1.60748 \nTraining set [20800/60000 (35%)] Loss: 1.57611 \nTraining set [21120/60000 (35%)] Loss: 1.54528 \nTraining set [21440/60000 (36%)] Loss: 1.55881 \nTraining set [21760/60000 (36%)] Loss: 1.64997 \nTraining set [22080/60000 (37%)] Loss: 1.55088 \nTraining set [22400/60000 (37%)] Loss: 1.57415 \nTraining set [22720/60000 (38%)] Loss: 1.53188 \nTraining set [23040/60000 (38%)] Loss: 1.59889 \nTraining set [23360/60000 (39%)] Loss: 1.59666 \nTraining set [23680/60000 (39%)] Loss: 1.59753 \nTraining set [24000/60000 (40%)] Loss: 1.59900 \nTraining set [24320/60000 (41%)] Loss: 1.60236 \nTraining set [24640/60000 (41%)] Loss: 1.57602 \nTraining set [24960/60000 (42%)] Loss: 1.59674 \nTraining set [25280/60000 (42%)] Loss: 1.54387 \nTraining set [25600/60000 (43%)] Loss: 1.66061 \nTraining set [25920/60000 (43%)] Loss: 1.52985 \nTraining set [26240/60000 (44%)] Loss: 1.57427 \nTraining set [26560/60000 (44%)] Loss: 1.65354 \nTraining set [26880/60000 (45%)] Loss: 1.61292 \nTraining set [27200/60000 (45%)] Loss: 1.56618 \nTraining set [27520/60000 (46%)] Loss: 1.58591 \nTraining set [27840/60000 (46%)] Loss: 1.59040 \nTraining set [28160/60000 (47%)] Loss: 1.54322 \nTraining set [28480/60000 (47%)] Loss: 1.60864 \nTraining set [28800/60000 (48%)] Loss: 1.64040 \nTraining set [29120/60000 (49%)] Loss: 1.56442 \nTraining set [29440/60000 (49%)] Loss: 1.54600 \nTraining set [29760/60000 (50%)] Loss: 1.61938 \nTraining set [30080/60000 (50%)] Loss: 1.63202 \nTraining set [30400/60000 (51%)] Loss: 1.55601 \nTraining set [30720/60000 (51%)] Loss: 1.53761 \nTraining set [31040/60000 (52%)] Loss: 1.52722 \nTraining set [31360/60000 (52%)] Loss: 1.60053 \nTraining set [31680/60000 (53%)] Loss: 1.60163 \nTraining set [32000/60000 (53%)] Loss: 1.56328 \nTraining set [32320/60000 (54%)] Loss: 1.61988 \nTraining set [32640/60000 (54%)] Loss: 1.59371 \nTraining set [32960/60000 (55%)] Loss: 1.57737 \nTraining set [33280/60000 (55%)] Loss: 1.54773 \nTraining set [33600/60000 (56%)] Loss: 1.57230 \nTraining set [33920/60000 (57%)] Loss: 1.58970 \nTraining set [34240/60000 (57%)] Loss: 1.56810 \nTraining set [34560/60000 (58%)] Loss: 1.53890 \nTraining set [34880/60000 (58%)] Loss: 1.57088 \nTraining set [35200/60000 (59%)] Loss: 1.61964 \nTraining set [35520/60000 (59%)] Loss: 1.55499 \nTraining set [35840/60000 (60%)] Loss: 1.61520 \nTraining set [36160/60000 (60%)] Loss: 1.59933 \nTraining set [36480/60000 (61%)] Loss: 1.63797 \nTraining set [36800/60000 (61%)] Loss: 1.58624 \nTraining set [37120/60000 (62%)] Loss: 1.61080 \nTraining set [37440/60000 (62%)] Loss: 1.57759 \nTraining set [37760/60000 (63%)] Loss: 1.57498 \nTraining set [38080/60000 (63%)] Loss: 1.60494 \nTraining set [38400/60000 (64%)] Loss: 1.59574 \nTraining set [38720/60000 (64%)] Loss: 1.57633 \nTraining set [39040/60000 (65%)] Loss: 1.51100 \nTraining set [39360/60000 (66%)] Loss: 1.53556 \nTraining set [39680/60000 (66%)] Loss: 1.63588 \nTraining set [40000/60000 (67%)] Loss: 1.63016 \nTraining set [40320/60000 (67%)] Loss: 1.66039 \nTraining set [40640/60000 (68%)] Loss: 1.54118 \nTraining set [40960/60000 (68%)] Loss: 1.60282 \nTraining set [41280/60000 (69%)] Loss: 1.62141 \nTraining set [41600/60000 (69%)] Loss: 1.56849 \nTraining set [41920/60000 (70%)] Loss: 1.55039 \nTraining set [42240/60000 (70%)] Loss: 1.52904 \nTraining set [42560/60000 (71%)] Loss: 1.60408 \nTraining set [42880/60000 (71%)] Loss: 1.58359 \nTraining set [43200/60000 (72%)] Loss: 1.67412 \nTraining set [43520/60000 (72%)] Loss: 1.57137 \nTraining set [43840/60000 (73%)] Loss: 1.69499 \nTraining set [44160/60000 (74%)] Loss: 1.61617 \nTraining set [44480/60000 (74%)] Loss: 1.59767 \nTraining set [44800/60000 (75%)] Loss: 1.61395 \nTraining set [45120/60000 (75%)] Loss: 1.53944 \nTraining set [45440/60000 (76%)] Loss: 1.59088 \nTraining set [45760/60000 (76%)] Loss: 1.55074 \nTraining set [46080/60000 (77%)] Loss: 1.55333 \nTraining set [46400/60000 (77%)] Loss: 1.57068 \nTraining set [46720/60000 (78%)] Loss: 1.60172 \nTraining set [47040/60000 (78%)] Loss: 1.60315 \nTraining set [47360/60000 (79%)] Loss: 1.56116 \nTraining set [47680/60000 (79%)] Loss: 1.58084 \nTraining set [48000/60000 (80%)] Loss: 1.54800 \nTraining set [48320/60000 (80%)] Loss: 1.57591 \nTraining set [48640/60000 (81%)] Loss: 1.53563 \nTraining set [48960/60000 (82%)] Loss: 1.56966 \nTraining set [49280/60000 (82%)] Loss: 1.57520 \nTraining set [49600/60000 (83%)] Loss: 1.63627 \nTraining set [49920/60000 (83%)] Loss: 1.59269 \nTraining set [50240/60000 (84%)] Loss: 1.64456 \nTraining set [50560/60000 (84%)] Loss: 1.57668 \nTraining set [50880/60000 (85%)] Loss: 1.55096 \nTraining set [51200/60000 (85%)] Loss: 1.58482 \nTraining set [51520/60000 (86%)] Loss: 1.51455 \nTraining set [51840/60000 (86%)] Loss: 1.58187 \nTraining set [52160/60000 (87%)] Loss: 1.60331 \nTraining set [52480/60000 (87%)] Loss: 1.53743 \nTraining set [52800/60000 (88%)] Loss: 1.56025 \nTraining set [53120/60000 (88%)] Loss: 1.60520 \nTraining set [53440/60000 (89%)] Loss: 1.60242 \nTraining set [53760/60000 (90%)] Loss: 1.52613 \nTraining set [54080/60000 (90%)] Loss: 1.62457 \nTraining set [54400/60000 (91%)] Loss: 1.61607 \nTraining set [54720/60000 (91%)] Loss: 1.57009 \nTraining set [55040/60000 (92%)] Loss: 1.54221 \nTraining set [55360/60000 (92%)] Loss: 1.59879 \nTraining set [55680/60000 (93%)] Loss: 1.62444 \nTraining set [56000/60000 (93%)] Loss: 1.52396 \nTraining set [56320/60000 (94%)] Loss: 1.59041 \nTraining set [56640/60000 (94%)] Loss: 1.61160 \nTraining set [56960/60000 (95%)] Loss: 1.58207 \nTraining set [57280/60000 (95%)] Loss: 1.57492 \nTraining set [57600/60000 (96%)] Loss: 1.53443 \nTraining set [57920/60000 (96%)] Loss: 1.63980 \nTraining set [58240/60000 (97%)] Loss: 1.53176 \nTraining set [58560/60000 (98%)] Loss: 1.60658 \nTraining set [58880/60000 (98%)] Loss: 1.57745 \nTraining set [59200/60000 (99%)] Loss: 1.51747 \nTraining set [59520/60000 (99%)] Loss: 1.53382 \nTraining set [59840/60000 (100%)] Loss: 1.62190 \nAverage Training Loss: 1.57774\nTraining Completed!\nModel Validation Starts!\nAverage Validation Loss: 1.58699, Accuracy: 8754/10000 (88%)\n\nEpoch: 11\nTraining set [0/60000 (0%)] Loss: 1.58667 \nTraining set [320/60000 (1%)] Loss: 1.59998 \nTraining set [640/60000 (1%)] Loss: 1.57286 \nTraining set [960/60000 (2%)] Loss: 1.59639 \nTraining set [1280/60000 (2%)] Loss: 1.56769 \nTraining set [1600/60000 (3%)] Loss: 1.54902 \nTraining set [1920/60000 (3%)] Loss: 1.51422 \nTraining set [2240/60000 (4%)] Loss: 1.54486 \nTraining set [2560/60000 (4%)] Loss: 1.58222 \nTraining set [2880/60000 (5%)] Loss: 1.57225 \nTraining set [3200/60000 (5%)] Loss: 1.59658 \nTraining set [3520/60000 (6%)] Loss: 1.60984 \nTraining set [3840/60000 (6%)] Loss: 1.58190 \nTraining set [4160/60000 (7%)] Loss: 1.50551 \nTraining set [4480/60000 (7%)] Loss: 1.66816 \nTraining set [4800/60000 (8%)] Loss: 1.57845 \nTraining set [5120/60000 (9%)] Loss: 1.57019 \nTraining set [5440/60000 (9%)] Loss: 1.63374 \nTraining set [5760/60000 (10%)] Loss: 1.64602 \nTraining set [6080/60000 (10%)] Loss: 1.57992 \nTraining set [6400/60000 (11%)] Loss: 1.52901 \nTraining set [6720/60000 (11%)] Loss: 1.51641 \nTraining set [7040/60000 (12%)] Loss: 1.66448 \nTraining set [7360/60000 (12%)] Loss: 1.57984 \nTraining set [7680/60000 (13%)] Loss: 1.56922 \nTraining set [8000/60000 (13%)] Loss: 1.57890 \nTraining set [8320/60000 (14%)] Loss: 1.48577 \nTraining set [8640/60000 (14%)] Loss: 1.55324 \nTraining set [8960/60000 (15%)] Loss: 1.61323 \nTraining set [9280/60000 (15%)] Loss: 1.62638 \nTraining set [9600/60000 (16%)] Loss: 1.61185 \nTraining set [9920/60000 (17%)] Loss: 1.54994 \nTraining set [10240/60000 (17%)] Loss: 1.58343 \nTraining set [10560/60000 (18%)] Loss: 1.64173 \nTraining set [10880/60000 (18%)] Loss: 1.64573 \nTraining set [11200/60000 (19%)] Loss: 1.53699 \nTraining set [11520/60000 (19%)] Loss: 1.61736 \nTraining set [11840/60000 (20%)] Loss: 1.60177 \nTraining set [12160/60000 (20%)] Loss: 1.58166 \nTraining set [12480/60000 (21%)] Loss: 1.57287 \nTraining set [12800/60000 (21%)] Loss: 1.50756 \nTraining set [13120/60000 (22%)] Loss: 1.54591 \nTraining set [13440/60000 (22%)] Loss: 1.61424 \nTraining set [13760/60000 (23%)] Loss: 1.54868 \nTraining set [14080/60000 (23%)] Loss: 1.57690 \nTraining set [14400/60000 (24%)] Loss: 1.60415 \nTraining set [14720/60000 (25%)] Loss: 1.54858 \nTraining set [15040/60000 (25%)] Loss: 1.58575 \nTraining set [15360/60000 (26%)] Loss: 1.59380 \nTraining set [15680/60000 (26%)] Loss: 1.60709 \nTraining set [16000/60000 (27%)] Loss: 1.55593 \nTraining set [16320/60000 (27%)] Loss: 1.57938 \nTraining set [16640/60000 (28%)] Loss: 1.57500 \nTraining set [16960/60000 (28%)] Loss: 1.60231 \nTraining set [17280/60000 (29%)] Loss: 1.57432 \nTraining set [17600/60000 (29%)] Loss: 1.56849 \nTraining set [17920/60000 (30%)] Loss: 1.63155 \nTraining set [18240/60000 (30%)] Loss: 1.57783 \nTraining set [18560/60000 (31%)] Loss: 1.55371 \nTraining set [18880/60000 (31%)] Loss: 1.52946 \nTraining set [19200/60000 (32%)] Loss: 1.56922 \nTraining set [19520/60000 (33%)] Loss: 1.56346 \nTraining set [19840/60000 (33%)] Loss: 1.64726 \nTraining set [20160/60000 (34%)] Loss: 1.54008 \nTraining set [20480/60000 (34%)] Loss: 1.54204 \nTraining set [20800/60000 (35%)] Loss: 1.62865 \nTraining set [21120/60000 (35%)] Loss: 1.63386 \nTraining set [21440/60000 (36%)] Loss: 1.60689 \nTraining set [21760/60000 (36%)] Loss: 1.64492 \nTraining set [22080/60000 (37%)] Loss: 1.63617 \nTraining set [22400/60000 (37%)] Loss: 1.55203 \nTraining set [22720/60000 (38%)] Loss: 1.61127 \nTraining set [23040/60000 (38%)] Loss: 1.61889 \nTraining set [23360/60000 (39%)] Loss: 1.51683 \nTraining set [23680/60000 (39%)] Loss: 1.60445 \nTraining set [24000/60000 (40%)] Loss: 1.56966 \nTraining set [24320/60000 (41%)] Loss: 1.56423 \nTraining set [24640/60000 (41%)] Loss: 1.53236 \nTraining set [24960/60000 (42%)] Loss: 1.58907 \nTraining set [25280/60000 (42%)] Loss: 1.51265 \nTraining set [25600/60000 (43%)] Loss: 1.52865 \nTraining set [25920/60000 (43%)] Loss: 1.56001 \nTraining set [26240/60000 (44%)] Loss: 1.54358 \nTraining set [26560/60000 (44%)] Loss: 1.59110 \nTraining set [26880/60000 (45%)] Loss: 1.54689 \nTraining set [27200/60000 (45%)] Loss: 1.56055 \nTraining set [27520/60000 (46%)] Loss: 1.56976 \nTraining set [27840/60000 (46%)] Loss: 1.52520 \nTraining set [28160/60000 (47%)] Loss: 1.56015 \nTraining set [28480/60000 (47%)] Loss: 1.51466 \nTraining set [28800/60000 (48%)] Loss: 1.56957 \nTraining set [29120/60000 (49%)] Loss: 1.58834 \nTraining set [29440/60000 (49%)] Loss: 1.61689 \nTraining set [29760/60000 (50%)] Loss: 1.57847 \nTraining set [30080/60000 (50%)] Loss: 1.59021 \nTraining set [30400/60000 (51%)] Loss: 1.51040 \nTraining set [30720/60000 (51%)] Loss: 1.58707 \nTraining set [31040/60000 (52%)] Loss: 1.52132 \nTraining set [31360/60000 (52%)] Loss: 1.59788 \nTraining set [31680/60000 (53%)] Loss: 1.52635 \nTraining set [32000/60000 (53%)] Loss: 1.53140 \nTraining set [32320/60000 (54%)] Loss: 1.56977 \nTraining set [32640/60000 (54%)] Loss: 1.57626 \nTraining set [32960/60000 (55%)] Loss: 1.60185 \nTraining set [33280/60000 (55%)] Loss: 1.60500 \nTraining set [33600/60000 (56%)] Loss: 1.57140 \nTraining set [33920/60000 (57%)] Loss: 1.60378 \nTraining set [34240/60000 (57%)] Loss: 1.59579 \nTraining set [34560/60000 (58%)] Loss: 1.53706 \nTraining set [34880/60000 (58%)] Loss: 1.67817 \nTraining set [35200/60000 (59%)] Loss: 1.57787 \nTraining set [35520/60000 (59%)] Loss: 1.58258 \nTraining set [35840/60000 (60%)] Loss: 1.60135 \nTraining set [36160/60000 (60%)] Loss: 1.56274 \nTraining set [36480/60000 (61%)] Loss: 1.57147 \nTraining set [36800/60000 (61%)] Loss: 1.62831 \nTraining set [37120/60000 (62%)] Loss: 1.58195 \nTraining set [37440/60000 (62%)] Loss: 1.59441 \nTraining set [37760/60000 (63%)] Loss: 1.62260 \nTraining set [38080/60000 (63%)] Loss: 1.55960 \nTraining set [38400/60000 (64%)] Loss: 1.58618 \nTraining set [38720/60000 (64%)] Loss: 1.54380 \nTraining set [39040/60000 (65%)] Loss: 1.54881 \nTraining set [39360/60000 (66%)] Loss: 1.58794 \nTraining set [39680/60000 (66%)] Loss: 1.57655 \nTraining set [40000/60000 (67%)] Loss: 1.56524 \nTraining set [40320/60000 (67%)] Loss: 1.52289 \nTraining set [40640/60000 (68%)] Loss: 1.64735 \nTraining set [40960/60000 (68%)] Loss: 1.58157 \nTraining set [41280/60000 (69%)] Loss: 1.58070 \nTraining set [41600/60000 (69%)] Loss: 1.62099 \nTraining set [41920/60000 (70%)] Loss: 1.54510 \nTraining set [42240/60000 (70%)] Loss: 1.56201 \nTraining set [42560/60000 (71%)] Loss: 1.56290 \nTraining set [42880/60000 (71%)] Loss: 1.59044 \nTraining set [43200/60000 (72%)] Loss: 1.59730 \nTraining set [43520/60000 (72%)] Loss: 1.59770 \nTraining set [43840/60000 (73%)] Loss: 1.62132 \nTraining set [44160/60000 (74%)] Loss: 1.57017 \nTraining set [44480/60000 (74%)] Loss: 1.58556 \nTraining set [44800/60000 (75%)] Loss: 1.59682 \nTraining set [45120/60000 (75%)] Loss: 1.61507 \nTraining set [45440/60000 (76%)] Loss: 1.52984 \nTraining set [45760/60000 (76%)] Loss: 1.54676 \nTraining set [46080/60000 (77%)] Loss: 1.60796 \nTraining set [46400/60000 (77%)] Loss: 1.60627 \nTraining set [46720/60000 (78%)] Loss: 1.58341 \nTraining set [47040/60000 (78%)] Loss: 1.60235 \nTraining set [47360/60000 (79%)] Loss: 1.59707 \nTraining set [47680/60000 (79%)] Loss: 1.55894 \nTraining set [48000/60000 (80%)] Loss: 1.60773 \nTraining set [48320/60000 (80%)] Loss: 1.57411 \nTraining set [48640/60000 (81%)] Loss: 1.53670 \nTraining set [48960/60000 (82%)] Loss: 1.63336 \nTraining set [49280/60000 (82%)] Loss: 1.62931 \nTraining set [49600/60000 (83%)] Loss: 1.61488 \nTraining set [49920/60000 (83%)] Loss: 1.55167 \nTraining set [50240/60000 (84%)] Loss: 1.62871 \nTraining set [50560/60000 (84%)] Loss: 1.59495 \nTraining set [50880/60000 (85%)] Loss: 1.57684 \nTraining set [51200/60000 (85%)] Loss: 1.54424 \nTraining set [51520/60000 (86%)] Loss: 1.56688 \nTraining set [51840/60000 (86%)] Loss: 1.49621 \nTraining set [52160/60000 (87%)] Loss: 1.54427 \nTraining set [52480/60000 (87%)] Loss: 1.60166 \nTraining set [52800/60000 (88%)] Loss: 1.59651 \nTraining set [53120/60000 (88%)] Loss: 1.55095 \nTraining set [53440/60000 (89%)] Loss: 1.56380 \nTraining set [53760/60000 (90%)] Loss: 1.56083 \nTraining set [54080/60000 (90%)] Loss: 1.50672 \nTraining set [54400/60000 (91%)] Loss: 1.62062 \nTraining set [54720/60000 (91%)] Loss: 1.55558 \nTraining set [55040/60000 (92%)] Loss: 1.56758 \nTraining set [55360/60000 (92%)] Loss: 1.58551 \nTraining set [55680/60000 (93%)] Loss: 1.56359 \nTraining set [56000/60000 (93%)] Loss: 1.53184 \nTraining set [56320/60000 (94%)] Loss: 1.58233 \nTraining set [56640/60000 (94%)] Loss: 1.53412 \nTraining set [56960/60000 (95%)] Loss: 1.58956 \nTraining set [57280/60000 (95%)] Loss: 1.57790 \nTraining set [57600/60000 (96%)] Loss: 1.56579 \nTraining set [57920/60000 (96%)] Loss: 1.57047 \nTraining set [58240/60000 (97%)] Loss: 1.57642 \nTraining set [58560/60000 (98%)] Loss: 1.57089 \nTraining set [58880/60000 (98%)] Loss: 1.58932 \nTraining set [59200/60000 (99%)] Loss: 1.48295 \nTraining set [59520/60000 (99%)] Loss: 1.55196 \nTraining set [59840/60000 (100%)] Loss: 1.59491 \nAverage Training Loss: 1.57355\nTraining Completed!\nModel Validation Starts!\nAverage Validation Loss: 1.58086, Accuracy: 8830/10000 (88%)\n\nEpoch: 12\nTraining set [0/60000 (0%)] Loss: 1.61546 \nTraining set [320/60000 (1%)] Loss: 1.59174 \nTraining set [640/60000 (1%)] Loss: 1.56406 \nTraining set [960/60000 (2%)] Loss: 1.60369 \nTraining set [1280/60000 (2%)] Loss: 1.59089 \nTraining set [1600/60000 (3%)] Loss: 1.57423 \nTraining set [1920/60000 (3%)] Loss: 1.58406 \nTraining set [2240/60000 (4%)] Loss: 1.54544 \nTraining set [2560/60000 (4%)] Loss: 1.49123 \nTraining set [2880/60000 (5%)] Loss: 1.58290 \nTraining set [3200/60000 (5%)] Loss: 1.57015 \nTraining set [3520/60000 (6%)] Loss: 1.52559 \nTraining set [3840/60000 (6%)] Loss: 1.58772 \nTraining set [4160/60000 (7%)] Loss: 1.52116 \nTraining set [4480/60000 (7%)] Loss: 1.58687 \nTraining set [4800/60000 (8%)] Loss: 1.56136 \nTraining set [5120/60000 (9%)] Loss: 1.66330 \nTraining set [5440/60000 (9%)] Loss: 1.64735 \nTraining set [5760/60000 (10%)] Loss: 1.53944 \nTraining set [6080/60000 (10%)] Loss: 1.57284 \nTraining set [6400/60000 (11%)] Loss: 1.55189 \nTraining set [6720/60000 (11%)] Loss: 1.59030 \nTraining set [7040/60000 (12%)] Loss: 1.53725 \nTraining set [7360/60000 (12%)] Loss: 1.57204 \nTraining set [7680/60000 (13%)] Loss: 1.55467 \nTraining set [8000/60000 (13%)] Loss: 1.58763 \nTraining set [8320/60000 (14%)] Loss: 1.58466 \nTraining set [8640/60000 (14%)] Loss: 1.56876 \nTraining set [8960/60000 (15%)] Loss: 1.55736 \nTraining set [9280/60000 (15%)] Loss: 1.57822 \nTraining set [9600/60000 (16%)] Loss: 1.51118 \nTraining set [9920/60000 (17%)] Loss: 1.59082 \nTraining set [10240/60000 (17%)] Loss: 1.56664 \nTraining set [10560/60000 (18%)] Loss: 1.58870 \nTraining set [10880/60000 (18%)] Loss: 1.51344 \nTraining set [11200/60000 (19%)] Loss: 1.59854 \nTraining set [11520/60000 (19%)] Loss: 1.65251 \nTraining set [11840/60000 (20%)] Loss: 1.57167 \nTraining set [12160/60000 (20%)] Loss: 1.56564 \nTraining set [12480/60000 (21%)] Loss: 1.57614 \nTraining set [12800/60000 (21%)] Loss: 1.54909 \nTraining set [13120/60000 (22%)] Loss: 1.56033 \nTraining set [13440/60000 (22%)] Loss: 1.59160 \nTraining set [13760/60000 (23%)] Loss: 1.55722 \nTraining set [14080/60000 (23%)] Loss: 1.56335 \nTraining set [14400/60000 (24%)] Loss: 1.54332 \nTraining set [14720/60000 (25%)] Loss: 1.53534 \nTraining set [15040/60000 (25%)] Loss: 1.55307 \nTraining set [15360/60000 (26%)] Loss: 1.64664 \nTraining set [15680/60000 (26%)] Loss: 1.51273 \nTraining set [16000/60000 (27%)] Loss: 1.57552 \nTraining set [16320/60000 (27%)] Loss: 1.62333 \nTraining set [16640/60000 (28%)] Loss: 1.52788 \nTraining set [16960/60000 (28%)] Loss: 1.59718 \nTraining set [17280/60000 (29%)] Loss: 1.57103 \nTraining set [17600/60000 (29%)] Loss: 1.59967 \nTraining set [17920/60000 (30%)] Loss: 1.56226 \nTraining set [18240/60000 (30%)] Loss: 1.60218 \nTraining set [18560/60000 (31%)] Loss: 1.53840 \nTraining set [18880/60000 (31%)] Loss: 1.52983 \nTraining set [19200/60000 (32%)] Loss: 1.56009 \nTraining set [19520/60000 (33%)] Loss: 1.57934 \nTraining set [19840/60000 (33%)] Loss: 1.59267 \nTraining set [20160/60000 (34%)] Loss: 1.61906 \nTraining set [20480/60000 (34%)] Loss: 1.52360 \nTraining set [20800/60000 (35%)] Loss: 1.54107 \nTraining set [21120/60000 (35%)] Loss: 1.54503 \nTraining set [21440/60000 (36%)] Loss: 1.61301 \nTraining set [21760/60000 (36%)] Loss: 1.55203 \nTraining set [22080/60000 (37%)] Loss: 1.54373 \nTraining set [22400/60000 (37%)] Loss: 1.59331 \nTraining set [22720/60000 (38%)] Loss: 1.55540 \nTraining set [23040/60000 (38%)] Loss: 1.55494 \nTraining set [23360/60000 (39%)] Loss: 1.62047 \nTraining set [23680/60000 (39%)] Loss: 1.59111 \nTraining set [24000/60000 (40%)] Loss: 1.65948 \nTraining set [24320/60000 (41%)] Loss: 1.60814 \nTraining set [24640/60000 (41%)] Loss: 1.51392 \nTraining set [24960/60000 (42%)] Loss: 1.56446 \nTraining set [25280/60000 (42%)] Loss: 1.59351 \nTraining set [25600/60000 (43%)] Loss: 1.61951 \nTraining set [25920/60000 (43%)] Loss: 1.58805 \nTraining set [26240/60000 (44%)] Loss: 1.56004 \nTraining set [26560/60000 (44%)] Loss: 1.59934 \nTraining set [26880/60000 (45%)] Loss: 1.52995 \nTraining set [27200/60000 (45%)] Loss: 1.60576 \nTraining set [27520/60000 (46%)] Loss: 1.59673 \nTraining set [27840/60000 (46%)] Loss: 1.56708 \nTraining set [28160/60000 (47%)] Loss: 1.61352 \nTraining set [28480/60000 (47%)] Loss: 1.56126 \nTraining set [28800/60000 (48%)] Loss: 1.61133 \nTraining set [29120/60000 (49%)] Loss: 1.56439 \nTraining set [29440/60000 (49%)] Loss: 1.53167 \nTraining set [29760/60000 (50%)] Loss: 1.61349 \nTraining set [30080/60000 (50%)] Loss: 1.59509 \nTraining set [30400/60000 (51%)] Loss: 1.61011 \nTraining set [30720/60000 (51%)] Loss: 1.62423 \nTraining set [31040/60000 (52%)] Loss: 1.50152 \nTraining set [31360/60000 (52%)] Loss: 1.57146 \nTraining set [31680/60000 (53%)] Loss: 1.59906 \nTraining set [32000/60000 (53%)] Loss: 1.55573 \nTraining set [32320/60000 (54%)] Loss: 1.60740 \nTraining set [32640/60000 (54%)] Loss: 1.58582 \nTraining set [32960/60000 (55%)] Loss: 1.60386 \nTraining set [33280/60000 (55%)] Loss: 1.58756 \nTraining set [33600/60000 (56%)] Loss: 1.58202 \nTraining set [33920/60000 (57%)] Loss: 1.55253 \nTraining set [34240/60000 (57%)] Loss: 1.57385 \nTraining set [34560/60000 (58%)] Loss: 1.52432 \nTraining set [34880/60000 (58%)] Loss: 1.61486 \nTraining set [35200/60000 (59%)] Loss: 1.61159 \nTraining set [35520/60000 (59%)] Loss: 1.60142 \nTraining set [35840/60000 (60%)] Loss: 1.53391 \nTraining set [36160/60000 (60%)] Loss: 1.53029 \nTraining set [36480/60000 (61%)] Loss: 1.56728 \nTraining set [36800/60000 (61%)] Loss: 1.54993 \nTraining set [37120/60000 (62%)] Loss: 1.58981 \nTraining set [37440/60000 (62%)] Loss: 1.60556 \nTraining set [37760/60000 (63%)] Loss: 1.54932 \nTraining set [38080/60000 (63%)] Loss: 1.54299 \nTraining set [38400/60000 (64%)] Loss: 1.60920 \nTraining set [38720/60000 (64%)] Loss: 1.52928 \nTraining set [39040/60000 (65%)] Loss: 1.63901 \nTraining set [39360/60000 (66%)] Loss: 1.54054 \nTraining set [39680/60000 (66%)] Loss: 1.56942 \nTraining set [40000/60000 (67%)] Loss: 1.66098 \nTraining set [40320/60000 (67%)] Loss: 1.59315 \nTraining set [40640/60000 (68%)] Loss: 1.59122 \nTraining set [40960/60000 (68%)] Loss: 1.57238 \nTraining set [41280/60000 (69%)] Loss: 1.61739 \nTraining set [41600/60000 (69%)] Loss: 1.57561 \nTraining set [41920/60000 (70%)] Loss: 1.58398 \nTraining set [42240/60000 (70%)] Loss: 1.57001 \nTraining set [42560/60000 (71%)] Loss: 1.55465 \nTraining set [42880/60000 (71%)] Loss: 1.68938 \nTraining set [43200/60000 (72%)] Loss: 1.58245 \nTraining set [43520/60000 (72%)] Loss: 1.56981 \nTraining set [43840/60000 (73%)] Loss: 1.58159 \nTraining set [44160/60000 (74%)] Loss: 1.66599 \nTraining set [44480/60000 (74%)] Loss: 1.54850 \nTraining set [44800/60000 (75%)] Loss: 1.57791 \nTraining set [45120/60000 (75%)] Loss: 1.56576 \nTraining set [45440/60000 (76%)] Loss: 1.57463 \nTraining set [45760/60000 (76%)] Loss: 1.57259 \nTraining set [46080/60000 (77%)] Loss: 1.55649 \nTraining set [46400/60000 (77%)] Loss: 1.57151 \nTraining set [46720/60000 (78%)] Loss: 1.54688 \nTraining set [47040/60000 (78%)] Loss: 1.54957 \nTraining set [47360/60000 (79%)] Loss: 1.60423 \nTraining set [47680/60000 (79%)] Loss: 1.54288 \nTraining set [48000/60000 (80%)] Loss: 1.60369 \nTraining set [48320/60000 (80%)] Loss: 1.61120 \nTraining set [48640/60000 (81%)] Loss: 1.51670 \nTraining set [48960/60000 (82%)] Loss: 1.65647 \nTraining set [49280/60000 (82%)] Loss: 1.64309 \nTraining set [49600/60000 (83%)] Loss: 1.60609 \nTraining set [49920/60000 (83%)] Loss: 1.53763 \nTraining set [50240/60000 (84%)] Loss: 1.63750 \nTraining set [50560/60000 (84%)] Loss: 1.52617 \nTraining set [50880/60000 (85%)] Loss: 1.57037 \nTraining set [51200/60000 (85%)] Loss: 1.57027 \nTraining set [51520/60000 (86%)] Loss: 1.52592 \nTraining set [51840/60000 (86%)] Loss: 1.54962 \nTraining set [52160/60000 (87%)] Loss: 1.53855 \nTraining set [52480/60000 (87%)] Loss: 1.54984 \nTraining set [52800/60000 (88%)] Loss: 1.59896 \nTraining set [53120/60000 (88%)] Loss: 1.51759 \nTraining set [53440/60000 (89%)] Loss: 1.55616 \nTraining set [53760/60000 (90%)] Loss: 1.51889 \nTraining set [54080/60000 (90%)] Loss: 1.56848 \nTraining set [54400/60000 (91%)] Loss: 1.57120 \nTraining set [54720/60000 (91%)] Loss: 1.62477 \nTraining set [55040/60000 (92%)] Loss: 1.54687 \nTraining set [55360/60000 (92%)] Loss: 1.56885 \nTraining set [55680/60000 (93%)] Loss: 1.58951 \nTraining set [56000/60000 (93%)] Loss: 1.56905 \nTraining set [56320/60000 (94%)] Loss: 1.53717 \nTraining set [56640/60000 (94%)] Loss: 1.58070 \nTraining set [56960/60000 (95%)] Loss: 1.53706 \nTraining set [57280/60000 (95%)] Loss: 1.55234 \nTraining set [57600/60000 (96%)] Loss: 1.51005 \nTraining set [57920/60000 (96%)] Loss: 1.60724 \nTraining set [58240/60000 (97%)] Loss: 1.52845 \nTraining set [58560/60000 (98%)] Loss: 1.52510 \nTraining set [58880/60000 (98%)] Loss: 1.58135 \nTraining set [59200/60000 (99%)] Loss: 1.52557 \nTraining set [59520/60000 (99%)] Loss: 1.68695 \nTraining set [59840/60000 (100%)] Loss: 1.55949 \nAverage Training Loss: 1.56996\nTraining Completed!\nModel Validation Starts!\nAverage Validation Loss: 1.57748, Accuracy: 8868/10000 (89%)\n\nEpoch: 13\nTraining set [0/60000 (0%)] Loss: 1.51322 \nTraining set [320/60000 (1%)] Loss: 1.57523 \nTraining set [640/60000 (1%)] Loss: 1.54912 \nTraining set [960/60000 (2%)] Loss: 1.60695 \nTraining set [1280/60000 (2%)] Loss: 1.56246 \nTraining set [1600/60000 (3%)] Loss: 1.54955 \nTraining set [1920/60000 (3%)] Loss: 1.63880 \nTraining set [2240/60000 (4%)] Loss: 1.57998 \nTraining set [2560/60000 (4%)] Loss: 1.57352 \nTraining set [2880/60000 (5%)] Loss: 1.55382 \nTraining set [3200/60000 (5%)] Loss: 1.61225 \nTraining set [3520/60000 (6%)] Loss: 1.54367 \nTraining set [3840/60000 (6%)] Loss: 1.56883 \nTraining set [4160/60000 (7%)] Loss: 1.54036 \nTraining set [4480/60000 (7%)] Loss: 1.61061 \nTraining set [4800/60000 (8%)] Loss: 1.59351 \nTraining set [5120/60000 (9%)] Loss: 1.52506 \nTraining set [5440/60000 (9%)] Loss: 1.53734 \nTraining set [5760/60000 (10%)] Loss: 1.58493 \nTraining set [6080/60000 (10%)] Loss: 1.59142 \nTraining set [6400/60000 (11%)] Loss: 1.52056 \nTraining set [6720/60000 (11%)] Loss: 1.57006 \nTraining set [7040/60000 (12%)] Loss: 1.55973 \nTraining set [7360/60000 (12%)] Loss: 1.60139 \nTraining set [7680/60000 (13%)] Loss: 1.60267 \nTraining set [8000/60000 (13%)] Loss: 1.57852 \nTraining set [8320/60000 (14%)] Loss: 1.54392 \nTraining set [8640/60000 (14%)] Loss: 1.55473 \nTraining set [8960/60000 (15%)] Loss: 1.50981 \nTraining set [9280/60000 (15%)] Loss: 1.50696 \nTraining set [9600/60000 (16%)] Loss: 1.49237 \nTraining set [9920/60000 (17%)] Loss: 1.55076 \nTraining set [10240/60000 (17%)] Loss: 1.58872 \nTraining set [10560/60000 (18%)] Loss: 1.55087 \nTraining set [10880/60000 (18%)] Loss: 1.61018 \nTraining set [11200/60000 (19%)] Loss: 1.51510 \nTraining set [11520/60000 (19%)] Loss: 1.55941 \nTraining set [11840/60000 (20%)] Loss: 1.58520 \nTraining set [12160/60000 (20%)] Loss: 1.55264 \nTraining set [12480/60000 (21%)] Loss: 1.54839 \nTraining set [12800/60000 (21%)] Loss: 1.60216 \nTraining set [13120/60000 (22%)] Loss: 1.53197 \nTraining set [13440/60000 (22%)] Loss: 1.55268 \nTraining set [13760/60000 (23%)] Loss: 1.60380 \nTraining set [14080/60000 (23%)] Loss: 1.61683 \nTraining set [14400/60000 (24%)] Loss: 1.53078 \nTraining set [14720/60000 (25%)] Loss: 1.55297 \nTraining set [15040/60000 (25%)] Loss: 1.62568 \nTraining set [15360/60000 (26%)] Loss: 1.61548 \nTraining set [15680/60000 (26%)] Loss: 1.50311 \nTraining set [16000/60000 (27%)] Loss: 1.49062 \nTraining set [16320/60000 (27%)] Loss: 1.55527 \nTraining set [16640/60000 (28%)] Loss: 1.56104 \nTraining set [16960/60000 (28%)] Loss: 1.57134 \nTraining set [17280/60000 (29%)] Loss: 1.60231 \nTraining set [17600/60000 (29%)] Loss: 1.53266 \nTraining set [17920/60000 (30%)] Loss: 1.56724 \nTraining set [18240/60000 (30%)] Loss: 1.55172 \nTraining set [18560/60000 (31%)] Loss: 1.57032 \nTraining set [18880/60000 (31%)] Loss: 1.55854 \nTraining set [19200/60000 (32%)] Loss: 1.56222 \nTraining set [19520/60000 (33%)] Loss: 1.56174 \nTraining set [19840/60000 (33%)] Loss: 1.55102 \nTraining set [20160/60000 (34%)] Loss: 1.55513 \nTraining set [20480/60000 (34%)] Loss: 1.61272 \nTraining set [20800/60000 (35%)] Loss: 1.54342 \nTraining set [21120/60000 (35%)] Loss: 1.58668 \nTraining set [21440/60000 (36%)] Loss: 1.54829 \nTraining set [21760/60000 (36%)] Loss: 1.52782 \nTraining set [22080/60000 (37%)] Loss: 1.60762 \nTraining set [22400/60000 (37%)] Loss: 1.51151 \nTraining set [22720/60000 (38%)] Loss: 1.52800 \nTraining set [23040/60000 (38%)] Loss: 1.55539 \nTraining set [23360/60000 (39%)] Loss: 1.57868 \nTraining set [23680/60000 (39%)] Loss: 1.65312 \nTraining set [24000/60000 (40%)] Loss: 1.58759 \nTraining set [24320/60000 (41%)] Loss: 1.55441 \nTraining set [24640/60000 (41%)] Loss: 1.57737 \nTraining set [24960/60000 (42%)] Loss: 1.55815 \nTraining set [25280/60000 (42%)] Loss: 1.50115 \nTraining set [25600/60000 (43%)] Loss: 1.57856 \nTraining set [25920/60000 (43%)] Loss: 1.57675 \nTraining set [26240/60000 (44%)] Loss: 1.60729 \nTraining set [26560/60000 (44%)] Loss: 1.54868 \nTraining set [26880/60000 (45%)] Loss: 1.48987 \nTraining set [27200/60000 (45%)] Loss: 1.61427 \nTraining set [27520/60000 (46%)] Loss: 1.62562 \nTraining set [27840/60000 (46%)] Loss: 1.56671 \nTraining set [28160/60000 (47%)] Loss: 1.53135 \nTraining set [28480/60000 (47%)] Loss: 1.56180 \nTraining set [28800/60000 (48%)] Loss: 1.54546 \nTraining set [29120/60000 (49%)] Loss: 1.55302 \nTraining set [29440/60000 (49%)] Loss: 1.56943 \nTraining set [29760/60000 (50%)] Loss: 1.52971 \nTraining set [30080/60000 (50%)] Loss: 1.51186 \nTraining set [30400/60000 (51%)] Loss: 1.58419 \nTraining set [30720/60000 (51%)] Loss: 1.56963 \nTraining set [31040/60000 (52%)] Loss: 1.66341 \nTraining set [31360/60000 (52%)] Loss: 1.56375 \nTraining set [31680/60000 (53%)] Loss: 1.54178 \nTraining set [32000/60000 (53%)] Loss: 1.54447 \nTraining set [32320/60000 (54%)] Loss: 1.55368 \nTraining set [32640/60000 (54%)] Loss: 1.54480 \nTraining set [32960/60000 (55%)] Loss: 1.57824 \nTraining set [33280/60000 (55%)] Loss: 1.59081 \nTraining set [33600/60000 (56%)] Loss: 1.60941 \nTraining set [33920/60000 (57%)] Loss: 1.63955 \nTraining set [34240/60000 (57%)] Loss: 1.50512 \nTraining set [34560/60000 (58%)] Loss: 1.57546 \nTraining set [34880/60000 (58%)] Loss: 1.51085 \nTraining set [35200/60000 (59%)] Loss: 1.54448 \nTraining set [35520/60000 (59%)] Loss: 1.63014 \nTraining set [35840/60000 (60%)] Loss: 1.61296 \nTraining set [36160/60000 (60%)] Loss: 1.54728 \nTraining set [36480/60000 (61%)] Loss: 1.57985 \nTraining set [36800/60000 (61%)] Loss: 1.57595 \nTraining set [37120/60000 (62%)] Loss: 1.57346 \nTraining set [37440/60000 (62%)] Loss: 1.57032 \nTraining set [37760/60000 (63%)] Loss: 1.55887 \nTraining set [38080/60000 (63%)] Loss: 1.57581 \nTraining set [38400/60000 (64%)] Loss: 1.62866 \nTraining set [38720/60000 (64%)] Loss: 1.56424 \nTraining set [39040/60000 (65%)] Loss: 1.56831 \nTraining set [39360/60000 (66%)] Loss: 1.53654 \nTraining set [39680/60000 (66%)] Loss: 1.56940 \nTraining set [40000/60000 (67%)] Loss: 1.60717 \nTraining set [40320/60000 (67%)] Loss: 1.60420 \nTraining set [40640/60000 (68%)] Loss: 1.59992 \nTraining set [40960/60000 (68%)] Loss: 1.61370 \nTraining set [41280/60000 (69%)] Loss: 1.50128 \nTraining set [41600/60000 (69%)] Loss: 1.56221 \nTraining set [41920/60000 (70%)] Loss: 1.61395 \nTraining set [42240/60000 (70%)] Loss: 1.53741 \nTraining set [42560/60000 (71%)] Loss: 1.58538 \nTraining set [42880/60000 (71%)] Loss: 1.66590 \nTraining set [43200/60000 (72%)] Loss: 1.57565 \nTraining set [43520/60000 (72%)] Loss: 1.58377 \nTraining set [43840/60000 (73%)] Loss: 1.60924 \nTraining set [44160/60000 (74%)] Loss: 1.55805 \nTraining set [44480/60000 (74%)] Loss: 1.51680 \nTraining set [44800/60000 (75%)] Loss: 1.54857 \nTraining set [45120/60000 (75%)] Loss: 1.58710 \nTraining set [45440/60000 (76%)] Loss: 1.57689 \nTraining set [45760/60000 (76%)] Loss: 1.55675 \nTraining set [46080/60000 (77%)] Loss: 1.63740 \nTraining set [46400/60000 (77%)] Loss: 1.53810 \nTraining set [46720/60000 (78%)] Loss: 1.58984 \nTraining set [47040/60000 (78%)] Loss: 1.60772 \nTraining set [47360/60000 (79%)] Loss: 1.52329 \nTraining set [47680/60000 (79%)] Loss: 1.57068 \nTraining set [48000/60000 (80%)] Loss: 1.63381 \nTraining set [48320/60000 (80%)] Loss: 1.55083 \nTraining set [48640/60000 (81%)] Loss: 1.64141 \nTraining set [48960/60000 (82%)] Loss: 1.55039 \nTraining set [49280/60000 (82%)] Loss: 1.58230 \nTraining set [49600/60000 (83%)] Loss: 1.58073 \nTraining set [49920/60000 (83%)] Loss: 1.51706 \nTraining set [50240/60000 (84%)] Loss: 1.55057 \nTraining set [50560/60000 (84%)] Loss: 1.51280 \nTraining set [50880/60000 (85%)] Loss: 1.59286 \nTraining set [51200/60000 (85%)] Loss: 1.52023 \nTraining set [51520/60000 (86%)] Loss: 1.60878 \nTraining set [51840/60000 (86%)] Loss: 1.52765 \nTraining set [52160/60000 (87%)] Loss: 1.50711 \nTraining set [52480/60000 (87%)] Loss: 1.56005 \nTraining set [52800/60000 (88%)] Loss: 1.56630 \nTraining set [53120/60000 (88%)] Loss: 1.54008 \nTraining set [53440/60000 (89%)] Loss: 1.55279 \nTraining set [53760/60000 (90%)] Loss: 1.55094 \nTraining set [54080/60000 (90%)] Loss: 1.58446 \nTraining set [54400/60000 (91%)] Loss: 1.58871 \nTraining set [54720/60000 (91%)] Loss: 1.58801 \nTraining set [55040/60000 (92%)] Loss: 1.52049 \nTraining set [55360/60000 (92%)] Loss: 1.57061 \nTraining set [55680/60000 (93%)] Loss: 1.59342 \nTraining set [56000/60000 (93%)] Loss: 1.59442 \nTraining set [56320/60000 (94%)] Loss: 1.55222 \nTraining set [56640/60000 (94%)] Loss: 1.49272 \nTraining set [56960/60000 (95%)] Loss: 1.57359 \nTraining set [57280/60000 (95%)] Loss: 1.56185 \nTraining set [57600/60000 (96%)] Loss: 1.61554 \nTraining set [57920/60000 (96%)] Loss: 1.59190 \nTraining set [58240/60000 (97%)] Loss: 1.51792 \nTraining set [58560/60000 (98%)] Loss: 1.58244 \nTraining set [58880/60000 (98%)] Loss: 1.59586 \nTraining set [59200/60000 (99%)] Loss: 1.59053 \nTraining set [59520/60000 (99%)] Loss: 1.58336 \nTraining set [59840/60000 (100%)] Loss: 1.55876 \nAverage Training Loss: 1.56754\nTraining Completed!\nModel Validation Starts!\nAverage Validation Loss: 1.57442, Accuracy: 8880/10000 (89%)\n\nEpoch: 14\nTraining set [0/60000 (0%)] Loss: 1.54481 \nTraining set [320/60000 (1%)] Loss: 1.67161 \nTraining set [640/60000 (1%)] Loss: 1.58004 \nTraining set [960/60000 (2%)] Loss: 1.56787 \nTraining set [1280/60000 (2%)] Loss: 1.55733 \nTraining set [1600/60000 (3%)] Loss: 1.60296 \nTraining set [1920/60000 (3%)] Loss: 1.58197 \nTraining set [2240/60000 (4%)] Loss: 1.58282 \nTraining set [2560/60000 (4%)] Loss: 1.58562 \nTraining set [2880/60000 (5%)] Loss: 1.56896 \nTraining set [3200/60000 (5%)] Loss: 1.48921 \nTraining set [3520/60000 (6%)] Loss: 1.54461 \nTraining set [3840/60000 (6%)] Loss: 1.61175 \nTraining set [4160/60000 (7%)] Loss: 1.56775 \nTraining set [4480/60000 (7%)] Loss: 1.54717 \nTraining set [4800/60000 (8%)] Loss: 1.60032 \nTraining set [5120/60000 (9%)] Loss: 1.55455 \nTraining set [5440/60000 (9%)] Loss: 1.58389 \nTraining set [5760/60000 (10%)] Loss: 1.52634 \nTraining set [6080/60000 (10%)] Loss: 1.57794 \nTraining set [6400/60000 (11%)] Loss: 1.64424 \nTraining set [6720/60000 (11%)] Loss: 1.58907 \nTraining set [7040/60000 (12%)] Loss: 1.53510 \nTraining set [7360/60000 (12%)] Loss: 1.61898 \nTraining set [7680/60000 (13%)] Loss: 1.58510 \nTraining set [8000/60000 (13%)] Loss: 1.58084 \nTraining set [8320/60000 (14%)] Loss: 1.55642 \nTraining set [8640/60000 (14%)] Loss: 1.56665 \nTraining set [8960/60000 (15%)] Loss: 1.52233 \nTraining set [9280/60000 (15%)] Loss: 1.53087 \nTraining set [9600/60000 (16%)] Loss: 1.65702 \nTraining set [9920/60000 (17%)] Loss: 1.53348 \nTraining set [10240/60000 (17%)] Loss: 1.54312 \nTraining set [10560/60000 (18%)] Loss: 1.54500 \nTraining set [10880/60000 (18%)] Loss: 1.58986 \nTraining set [11200/60000 (19%)] Loss: 1.50359 \nTraining set [11520/60000 (19%)] Loss: 1.60246 \nTraining set [11840/60000 (20%)] Loss: 1.54468 \nTraining set [12160/60000 (20%)] Loss: 1.60301 \nTraining set [12480/60000 (21%)] Loss: 1.52745 \nTraining set [12800/60000 (21%)] Loss: 1.57584 \nTraining set [13120/60000 (22%)] Loss: 1.50234 \nTraining set [13440/60000 (22%)] Loss: 1.53585 \nTraining set [13760/60000 (23%)] Loss: 1.60949 \nTraining set [14080/60000 (23%)] Loss: 1.57793 \nTraining set [14400/60000 (24%)] Loss: 1.52069 \nTraining set [14720/60000 (25%)] Loss: 1.58734 \nTraining set [15040/60000 (25%)] Loss: 1.55551 \nTraining set [15360/60000 (26%)] Loss: 1.57139 \nTraining set [15680/60000 (26%)] Loss: 1.58795 \nTraining set [16000/60000 (27%)] Loss: 1.59959 \nTraining set [16320/60000 (27%)] Loss: 1.57683 \nTraining set [16640/60000 (28%)] Loss: 1.53277 \nTraining set [16960/60000 (28%)] Loss: 1.54954 \nTraining set [17280/60000 (29%)] Loss: 1.50695 \nTraining set [17600/60000 (29%)] Loss: 1.54575 \nTraining set [17920/60000 (30%)] Loss: 1.58688 \nTraining set [18240/60000 (30%)] Loss: 1.52353 \nTraining set [18560/60000 (31%)] Loss: 1.53206 \nTraining set [18880/60000 (31%)] Loss: 1.62720 \nTraining set [19200/60000 (32%)] Loss: 1.60261 \nTraining set [19520/60000 (33%)] Loss: 1.54706 \nTraining set [19840/60000 (33%)] Loss: 1.63383 \nTraining set [20160/60000 (34%)] Loss: 1.51967 \nTraining set [20480/60000 (34%)] Loss: 1.51161 \nTraining set [20800/60000 (35%)] Loss: 1.61992 \nTraining set [21120/60000 (35%)] Loss: 1.50127 \nTraining set [21440/60000 (36%)] Loss: 1.52753 \nTraining set [21760/60000 (36%)] Loss: 1.51962 \nTraining set [22080/60000 (37%)] Loss: 1.61687 \nTraining set [22400/60000 (37%)] Loss: 1.54258 \nTraining set [22720/60000 (38%)] Loss: 1.52822 \nTraining set [23040/60000 (38%)] Loss: 1.57017 \nTraining set [23360/60000 (39%)] Loss: 1.54321 \nTraining set [23680/60000 (39%)] Loss: 1.57817 \nTraining set [24000/60000 (40%)] Loss: 1.57149 \nTraining set [24320/60000 (41%)] Loss: 1.64900 \nTraining set [24640/60000 (41%)] Loss: 1.52208 \nTraining set [24960/60000 (42%)] Loss: 1.57278 \nTraining set [25280/60000 (42%)] Loss: 1.55272 \nTraining set [25600/60000 (43%)] Loss: 1.59561 \nTraining set [25920/60000 (43%)] Loss: 1.54904 \nTraining set [26240/60000 (44%)] Loss: 1.58177 \nTraining set [26560/60000 (44%)] Loss: 1.54209 \nTraining set [26880/60000 (45%)] Loss: 1.51917 \nTraining set [27200/60000 (45%)] Loss: 1.61231 \nTraining set [27520/60000 (46%)] Loss: 1.53320 \nTraining set [27840/60000 (46%)] Loss: 1.58018 \nTraining set [28160/60000 (47%)] Loss: 1.53804 \nTraining set [28480/60000 (47%)] Loss: 1.56066 \nTraining set [28800/60000 (48%)] Loss: 1.56511 \nTraining set [29120/60000 (49%)] Loss: 1.58074 \nTraining set [29440/60000 (49%)] Loss: 1.50272 \nTraining set [29760/60000 (50%)] Loss: 1.57614 \nTraining set [30080/60000 (50%)] Loss: 1.59697 \nTraining set [30400/60000 (51%)] Loss: 1.54454 \nTraining set [30720/60000 (51%)] Loss: 1.54301 \nTraining set [31040/60000 (52%)] Loss: 1.61244 \nTraining set [31360/60000 (52%)] Loss: 1.58808 \nTraining set [31680/60000 (53%)] Loss: 1.63785 \nTraining set [32000/60000 (53%)] Loss: 1.55681 \nTraining set [32320/60000 (54%)] Loss: 1.55791 \nTraining set [32640/60000 (54%)] Loss: 1.55491 \nTraining set [32960/60000 (55%)] Loss: 1.58345 \nTraining set [33280/60000 (55%)] Loss: 1.54422 \nTraining set [33600/60000 (56%)] Loss: 1.52821 \nTraining set [33920/60000 (57%)] Loss: 1.50554 \nTraining set [34240/60000 (57%)] Loss: 1.55438 \nTraining set [34560/60000 (58%)] Loss: 1.55551 \nTraining set [34880/60000 (58%)] Loss: 1.57226 \nTraining set [35200/60000 (59%)] Loss: 1.56017 \nTraining set [35520/60000 (59%)] Loss: 1.60603 \nTraining set [35840/60000 (60%)] Loss: 1.58357 \nTraining set [36160/60000 (60%)] Loss: 1.56911 \nTraining set [36480/60000 (61%)] Loss: 1.59395 \nTraining set [36800/60000 (61%)] Loss: 1.57997 \nTraining set [37120/60000 (62%)] Loss: 1.61516 \nTraining set [37440/60000 (62%)] Loss: 1.58373 \nTraining set [37760/60000 (63%)] Loss: 1.54272 \nTraining set [38080/60000 (63%)] Loss: 1.57475 \nTraining set [38400/60000 (64%)] Loss: 1.68386 \nTraining set [38720/60000 (64%)] Loss: 1.53186 \nTraining set [39040/60000 (65%)] Loss: 1.58620 \nTraining set [39360/60000 (66%)] Loss: 1.52882 \nTraining set [39680/60000 (66%)] Loss: 1.57392 \nTraining set [40000/60000 (67%)] Loss: 1.55560 \nTraining set [40320/60000 (67%)] Loss: 1.59609 \nTraining set [40640/60000 (68%)] Loss: 1.54353 \nTraining set [40960/60000 (68%)] Loss: 1.54910 \nTraining set [41280/60000 (69%)] Loss: 1.59412 \nTraining set [41600/60000 (69%)] Loss: 1.61421 \nTraining set [41920/60000 (70%)] Loss: 1.57512 \nTraining set [42240/60000 (70%)] Loss: 1.56413 \nTraining set [42560/60000 (71%)] Loss: 1.48354 \nTraining set [42880/60000 (71%)] Loss: 1.55966 \nTraining set [43200/60000 (72%)] Loss: 1.55323 \nTraining set [43520/60000 (72%)] Loss: 1.53402 \nTraining set [43840/60000 (73%)] Loss: 1.52789 \nTraining set [44160/60000 (74%)] Loss: 1.62828 \nTraining set [44480/60000 (74%)] Loss: 1.55442 \nTraining set [44800/60000 (75%)] Loss: 1.54756 \nTraining set [45120/60000 (75%)] Loss: 1.62795 \nTraining set [45440/60000 (76%)] Loss: 1.54603 \nTraining set [45760/60000 (76%)] Loss: 1.54491 \nTraining set [46080/60000 (77%)] Loss: 1.53920 \nTraining set [46400/60000 (77%)] Loss: 1.52706 \nTraining set [46720/60000 (78%)] Loss: 1.57193 \nTraining set [47040/60000 (78%)] Loss: 1.55720 \nTraining set [47360/60000 (79%)] Loss: 1.53869 \nTraining set [47680/60000 (79%)] Loss: 1.56494 \nTraining set [48000/60000 (80%)] Loss: 1.57609 \nTraining set [48320/60000 (80%)] Loss: 1.59270 \nTraining set [48640/60000 (81%)] Loss: 1.57302 \nTraining set [48960/60000 (82%)] Loss: 1.55002 \nTraining set [49280/60000 (82%)] Loss: 1.52606 \nTraining set [49600/60000 (83%)] Loss: 1.49626 \nTraining set [49920/60000 (83%)] Loss: 1.58189 \nTraining set [50240/60000 (84%)] Loss: 1.51443 \nTraining set [50560/60000 (84%)] Loss: 1.52931 \nTraining set [50880/60000 (85%)] Loss: 1.64766 \nTraining set [51200/60000 (85%)] Loss: 1.62697 \nTraining set [51520/60000 (86%)] Loss: 1.56123 \nTraining set [51840/60000 (86%)] Loss: 1.59031 \nTraining set [52160/60000 (87%)] Loss: 1.55747 \nTraining set [52480/60000 (87%)] Loss: 1.57531 \nTraining set [52800/60000 (88%)] Loss: 1.61777 \nTraining set [53120/60000 (88%)] Loss: 1.59674 \nTraining set [53440/60000 (89%)] Loss: 1.61660 \nTraining set [53760/60000 (90%)] Loss: 1.53801 \nTraining set [54080/60000 (90%)] Loss: 1.56638 \nTraining set [54400/60000 (91%)] Loss: 1.59720 \nTraining set [54720/60000 (91%)] Loss: 1.56092 \nTraining set [55040/60000 (92%)] Loss: 1.53478 \nTraining set [55360/60000 (92%)] Loss: 1.53120 \nTraining set [55680/60000 (93%)] Loss: 1.57214 \nTraining set [56000/60000 (93%)] Loss: 1.58658 \nTraining set [56320/60000 (94%)] Loss: 1.54148 \nTraining set [56640/60000 (94%)] Loss: 1.57190 \nTraining set [56960/60000 (95%)] Loss: 1.54440 \nTraining set [57280/60000 (95%)] Loss: 1.55066 \nTraining set [57600/60000 (96%)] Loss: 1.57926 \nTraining set [57920/60000 (96%)] Loss: 1.56457 \nTraining set [58240/60000 (97%)] Loss: 1.54426 \nTraining set [58560/60000 (98%)] Loss: 1.54647 \nTraining set [58880/60000 (98%)] Loss: 1.54471 \nTraining set [59200/60000 (99%)] Loss: 1.55695 \nTraining set [59520/60000 (99%)] Loss: 1.53443 \nTraining set [59840/60000 (100%)] Loss: 1.57148 \nAverage Training Loss: 1.56477\nTraining Completed!\nModel Validation Starts!\nAverage Validation Loss: 1.57362, Accuracy: 8907/10000 (89%)\n\nEpoch: 15\nTraining set [0/60000 (0%)] Loss: 1.51538 \nTraining set [320/60000 (1%)] Loss: 1.55019 \nTraining set [640/60000 (1%)] Loss: 1.54124 \nTraining set [960/60000 (2%)] Loss: 1.54118 \nTraining set [1280/60000 (2%)] Loss: 1.52099 \nTraining set [1600/60000 (3%)] Loss: 1.65654 \nTraining set [1920/60000 (3%)] Loss: 1.56434 \nTraining set [2240/60000 (4%)] Loss: 1.55736 \nTraining set [2560/60000 (4%)] Loss: 1.50580 \nTraining set [2880/60000 (5%)] Loss: 1.56984 \nTraining set [3200/60000 (5%)] Loss: 1.51922 \nTraining set [3520/60000 (6%)] Loss: 1.53270 \nTraining set [3840/60000 (6%)] Loss: 1.56671 \nTraining set [4160/60000 (7%)] Loss: 1.61447 \nTraining set [4480/60000 (7%)] Loss: 1.55657 \nTraining set [4800/60000 (8%)] Loss: 1.63810 \nTraining set [5120/60000 (9%)] Loss: 1.54366 \nTraining set [5440/60000 (9%)] Loss: 1.56396 \nTraining set [5760/60000 (10%)] Loss: 1.56364 \nTraining set [6080/60000 (10%)] Loss: 1.53618 \nTraining set [6400/60000 (11%)] Loss: 1.55465 \nTraining set [6720/60000 (11%)] Loss: 1.67027 \nTraining set [7040/60000 (12%)] Loss: 1.55133 \nTraining set [7360/60000 (12%)] Loss: 1.60210 \nTraining set [7680/60000 (13%)] Loss: 1.59558 \nTraining set [8000/60000 (13%)] Loss: 1.60204 \nTraining set [8320/60000 (14%)] Loss: 1.52629 \nTraining set [8640/60000 (14%)] Loss: 1.57752 \nTraining set [8960/60000 (15%)] Loss: 1.60265 \nTraining set [9280/60000 (15%)] Loss: 1.53851 \nTraining set [9600/60000 (16%)] Loss: 1.56306 \nTraining set [9920/60000 (17%)] Loss: 1.48804 \nTraining set [10240/60000 (17%)] Loss: 1.53725 \nTraining set [10560/60000 (18%)] Loss: 1.52733 \nTraining set [10880/60000 (18%)] Loss: 1.52586 \nTraining set [11200/60000 (19%)] Loss: 1.54994 \nTraining set [11520/60000 (19%)] Loss: 1.50243 \nTraining set [11840/60000 (20%)] Loss: 1.48187 \nTraining set [12160/60000 (20%)] Loss: 1.53750 \nTraining set [12480/60000 (21%)] Loss: 1.53827 \nTraining set [12800/60000 (21%)] Loss: 1.59326 \nTraining set [13120/60000 (22%)] Loss: 1.58989 \nTraining set [13440/60000 (22%)] Loss: 1.61632 \nTraining set [13760/60000 (23%)] Loss: 1.57915 \nTraining set [14080/60000 (23%)] Loss: 1.51399 \nTraining set [14400/60000 (24%)] Loss: 1.54312 \nTraining set [14720/60000 (25%)] Loss: 1.58878 \nTraining set [15040/60000 (25%)] Loss: 1.56603 \nTraining set [15360/60000 (26%)] Loss: 1.54012 \nTraining set [15680/60000 (26%)] Loss: 1.52188 \nTraining set [16000/60000 (27%)] Loss: 1.53139 \nTraining set [16320/60000 (27%)] Loss: 1.57117 \nTraining set [16640/60000 (28%)] Loss: 1.52919 \nTraining set [16960/60000 (28%)] Loss: 1.58466 \nTraining set [17280/60000 (29%)] Loss: 1.56497 \nTraining set [17600/60000 (29%)] Loss: 1.58642 \nTraining set [17920/60000 (30%)] Loss: 1.57969 \nTraining set [18240/60000 (30%)] Loss: 1.58250 \nTraining set [18560/60000 (31%)] Loss: 1.50990 \nTraining set [18880/60000 (31%)] Loss: 1.55863 \nTraining set [19200/60000 (32%)] Loss: 1.61826 \nTraining set [19520/60000 (33%)] Loss: 1.55754 \nTraining set [19840/60000 (33%)] Loss: 1.62826 \nTraining set [20160/60000 (34%)] Loss: 1.54696 \nTraining set [20480/60000 (34%)] Loss: 1.56917 \nTraining set [20800/60000 (35%)] Loss: 1.56149 \nTraining set [21120/60000 (35%)] Loss: 1.59971 \nTraining set [21440/60000 (36%)] Loss: 1.62723 \nTraining set [21760/60000 (36%)] Loss: 1.54019 \nTraining set [22080/60000 (37%)] Loss: 1.55813 \nTraining set [22400/60000 (37%)] Loss: 1.58678 \nTraining set [22720/60000 (38%)] Loss: 1.56239 \nTraining set [23040/60000 (38%)] Loss: 1.52435 \nTraining set [23360/60000 (39%)] Loss: 1.50739 \nTraining set [23680/60000 (39%)] Loss: 1.61495 \nTraining set [24000/60000 (40%)] Loss: 1.55419 \nTraining set [24320/60000 (41%)] Loss: 1.59098 \nTraining set [24640/60000 (41%)] Loss: 1.53984 \nTraining set [24960/60000 (42%)] Loss: 1.54976 \nTraining set [25280/60000 (42%)] Loss: 1.51873 \nTraining set [25600/60000 (43%)] Loss: 1.54558 \nTraining set [25920/60000 (43%)] Loss: 1.54194 \nTraining set [26240/60000 (44%)] Loss: 1.54106 \nTraining set [26560/60000 (44%)] Loss: 1.54336 \nTraining set [26880/60000 (45%)] Loss: 1.59379 \nTraining set [27200/60000 (45%)] Loss: 1.53359 \nTraining set [27520/60000 (46%)] Loss: 1.51219 \nTraining set [27840/60000 (46%)] Loss: 1.55573 \nTraining set [28160/60000 (47%)] Loss: 1.55298 \nTraining set [28480/60000 (47%)] Loss: 1.54613 \nTraining set [28800/60000 (48%)] Loss: 1.53555 \nTraining set [29120/60000 (49%)] Loss: 1.65879 \nTraining set [29440/60000 (49%)] Loss: 1.51245 \nTraining set [29760/60000 (50%)] Loss: 1.54163 \nTraining set [30080/60000 (50%)] Loss: 1.66759 \nTraining set [30400/60000 (51%)] Loss: 1.53023 \nTraining set [30720/60000 (51%)] Loss: 1.52383 \nTraining set [31040/60000 (52%)] Loss: 1.54159 \nTraining set [31360/60000 (52%)] Loss: 1.56080 \nTraining set [31680/60000 (53%)] Loss: 1.56848 \nTraining set [32000/60000 (53%)] Loss: 1.57041 \nTraining set [32320/60000 (54%)] Loss: 1.56159 \nTraining set [32640/60000 (54%)] Loss: 1.54927 \nTraining set [32960/60000 (55%)] Loss: 1.58388 \nTraining set [33280/60000 (55%)] Loss: 1.54777 \nTraining set [33600/60000 (56%)] Loss: 1.55932 \nTraining set [33920/60000 (57%)] Loss: 1.50711 \nTraining set [34240/60000 (57%)] Loss: 1.60563 \nTraining set [34560/60000 (58%)] Loss: 1.64550 \nTraining set [34880/60000 (58%)] Loss: 1.57981 \nTraining set [35200/60000 (59%)] Loss: 1.60672 \nTraining set [35520/60000 (59%)] Loss: 1.57275 \nTraining set [35840/60000 (60%)] Loss: 1.63044 \nTraining set [36160/60000 (60%)] Loss: 1.56780 \nTraining set [36480/60000 (61%)] Loss: 1.52915 \nTraining set [36800/60000 (61%)] Loss: 1.55671 \nTraining set [37120/60000 (62%)] Loss: 1.51431 \nTraining set [37440/60000 (62%)] Loss: 1.57065 \nTraining set [37760/60000 (63%)] Loss: 1.48642 \nTraining set [38080/60000 (63%)] Loss: 1.58527 \nTraining set [38400/60000 (64%)] Loss: 1.59528 \nTraining set [38720/60000 (64%)] Loss: 1.54518 \nTraining set [39040/60000 (65%)] Loss: 1.59515 \nTraining set [39360/60000 (66%)] Loss: 1.53140 \nTraining set [39680/60000 (66%)] Loss: 1.52407 \nTraining set [40000/60000 (67%)] Loss: 1.55321 \nTraining set [40320/60000 (67%)] Loss: 1.59341 \nTraining set [40640/60000 (68%)] Loss: 1.55677 \nTraining set [40960/60000 (68%)] Loss: 1.53901 \nTraining set [41280/60000 (69%)] Loss: 1.54464 \nTraining set [41600/60000 (69%)] Loss: 1.57491 \nTraining set [41920/60000 (70%)] Loss: 1.55541 \nTraining set [42240/60000 (70%)] Loss: 1.51580 \nTraining set [42560/60000 (71%)] Loss: 1.59044 \nTraining set [42880/60000 (71%)] Loss: 1.58148 \nTraining set [43200/60000 (72%)] Loss: 1.59128 \nTraining set [43520/60000 (72%)] Loss: 1.48775 \nTraining set [43840/60000 (73%)] Loss: 1.49434 \nTraining set [44160/60000 (74%)] Loss: 1.55904 \nTraining set [44480/60000 (74%)] Loss: 1.58355 \nTraining set [44800/60000 (75%)] Loss: 1.56423 \nTraining set [45120/60000 (75%)] Loss: 1.55277 \nTraining set [45440/60000 (76%)] Loss: 1.52944 \nTraining set [45760/60000 (76%)] Loss: 1.57429 \nTraining set [46080/60000 (77%)] Loss: 1.58237 \nTraining set [46400/60000 (77%)] Loss: 1.53888 \nTraining set [46720/60000 (78%)] Loss: 1.55919 \nTraining set [47040/60000 (78%)] Loss: 1.55291 \nTraining set [47360/60000 (79%)] Loss: 1.54371 \nTraining set [47680/60000 (79%)] Loss: 1.50015 \nTraining set [48000/60000 (80%)] Loss: 1.60255 \nTraining set [48320/60000 (80%)] Loss: 1.53688 \nTraining set [48640/60000 (81%)] Loss: 1.55667 \nTraining set [48960/60000 (82%)] Loss: 1.56040 \nTraining set [49280/60000 (82%)] Loss: 1.52274 \nTraining set [49600/60000 (83%)] Loss: 1.54106 \nTraining set [49920/60000 (83%)] Loss: 1.54718 \nTraining set [50240/60000 (84%)] Loss: 1.55698 \nTraining set [50560/60000 (84%)] Loss: 1.57526 \nTraining set [50880/60000 (85%)] Loss: 1.51448 \nTraining set [51200/60000 (85%)] Loss: 1.57218 \nTraining set [51520/60000 (86%)] Loss: 1.47915 \nTraining set [51840/60000 (86%)] Loss: 1.56354 \nTraining set [52160/60000 (87%)] Loss: 1.50138 \nTraining set [52480/60000 (87%)] Loss: 1.59431 \nTraining set [52800/60000 (88%)] Loss: 1.59165 \nTraining set [53120/60000 (88%)] Loss: 1.56358 \nTraining set [53440/60000 (89%)] Loss: 1.57304 \nTraining set [53760/60000 (90%)] Loss: 1.50036 \nTraining set [54080/60000 (90%)] Loss: 1.56196 \nTraining set [54400/60000 (91%)] Loss: 1.62191 \nTraining set [54720/60000 (91%)] Loss: 1.55302 \nTraining set [55040/60000 (92%)] Loss: 1.56961 \nTraining set [55360/60000 (92%)] Loss: 1.50502 \nTraining set [55680/60000 (93%)] Loss: 1.50452 \nTraining set [56000/60000 (93%)] Loss: 1.55973 \nTraining set [56320/60000 (94%)] Loss: 1.54217 \nTraining set [56640/60000 (94%)] Loss: 1.61029 \nTraining set [56960/60000 (95%)] Loss: 1.54398 \nTraining set [57280/60000 (95%)] Loss: 1.54553 \nTraining set [57600/60000 (96%)] Loss: 1.63200 \nTraining set [57920/60000 (96%)] Loss: 1.53229 \nTraining set [58240/60000 (97%)] Loss: 1.57898 \nTraining set [58560/60000 (98%)] Loss: 1.50859 \nTraining set [58880/60000 (98%)] Loss: 1.59433 \nTraining set [59200/60000 (99%)] Loss: 1.60948 \nTraining set [59520/60000 (99%)] Loss: 1.53683 \nTraining set [59840/60000 (100%)] Loss: 1.56472 \nAverage Training Loss: 1.56084\nTraining Completed!\nModel Validation Starts!\nAverage Validation Loss: 1.56860, Accuracy: 8946/10000 (89%)\n\nEpoch: 16\nTraining set [0/60000 (0%)] Loss: 1.54272 \nTraining set [320/60000 (1%)] Loss: 1.52997 \nTraining set [640/60000 (1%)] Loss: 1.54423 \nTraining set [960/60000 (2%)] Loss: 1.56179 \nTraining set [1280/60000 (2%)] Loss: 1.47149 \nTraining set [1600/60000 (3%)] Loss: 1.56694 \nTraining set [1920/60000 (3%)] Loss: 1.49773 \nTraining set [2240/60000 (4%)] Loss: 1.53721 \nTraining set [2560/60000 (4%)] Loss: 1.56487 \nTraining set [2880/60000 (5%)] Loss: 1.60291 \nTraining set [3200/60000 (5%)] Loss: 1.59132 \nTraining set [3520/60000 (6%)] Loss: 1.68923 \nTraining set [3840/60000 (6%)] Loss: 1.58587 \nTraining set [4160/60000 (7%)] Loss: 1.61164 \nTraining set [4480/60000 (7%)] Loss: 1.55639 \nTraining set [4800/60000 (8%)] Loss: 1.51718 \nTraining set [5120/60000 (9%)] Loss: 1.52692 \nTraining set [5440/60000 (9%)] Loss: 1.57707 \nTraining set [5760/60000 (10%)] Loss: 1.58361 \nTraining set [6080/60000 (10%)] Loss: 1.56609 \nTraining set [6400/60000 (11%)] Loss: 1.54476 \nTraining set [6720/60000 (11%)] Loss: 1.55274 \nTraining set [7040/60000 (12%)] Loss: 1.58453 \nTraining set [7360/60000 (12%)] Loss: 1.51892 \nTraining set [7680/60000 (13%)] Loss: 1.54227 \nTraining set [8000/60000 (13%)] Loss: 1.52416 \nTraining set [8320/60000 (14%)] Loss: 1.54710 \nTraining set [8640/60000 (14%)] Loss: 1.57247 \nTraining set [8960/60000 (15%)] Loss: 1.55594 \nTraining set [9280/60000 (15%)] Loss: 1.53378 \nTraining set [9600/60000 (16%)] Loss: 1.54054 \nTraining set [9920/60000 (17%)] Loss: 1.55961 \nTraining set [10240/60000 (17%)] Loss: 1.54632 \nTraining set [10560/60000 (18%)] Loss: 1.60713 \nTraining set [10880/60000 (18%)] Loss: 1.54139 \nTraining set [11200/60000 (19%)] Loss: 1.51097 \nTraining set [11520/60000 (19%)] Loss: 1.59694 \nTraining set [11840/60000 (20%)] Loss: 1.57067 \nTraining set [12160/60000 (20%)] Loss: 1.55372 \nTraining set [12480/60000 (21%)] Loss: 1.57834 \nTraining set [12800/60000 (21%)] Loss: 1.53200 \nTraining set [13120/60000 (22%)] Loss: 1.55619 \nTraining set [13440/60000 (22%)] Loss: 1.56282 \nTraining set [13760/60000 (23%)] Loss: 1.53889 \nTraining set [14080/60000 (23%)] Loss: 1.56842 \nTraining set [14400/60000 (24%)] Loss: 1.58207 \nTraining set [14720/60000 (25%)] Loss: 1.55060 \nTraining set [15040/60000 (25%)] Loss: 1.51092 \nTraining set [15360/60000 (26%)] Loss: 1.59931 \nTraining set [15680/60000 (26%)] Loss: 1.60330 \nTraining set [16000/60000 (27%)] Loss: 1.55831 \nTraining set [16320/60000 (27%)] Loss: 1.58855 \nTraining set [16640/60000 (28%)] Loss: 1.57408 \nTraining set [16960/60000 (28%)] Loss: 1.52646 \nTraining set [17280/60000 (29%)] Loss: 1.58198 \nTraining set [17600/60000 (29%)] Loss: 1.56011 \nTraining set [17920/60000 (30%)] Loss: 1.58057 \nTraining set [18240/60000 (30%)] Loss: 1.56405 \nTraining set [18560/60000 (31%)] Loss: 1.52901 \nTraining set [18880/60000 (31%)] Loss: 1.54071 \nTraining set [19200/60000 (32%)] Loss: 1.58944 \nTraining set [19520/60000 (33%)] Loss: 1.54875 \nTraining set [19840/60000 (33%)] Loss: 1.58457 \nTraining set [20160/60000 (34%)] Loss: 1.51047 \nTraining set [20480/60000 (34%)] Loss: 1.57363 \nTraining set [20800/60000 (35%)] Loss: 1.54799 \nTraining set [21120/60000 (35%)] Loss: 1.57566 \nTraining set [21440/60000 (36%)] Loss: 1.54404 \nTraining set [21760/60000 (36%)] Loss: 1.54349 \nTraining set [22080/60000 (37%)] Loss: 1.57880 \nTraining set [22400/60000 (37%)] Loss: 1.54347 \nTraining set [22720/60000 (38%)] Loss: 1.62821 \nTraining set [23040/60000 (38%)] Loss: 1.58382 \nTraining set [23360/60000 (39%)] Loss: 1.57460 \nTraining set [23680/60000 (39%)] Loss: 1.57426 \nTraining set [24000/60000 (40%)] Loss: 1.60544 \nTraining set [24320/60000 (41%)] Loss: 1.55994 \nTraining set [24640/60000 (41%)] Loss: 1.56367 \nTraining set [24960/60000 (42%)] Loss: 1.51435 \nTraining set [25280/60000 (42%)] Loss: 1.62660 \nTraining set [25600/60000 (43%)] Loss: 1.54049 \nTraining set [25920/60000 (43%)] Loss: 1.52582 \nTraining set [26240/60000 (44%)] Loss: 1.53688 \nTraining set [26560/60000 (44%)] Loss: 1.59727 \nTraining set [26880/60000 (45%)] Loss: 1.61480 \nTraining set [27200/60000 (45%)] Loss: 1.57739 \nTraining set [27520/60000 (46%)] Loss: 1.57213 \nTraining set [27840/60000 (46%)] Loss: 1.49929 \nTraining set [28160/60000 (47%)] Loss: 1.52199 \nTraining set [28480/60000 (47%)] Loss: 1.69606 \nTraining set [28800/60000 (48%)] Loss: 1.56286 \nTraining set [29120/60000 (49%)] Loss: 1.58356 \nTraining set [29440/60000 (49%)] Loss: 1.58028 \nTraining set [29760/60000 (50%)] Loss: 1.53975 \nTraining set [30080/60000 (50%)] Loss: 1.54569 \nTraining set [30400/60000 (51%)] Loss: 1.51067 \nTraining set [30720/60000 (51%)] Loss: 1.52943 \nTraining set [31040/60000 (52%)] Loss: 1.52077 \nTraining set [31360/60000 (52%)] Loss: 1.53984 \nTraining set [31680/60000 (53%)] Loss: 1.54695 \nTraining set [32000/60000 (53%)] Loss: 1.48393 \nTraining set [32320/60000 (54%)] Loss: 1.52909 \nTraining set [32640/60000 (54%)] Loss: 1.56846 \nTraining set [32960/60000 (55%)] Loss: 1.56370 \nTraining set [33280/60000 (55%)] Loss: 1.57932 \nTraining set [33600/60000 (56%)] Loss: 1.53124 \nTraining set [33920/60000 (57%)] Loss: 1.54285 \nTraining set [34240/60000 (57%)] Loss: 1.49433 \nTraining set [34560/60000 (58%)] Loss: 1.54429 \nTraining set [34880/60000 (58%)] Loss: 1.58111 \nTraining set [35200/60000 (59%)] Loss: 1.57983 \nTraining set [35520/60000 (59%)] Loss: 1.53779 \nTraining set [35840/60000 (60%)] Loss: 1.57401 \nTraining set [36160/60000 (60%)] Loss: 1.55575 \nTraining set [36480/60000 (61%)] Loss: 1.56948 \nTraining set [36800/60000 (61%)] Loss: 1.56971 \nTraining set [37120/60000 (62%)] Loss: 1.57262 \nTraining set [37440/60000 (62%)] Loss: 1.56940 \nTraining set [37760/60000 (63%)] Loss: 1.53269 \nTraining set [38080/60000 (63%)] Loss: 1.51443 \nTraining set [38400/60000 (64%)] Loss: 1.52832 \nTraining set [38720/60000 (64%)] Loss: 1.53351 \nTraining set [39040/60000 (65%)] Loss: 1.59721 \nTraining set [39360/60000 (66%)] Loss: 1.52092 \nTraining set [39680/60000 (66%)] Loss: 1.50871 \nTraining set [40000/60000 (67%)] Loss: 1.58882 \nTraining set [40320/60000 (67%)] Loss: 1.58403 \nTraining set [40640/60000 (68%)] Loss: 1.57630 \nTraining set [40960/60000 (68%)] Loss: 1.54856 \nTraining set [41280/60000 (69%)] Loss: 1.50604 \nTraining set [41600/60000 (69%)] Loss: 1.60993 \nTraining set [41920/60000 (70%)] Loss: 1.53964 \nTraining set [42240/60000 (70%)] Loss: 1.53400 \nTraining set [42560/60000 (71%)] Loss: 1.57568 \nTraining set [42880/60000 (71%)] Loss: 1.53110 \nTraining set [43200/60000 (72%)] Loss: 1.56855 \nTraining set [43520/60000 (72%)] Loss: 1.59267 \nTraining set [43840/60000 (73%)] Loss: 1.54789 \nTraining set [44160/60000 (74%)] Loss: 1.57995 \nTraining set [44480/60000 (74%)] Loss: 1.51990 \nTraining set [44800/60000 (75%)] Loss: 1.55938 \nTraining set [45120/60000 (75%)] Loss: 1.53085 \nTraining set [45440/60000 (76%)] Loss: 1.57095 \nTraining set [45760/60000 (76%)] Loss: 1.59256 \nTraining set [46080/60000 (77%)] Loss: 1.51163 \nTraining set [46400/60000 (77%)] Loss: 1.56315 \nTraining set [46720/60000 (78%)] Loss: 1.51801 \nTraining set [47040/60000 (78%)] Loss: 1.58835 \nTraining set [47360/60000 (79%)] Loss: 1.51396 \nTraining set [47680/60000 (79%)] Loss: 1.56697 \nTraining set [48000/60000 (80%)] Loss: 1.56177 \nTraining set [48320/60000 (80%)] Loss: 1.57133 \nTraining set [48640/60000 (81%)] Loss: 1.49129 \nTraining set [48960/60000 (82%)] Loss: 1.55607 \nTraining set [49280/60000 (82%)] Loss: 1.52308 \nTraining set [49600/60000 (83%)] Loss: 1.56433 \nTraining set [49920/60000 (83%)] Loss: 1.56942 \nTraining set [50240/60000 (84%)] Loss: 1.61817 \nTraining set [50560/60000 (84%)] Loss: 1.53306 \nTraining set [50880/60000 (85%)] Loss: 1.53626 \nTraining set [51200/60000 (85%)] Loss: 1.51641 \nTraining set [51520/60000 (86%)] Loss: 1.53153 \nTraining set [51840/60000 (86%)] Loss: 1.53554 \nTraining set [52160/60000 (87%)] Loss: 1.49924 \nTraining set [52480/60000 (87%)] Loss: 1.55057 \nTraining set [52800/60000 (88%)] Loss: 1.60804 \nTraining set [53120/60000 (88%)] Loss: 1.57196 \nTraining set [53440/60000 (89%)] Loss: 1.55410 \nTraining set [53760/60000 (90%)] Loss: 1.54906 \nTraining set [54080/60000 (90%)] Loss: 1.57351 \nTraining set [54400/60000 (91%)] Loss: 1.52293 \nTraining set [54720/60000 (91%)] Loss: 1.58311 \nTraining set [55040/60000 (92%)] Loss: 1.50106 \nTraining set [55360/60000 (92%)] Loss: 1.53798 \nTraining set [55680/60000 (93%)] Loss: 1.59351 \nTraining set [56000/60000 (93%)] Loss: 1.59589 \nTraining set [56320/60000 (94%)] Loss: 1.57516 \nTraining set [56640/60000 (94%)] Loss: 1.61969 \nTraining set [56960/60000 (95%)] Loss: 1.54802 \nTraining set [57280/60000 (95%)] Loss: 1.59737 \nTraining set [57600/60000 (96%)] Loss: 1.59356 \nTraining set [57920/60000 (96%)] Loss: 1.55859 \nTraining set [58240/60000 (97%)] Loss: 1.55119 \nTraining set [58560/60000 (98%)] Loss: 1.57866 \nTraining set [58880/60000 (98%)] Loss: 1.61915 \nTraining set [59200/60000 (99%)] Loss: 1.58872 \nTraining set [59520/60000 (99%)] Loss: 1.63570 \nTraining set [59840/60000 (100%)] Loss: 1.54570 \nAverage Training Loss: 1.55963\nTraining Completed!\nModel Validation Starts!\nAverage Validation Loss: 1.56826, Accuracy: 8954/10000 (90%)\n\nEpoch: 17\nTraining set [0/60000 (0%)] Loss: 1.56100 \nTraining set [320/60000 (1%)] Loss: 1.55690 \nTraining set [640/60000 (1%)] Loss: 1.51867 \nTraining set [960/60000 (2%)] Loss: 1.54787 \nTraining set [1280/60000 (2%)] Loss: 1.53836 \nTraining set [1600/60000 (3%)] Loss: 1.52598 \nTraining set [1920/60000 (3%)] Loss: 1.60847 \nTraining set [2240/60000 (4%)] Loss: 1.57277 \nTraining set [2560/60000 (4%)] Loss: 1.53998 \nTraining set [2880/60000 (5%)] Loss: 1.57299 \nTraining set [3200/60000 (5%)] Loss: 1.56245 \nTraining set [3520/60000 (6%)] Loss: 1.55349 \nTraining set [3840/60000 (6%)] Loss: 1.52054 \nTraining set [4160/60000 (7%)] Loss: 1.54833 \nTraining set [4480/60000 (7%)] Loss: 1.56639 \nTraining set [4800/60000 (8%)] Loss: 1.52966 \nTraining set [5120/60000 (9%)] Loss: 1.53438 \nTraining set [5440/60000 (9%)] Loss: 1.54211 \nTraining set [5760/60000 (10%)] Loss: 1.61880 \nTraining set [6080/60000 (10%)] Loss: 1.57792 \nTraining set [6400/60000 (11%)] Loss: 1.57086 \nTraining set [6720/60000 (11%)] Loss: 1.56021 \nTraining set [7040/60000 (12%)] Loss: 1.56602 \nTraining set [7360/60000 (12%)] Loss: 1.51633 \nTraining set [7680/60000 (13%)] Loss: 1.53620 \nTraining set [8000/60000 (13%)] Loss: 1.58807 \nTraining set [8320/60000 (14%)] Loss: 1.64491 \nTraining set [8640/60000 (14%)] Loss: 1.62467 \nTraining set [8960/60000 (15%)] Loss: 1.50662 \nTraining set [9280/60000 (15%)] Loss: 1.56706 \nTraining set [9600/60000 (16%)] Loss: 1.58903 \nTraining set [9920/60000 (17%)] Loss: 1.54183 \nTraining set [10240/60000 (17%)] Loss: 1.57036 \nTraining set [10560/60000 (18%)] Loss: 1.57114 \nTraining set [10880/60000 (18%)] Loss: 1.58877 \nTraining set [11200/60000 (19%)] Loss: 1.52485 \nTraining set [11520/60000 (19%)] Loss: 1.53638 \nTraining set [11840/60000 (20%)] Loss: 1.53997 \nTraining set [12160/60000 (20%)] Loss: 1.54779 \nTraining set [12480/60000 (21%)] Loss: 1.58420 \nTraining set [12800/60000 (21%)] Loss: 1.55759 \nTraining set [13120/60000 (22%)] Loss: 1.59800 \nTraining set [13440/60000 (22%)] Loss: 1.59030 \nTraining set [13760/60000 (23%)] Loss: 1.61386 \nTraining set [14080/60000 (23%)] Loss: 1.49385 \nTraining set [14400/60000 (24%)] Loss: 1.53442 \nTraining set [14720/60000 (25%)] Loss: 1.60221 \nTraining set [15040/60000 (25%)] Loss: 1.56728 \nTraining set [15360/60000 (26%)] Loss: 1.58776 \nTraining set [15680/60000 (26%)] Loss: 1.55585 \nTraining set [16000/60000 (27%)] Loss: 1.57204 \nTraining set [16320/60000 (27%)] Loss: 1.51525 \nTraining set [16640/60000 (28%)] Loss: 1.56914 \nTraining set [16960/60000 (28%)] Loss: 1.54938 \nTraining set [17280/60000 (29%)] Loss: 1.53938 \nTraining set [17600/60000 (29%)] Loss: 1.56600 \nTraining set [17920/60000 (30%)] Loss: 1.56411 \nTraining set [18240/60000 (30%)] Loss: 1.56971 \nTraining set [18560/60000 (31%)] Loss: 1.58388 \nTraining set [18880/60000 (31%)] Loss: 1.53023 \nTraining set [19200/60000 (32%)] Loss: 1.52391 \nTraining set [19520/60000 (33%)] Loss: 1.51473 \nTraining set [19840/60000 (33%)] Loss: 1.57434 \nTraining set [20160/60000 (34%)] Loss: 1.56716 \nTraining set [20480/60000 (34%)] Loss: 1.50166 \nTraining set [20800/60000 (35%)] Loss: 1.56437 \nTraining set [21120/60000 (35%)] Loss: 1.58814 \nTraining set [21440/60000 (36%)] Loss: 1.50170 \nTraining set [21760/60000 (36%)] Loss: 1.54237 \nTraining set [22080/60000 (37%)] Loss: 1.59791 \nTraining set [22400/60000 (37%)] Loss: 1.59028 \nTraining set [22720/60000 (38%)] Loss: 1.57923 \nTraining set [23040/60000 (38%)] Loss: 1.49796 \nTraining set [23360/60000 (39%)] Loss: 1.56353 \nTraining set [23680/60000 (39%)] Loss: 1.60734 \nTraining set [24000/60000 (40%)] Loss: 1.53841 \nTraining set [24320/60000 (41%)] Loss: 1.60075 \nTraining set [24640/60000 (41%)] Loss: 1.52077 \nTraining set [24960/60000 (42%)] Loss: 1.55705 \nTraining set [25280/60000 (42%)] Loss: 1.56460 \nTraining set [25600/60000 (43%)] Loss: 1.51608 \nTraining set [25920/60000 (43%)] Loss: 1.52395 \nTraining set [26240/60000 (44%)] Loss: 1.50751 \nTraining set [26560/60000 (44%)] Loss: 1.58829 \nTraining set [26880/60000 (45%)] Loss: 1.52729 \nTraining set [27200/60000 (45%)] Loss: 1.53276 \nTraining set [27520/60000 (46%)] Loss: 1.50553 \nTraining set [27840/60000 (46%)] Loss: 1.56605 \nTraining set [28160/60000 (47%)] Loss: 1.53784 \nTraining set [28480/60000 (47%)] Loss: 1.58005 \nTraining set [28800/60000 (48%)] Loss: 1.57463 \nTraining set [29120/60000 (49%)] Loss: 1.59602 \nTraining set [29440/60000 (49%)] Loss: 1.56125 \nTraining set [29760/60000 (50%)] Loss: 1.51098 \nTraining set [30080/60000 (50%)] Loss: 1.54333 \nTraining set [30400/60000 (51%)] Loss: 1.51938 \nTraining set [30720/60000 (51%)] Loss: 1.56899 \nTraining set [31040/60000 (52%)] Loss: 1.55699 \nTraining set [31360/60000 (52%)] Loss: 1.55152 \nTraining set [31680/60000 (53%)] Loss: 1.59902 \nTraining set [32000/60000 (53%)] Loss: 1.54313 \nTraining set [32320/60000 (54%)] Loss: 1.60061 \nTraining set [32640/60000 (54%)] Loss: 1.53672 \nTraining set [32960/60000 (55%)] Loss: 1.55036 \nTraining set [33280/60000 (55%)] Loss: 1.52027 \nTraining set [33600/60000 (56%)] Loss: 1.54485 \nTraining set [33920/60000 (57%)] Loss: 1.52074 \nTraining set [34240/60000 (57%)] Loss: 1.57121 \nTraining set [34560/60000 (58%)] Loss: 1.48632 \nTraining set [34880/60000 (58%)] Loss: 1.56756 \nTraining set [35200/60000 (59%)] Loss: 1.50238 \nTraining set [35520/60000 (59%)] Loss: 1.55187 \nTraining set [35840/60000 (60%)] Loss: 1.57669 \nTraining set [36160/60000 (60%)] Loss: 1.57913 \nTraining set [36480/60000 (61%)] Loss: 1.56397 \nTraining set [36800/60000 (61%)] Loss: 1.54188 \nTraining set [37120/60000 (62%)] Loss: 1.57015 \nTraining set [37440/60000 (62%)] Loss: 1.52631 \nTraining set [37760/60000 (63%)] Loss: 1.51779 \nTraining set [38080/60000 (63%)] Loss: 1.49969 \nTraining set [38400/60000 (64%)] Loss: 1.52308 \nTraining set [38720/60000 (64%)] Loss: 1.51832 \nTraining set [39040/60000 (65%)] Loss: 1.55848 \nTraining set [39360/60000 (66%)] Loss: 1.50961 \nTraining set [39680/60000 (66%)] Loss: 1.53678 \nTraining set [40000/60000 (67%)] Loss: 1.63798 \nTraining set [40320/60000 (67%)] Loss: 1.59422 \nTraining set [40640/60000 (68%)] Loss: 1.59104 \nTraining set [40960/60000 (68%)] Loss: 1.52513 \nTraining set [41280/60000 (69%)] Loss: 1.57376 \nTraining set [41600/60000 (69%)] Loss: 1.54726 \nTraining set [41920/60000 (70%)] Loss: 1.49049 \nTraining set [42240/60000 (70%)] Loss: 1.53943 \nTraining set [42560/60000 (71%)] Loss: 1.53140 \nTraining set [42880/60000 (71%)] Loss: 1.58917 \nTraining set [43200/60000 (72%)] Loss: 1.57925 \nTraining set [43520/60000 (72%)] Loss: 1.59675 \nTraining set [43840/60000 (73%)] Loss: 1.61414 \nTraining set [44160/60000 (74%)] Loss: 1.55101 \nTraining set [44480/60000 (74%)] Loss: 1.61092 \nTraining set [44800/60000 (75%)] Loss: 1.58274 \nTraining set [45120/60000 (75%)] Loss: 1.54823 \nTraining set [45440/60000 (76%)] Loss: 1.55910 \nTraining set [45760/60000 (76%)] Loss: 1.63642 \nTraining set [46080/60000 (77%)] Loss: 1.58473 \nTraining set [46400/60000 (77%)] Loss: 1.52600 \nTraining set [46720/60000 (78%)] Loss: 1.60483 \nTraining set [47040/60000 (78%)] Loss: 1.56019 \nTraining set [47360/60000 (79%)] Loss: 1.55064 \nTraining set [47680/60000 (79%)] Loss: 1.58845 \nTraining set [48000/60000 (80%)] Loss: 1.58304 \nTraining set [48320/60000 (80%)] Loss: 1.53464 \nTraining set [48640/60000 (81%)] Loss: 1.55296 \nTraining set [48960/60000 (82%)] Loss: 1.57631 \nTraining set [49280/60000 (82%)] Loss: 1.53915 \nTraining set [49600/60000 (83%)] Loss: 1.59581 \nTraining set [49920/60000 (83%)] Loss: 1.56702 \nTraining set [50240/60000 (84%)] Loss: 1.56405 \nTraining set [50560/60000 (84%)] Loss: 1.54945 \nTraining set [50880/60000 (85%)] Loss: 1.54188 \nTraining set [51200/60000 (85%)] Loss: 1.55838 \nTraining set [51520/60000 (86%)] Loss: 1.52557 \nTraining set [51840/60000 (86%)] Loss: 1.57453 \nTraining set [52160/60000 (87%)] Loss: 1.55852 \nTraining set [52480/60000 (87%)] Loss: 1.54171 \nTraining set [52800/60000 (88%)] Loss: 1.51513 \nTraining set [53120/60000 (88%)] Loss: 1.53644 \nTraining set [53440/60000 (89%)] Loss: 1.55184 \nTraining set [53760/60000 (90%)] Loss: 1.55388 \nTraining set [54080/60000 (90%)] Loss: 1.55307 \nTraining set [54400/60000 (91%)] Loss: 1.51294 \nTraining set [54720/60000 (91%)] Loss: 1.54298 \nTraining set [55040/60000 (92%)] Loss: 1.50909 \nTraining set [55360/60000 (92%)] Loss: 1.51207 \nTraining set [55680/60000 (93%)] Loss: 1.64289 \nTraining set [56000/60000 (93%)] Loss: 1.54992 \nTraining set [56320/60000 (94%)] Loss: 1.52426 \nTraining set [56640/60000 (94%)] Loss: 1.58795 \nTraining set [56960/60000 (95%)] Loss: 1.52828 \nTraining set [57280/60000 (95%)] Loss: 1.50500 \nTraining set [57600/60000 (96%)] Loss: 1.55617 \nTraining set [57920/60000 (96%)] Loss: 1.58490 \nTraining set [58240/60000 (97%)] Loss: 1.60242 \nTraining set [58560/60000 (98%)] Loss: 1.54671 \nTraining set [58880/60000 (98%)] Loss: 1.58738 \nTraining set [59200/60000 (99%)] Loss: 1.55082 \nTraining set [59520/60000 (99%)] Loss: 1.62204 \nTraining set [59840/60000 (100%)] Loss: 1.55414 \nAverage Training Loss: 1.55745\nTraining Completed!\nModel Validation Starts!\nAverage Validation Loss: 1.56674, Accuracy: 8949/10000 (89%)\n\nEpoch: 18\nTraining set [0/60000 (0%)] Loss: 1.55892 \nTraining set [320/60000 (1%)] Loss: 1.59723 \nTraining set [640/60000 (1%)] Loss: 1.63432 \nTraining set [960/60000 (2%)] Loss: 1.51064 \nTraining set [1280/60000 (2%)] Loss: 1.55871 \nTraining set [1600/60000 (3%)] Loss: 1.53913 \nTraining set [1920/60000 (3%)] Loss: 1.57604 \nTraining set [2240/60000 (4%)] Loss: 1.49472 \nTraining set [2560/60000 (4%)] Loss: 1.52271 \nTraining set [2880/60000 (5%)] Loss: 1.54671 \nTraining set [3200/60000 (5%)] Loss: 1.51568 \nTraining set [3520/60000 (6%)] Loss: 1.62642 \nTraining set [3840/60000 (6%)] Loss: 1.54415 \nTraining set [4160/60000 (7%)] Loss: 1.52165 \nTraining set [4480/60000 (7%)] Loss: 1.49551 \nTraining set [4800/60000 (8%)] Loss: 1.56331 \nTraining set [5120/60000 (9%)] Loss: 1.55765 \nTraining set [5440/60000 (9%)] Loss: 1.52094 \nTraining set [5760/60000 (10%)] Loss: 1.52406 \nTraining set [6080/60000 (10%)] Loss: 1.57820 \nTraining set [6400/60000 (11%)] Loss: 1.53916 \nTraining set [6720/60000 (11%)] Loss: 1.55466 \nTraining set [7040/60000 (12%)] Loss: 1.56441 \nTraining set [7360/60000 (12%)] Loss: 1.62474 \nTraining set [7680/60000 (13%)] Loss: 1.62033 \nTraining set [8000/60000 (13%)] Loss: 1.56152 \nTraining set [8320/60000 (14%)] Loss: 1.55191 \nTraining set [8640/60000 (14%)] Loss: 1.56625 \nTraining set [8960/60000 (15%)] Loss: 1.53681 \nTraining set [9280/60000 (15%)] Loss: 1.55328 \nTraining set [9600/60000 (16%)] Loss: 1.52004 \nTraining set [9920/60000 (17%)] Loss: 1.47945 \nTraining set [10240/60000 (17%)] Loss: 1.56042 \nTraining set [10560/60000 (18%)] Loss: 1.57717 \nTraining set [10880/60000 (18%)] Loss: 1.58709 \nTraining set [11200/60000 (19%)] Loss: 1.54848 \nTraining set [11520/60000 (19%)] Loss: 1.55182 \nTraining set [11840/60000 (20%)] Loss: 1.53702 \nTraining set [12160/60000 (20%)] Loss: 1.54092 \nTraining set [12480/60000 (21%)] Loss: 1.53224 \nTraining set [12800/60000 (21%)] Loss: 1.53515 \nTraining set [13120/60000 (22%)] Loss: 1.55499 \nTraining set [13440/60000 (22%)] Loss: 1.57197 \nTraining set [13760/60000 (23%)] Loss: 1.58529 \nTraining set [14080/60000 (23%)] Loss: 1.61183 \nTraining set [14400/60000 (24%)] Loss: 1.54082 \nTraining set [14720/60000 (25%)] Loss: 1.57304 \nTraining set [15040/60000 (25%)] Loss: 1.56904 \nTraining set [15360/60000 (26%)] Loss: 1.55884 \nTraining set [15680/60000 (26%)] Loss: 1.62958 \nTraining set [16000/60000 (27%)] Loss: 1.61312 \nTraining set [16320/60000 (27%)] Loss: 1.52886 \nTraining set [16640/60000 (28%)] Loss: 1.58246 \nTraining set [16960/60000 (28%)] Loss: 1.51917 \nTraining set [17280/60000 (29%)] Loss: 1.54509 \nTraining set [17600/60000 (29%)] Loss: 1.53635 \nTraining set [17920/60000 (30%)] Loss: 1.53873 \nTraining set [18240/60000 (30%)] Loss: 1.60141 \nTraining set [18560/60000 (31%)] Loss: 1.59469 \nTraining set [18880/60000 (31%)] Loss: 1.59217 \nTraining set [19200/60000 (32%)] Loss: 1.55817 \nTraining set [19520/60000 (33%)] Loss: 1.56943 \nTraining set [19840/60000 (33%)] Loss: 1.62269 \nTraining set [20160/60000 (34%)] Loss: 1.53890 \nTraining set [20480/60000 (34%)] Loss: 1.55216 \nTraining set [20800/60000 (35%)] Loss: 1.60334 \nTraining set [21120/60000 (35%)] Loss: 1.53721 \nTraining set [21440/60000 (36%)] Loss: 1.53832 \nTraining set [21760/60000 (36%)] Loss: 1.57286 \nTraining set [22080/60000 (37%)] Loss: 1.53105 \nTraining set [22400/60000 (37%)] Loss: 1.60294 \nTraining set [22720/60000 (38%)] Loss: 1.56052 \nTraining set [23040/60000 (38%)] Loss: 1.55499 \nTraining set [23360/60000 (39%)] Loss: 1.55076 \nTraining set [23680/60000 (39%)] Loss: 1.53867 \nTraining set [24000/60000 (40%)] Loss: 1.60725 \nTraining set [24320/60000 (41%)] Loss: 1.56517 \nTraining set [24640/60000 (41%)] Loss: 1.60516 \nTraining set [24960/60000 (42%)] Loss: 1.55682 \nTraining set [25280/60000 (42%)] Loss: 1.55001 \nTraining set [25600/60000 (43%)] Loss: 1.55964 \nTraining set [25920/60000 (43%)] Loss: 1.56788 \nTraining set [26240/60000 (44%)] Loss: 1.58409 \nTraining set [26560/60000 (44%)] Loss: 1.56355 \nTraining set [26880/60000 (45%)] Loss: 1.54471 \nTraining set [27200/60000 (45%)] Loss: 1.52432 \nTraining set [27520/60000 (46%)] Loss: 1.58216 \nTraining set [27840/60000 (46%)] Loss: 1.54739 \nTraining set [28160/60000 (47%)] Loss: 1.53096 \nTraining set [28480/60000 (47%)] Loss: 1.63169 \nTraining set [28800/60000 (48%)] Loss: 1.53046 \nTraining set [29120/60000 (49%)] Loss: 1.55971 \nTraining set [29440/60000 (49%)] Loss: 1.54210 \nTraining set [29760/60000 (50%)] Loss: 1.54115 \nTraining set [30080/60000 (50%)] Loss: 1.55122 \nTraining set [30400/60000 (51%)] Loss: 1.53938 \nTraining set [30720/60000 (51%)] Loss: 1.58855 \nTraining set [31040/60000 (52%)] Loss: 1.53144 \nTraining set [31360/60000 (52%)] Loss: 1.51538 \nTraining set [31680/60000 (53%)] Loss: 1.54396 \nTraining set [32000/60000 (53%)] Loss: 1.51924 \nTraining set [32320/60000 (54%)] Loss: 1.53510 \nTraining set [32640/60000 (54%)] Loss: 1.57648 \nTraining set [32960/60000 (55%)] Loss: 1.61628 \nTraining set [33280/60000 (55%)] Loss: 1.57155 \nTraining set [33600/60000 (56%)] Loss: 1.55757 \nTraining set [33920/60000 (57%)] Loss: 1.53819 \nTraining set [34240/60000 (57%)] Loss: 1.54329 \nTraining set [34560/60000 (58%)] Loss: 1.55051 \nTraining set [34880/60000 (58%)] Loss: 1.51458 \nTraining set [35200/60000 (59%)] Loss: 1.62754 \nTraining set [35520/60000 (59%)] Loss: 1.55163 \nTraining set [35840/60000 (60%)] Loss: 1.54153 \nTraining set [36160/60000 (60%)] Loss: 1.57324 \nTraining set [36480/60000 (61%)] Loss: 1.54976 \nTraining set [36800/60000 (61%)] Loss: 1.51971 \nTraining set [37120/60000 (62%)] Loss: 1.56619 \nTraining set [37440/60000 (62%)] Loss: 1.56878 \nTraining set [37760/60000 (63%)] Loss: 1.53982 \nTraining set [38080/60000 (63%)] Loss: 1.54686 \nTraining set [38400/60000 (64%)] Loss: 1.61114 \nTraining set [38720/60000 (64%)] Loss: 1.55259 \nTraining set [39040/60000 (65%)] Loss: 1.60251 \nTraining set [39360/60000 (66%)] Loss: 1.55390 \nTraining set [39680/60000 (66%)] Loss: 1.64564 \nTraining set [40000/60000 (67%)] Loss: 1.57271 \nTraining set [40320/60000 (67%)] Loss: 1.51240 \nTraining set [40640/60000 (68%)] Loss: 1.52792 \nTraining set [40960/60000 (68%)] Loss: 1.56525 \nTraining set [41280/60000 (69%)] Loss: 1.60555 \nTraining set [41600/60000 (69%)] Loss: 1.50956 \nTraining set [41920/60000 (70%)] Loss: 1.60377 \nTraining set [42240/60000 (70%)] Loss: 1.56160 \nTraining set [42560/60000 (71%)] Loss: 1.51004 \nTraining set [42880/60000 (71%)] Loss: 1.53234 \nTraining set [43200/60000 (72%)] Loss: 1.53599 \nTraining set [43520/60000 (72%)] Loss: 1.57135 \nTraining set [43840/60000 (73%)] Loss: 1.49348 \nTraining set [44160/60000 (74%)] Loss: 1.54774 \nTraining set [44480/60000 (74%)] Loss: 1.58096 \nTraining set [44800/60000 (75%)] Loss: 1.60379 \nTraining set [45120/60000 (75%)] Loss: 1.47767 \nTraining set [45440/60000 (76%)] Loss: 1.55637 \nTraining set [45760/60000 (76%)] Loss: 1.56691 \nTraining set [46080/60000 (77%)] Loss: 1.54514 \nTraining set [46400/60000 (77%)] Loss: 1.61320 \nTraining set [46720/60000 (78%)] Loss: 1.53501 \nTraining set [47040/60000 (78%)] Loss: 1.53679 \nTraining set [47360/60000 (79%)] Loss: 1.53214 \nTraining set [47680/60000 (79%)] Loss: 1.58514 \nTraining set [48000/60000 (80%)] Loss: 1.52986 \nTraining set [48320/60000 (80%)] Loss: 1.55851 \nTraining set [48640/60000 (81%)] Loss: 1.49359 \nTraining set [48960/60000 (82%)] Loss: 1.59521 \nTraining set [49280/60000 (82%)] Loss: 1.52158 \nTraining set [49600/60000 (83%)] Loss: 1.55166 \nTraining set [49920/60000 (83%)] Loss: 1.57454 \nTraining set [50240/60000 (84%)] Loss: 1.53656 \nTraining set [50560/60000 (84%)] Loss: 1.55473 \nTraining set [50880/60000 (85%)] Loss: 1.56463 \nTraining set [51200/60000 (85%)] Loss: 1.54799 \nTraining set [51520/60000 (86%)] Loss: 1.58074 \nTraining set [51840/60000 (86%)] Loss: 1.53820 \nTraining set [52160/60000 (87%)] Loss: 1.54675 \nTraining set [52480/60000 (87%)] Loss: 1.55708 \nTraining set [52800/60000 (88%)] Loss: 1.54843 \nTraining set [53120/60000 (88%)] Loss: 1.50106 \nTraining set [53440/60000 (89%)] Loss: 1.52246 \nTraining set [53760/60000 (90%)] Loss: 1.54816 \nTraining set [54080/60000 (90%)] Loss: 1.60279 \nTraining set [54400/60000 (91%)] Loss: 1.54656 \nTraining set [54720/60000 (91%)] Loss: 1.53959 \nTraining set [55040/60000 (92%)] Loss: 1.58140 \nTraining set [55360/60000 (92%)] Loss: 1.57220 \nTraining set [55680/60000 (93%)] Loss: 1.58031 \nTraining set [56000/60000 (93%)] Loss: 1.57646 \nTraining set [56320/60000 (94%)] Loss: 1.58636 \nTraining set [56640/60000 (94%)] Loss: 1.49903 \nTraining set [56960/60000 (95%)] Loss: 1.55701 \nTraining set [57280/60000 (95%)] Loss: 1.53497 \nTraining set [57600/60000 (96%)] Loss: 1.53535 \nTraining set [57920/60000 (96%)] Loss: 1.56661 \nTraining set [58240/60000 (97%)] Loss: 1.55562 \nTraining set [58560/60000 (98%)] Loss: 1.52385 \nTraining set [58880/60000 (98%)] Loss: 1.55474 \nTraining set [59200/60000 (99%)] Loss: 1.55111 \nTraining set [59520/60000 (99%)] Loss: 1.57655 \nTraining set [59840/60000 (100%)] Loss: 1.55457 \nAverage Training Loss: 1.55531\nTraining Completed!\nModel Validation Starts!\nAverage Validation Loss: 1.56529, Accuracy: 8978/10000 (90%)\n\nEpoch: 19\nTraining set [0/60000 (0%)] Loss: 1.63342 \nTraining set [320/60000 (1%)] Loss: 1.49383 \nTraining set [640/60000 (1%)] Loss: 1.56405 \nTraining set [960/60000 (2%)] Loss: 1.54847 \nTraining set [1280/60000 (2%)] Loss: 1.54016 \nTraining set [1600/60000 (3%)] Loss: 1.56523 \nTraining set [1920/60000 (3%)] Loss: 1.57391 \nTraining set [2240/60000 (4%)] Loss: 1.51896 \nTraining set [2560/60000 (4%)] Loss: 1.53860 \nTraining set [2880/60000 (5%)] Loss: 1.57758 \nTraining set [3200/60000 (5%)] Loss: 1.55748 \nTraining set [3520/60000 (6%)] Loss: 1.57771 \nTraining set [3840/60000 (6%)] Loss: 1.52398 \nTraining set [4160/60000 (7%)] Loss: 1.55599 \nTraining set [4480/60000 (7%)] Loss: 1.52643 \nTraining set [4800/60000 (8%)] Loss: 1.56490 \nTraining set [5120/60000 (9%)] Loss: 1.54871 \nTraining set [5440/60000 (9%)] Loss: 1.49602 \nTraining set [5760/60000 (10%)] Loss: 1.55379 \nTraining set [6080/60000 (10%)] Loss: 1.58405 \nTraining set [6400/60000 (11%)] Loss: 1.54118 \nTraining set [6720/60000 (11%)] Loss: 1.53035 \nTraining set [7040/60000 (12%)] Loss: 1.59712 \nTraining set [7360/60000 (12%)] Loss: 1.61492 \nTraining set [7680/60000 (13%)] Loss: 1.55503 \nTraining set [8000/60000 (13%)] Loss: 1.56458 \nTraining set [8320/60000 (14%)] Loss: 1.52294 \nTraining set [8640/60000 (14%)] Loss: 1.54228 \nTraining set [8960/60000 (15%)] Loss: 1.51630 \nTraining set [9280/60000 (15%)] Loss: 1.50330 \nTraining set [9600/60000 (16%)] Loss: 1.58320 \nTraining set [9920/60000 (17%)] Loss: 1.57560 \nTraining set [10240/60000 (17%)] Loss: 1.57824 \nTraining set [10560/60000 (18%)] Loss: 1.59970 \nTraining set [10880/60000 (18%)] Loss: 1.56175 \nTraining set [11200/60000 (19%)] Loss: 1.57225 \nTraining set [11520/60000 (19%)] Loss: 1.52100 \nTraining set [11840/60000 (20%)] Loss: 1.52369 \nTraining set [12160/60000 (20%)] Loss: 1.50498 \nTraining set [12480/60000 (21%)] Loss: 1.53063 \nTraining set [12800/60000 (21%)] Loss: 1.52968 \nTraining set [13120/60000 (22%)] Loss: 1.61300 \nTraining set [13440/60000 (22%)] Loss: 1.58317 \nTraining set [13760/60000 (23%)] Loss: 1.52625 \nTraining set [14080/60000 (23%)] Loss: 1.50792 \nTraining set [14400/60000 (24%)] Loss: 1.49465 \nTraining set [14720/60000 (25%)] Loss: 1.56741 \nTraining set [15040/60000 (25%)] Loss: 1.57948 \nTraining set [15360/60000 (26%)] Loss: 1.52406 \nTraining set [15680/60000 (26%)] Loss: 1.52619 \nTraining set [16000/60000 (27%)] Loss: 1.62861 \nTraining set [16320/60000 (27%)] Loss: 1.55683 \nTraining set [16640/60000 (28%)] Loss: 1.56911 \nTraining set [16960/60000 (28%)] Loss: 1.55473 \nTraining set [17280/60000 (29%)] Loss: 1.53899 \nTraining set [17600/60000 (29%)] Loss: 1.52711 \nTraining set [17920/60000 (30%)] Loss: 1.59766 \nTraining set [18240/60000 (30%)] Loss: 1.54819 \nTraining set [18560/60000 (31%)] Loss: 1.58403 \nTraining set [18880/60000 (31%)] Loss: 1.56439 \nTraining set [19200/60000 (32%)] Loss: 1.54476 \nTraining set [19520/60000 (33%)] Loss: 1.54153 \nTraining set [19840/60000 (33%)] Loss: 1.52608 \nTraining set [20160/60000 (34%)] Loss: 1.56841 \nTraining set [20480/60000 (34%)] Loss: 1.51785 \nTraining set [20800/60000 (35%)] Loss: 1.54333 \nTraining set [21120/60000 (35%)] Loss: 1.49448 \nTraining set [21440/60000 (36%)] Loss: 1.57665 \nTraining set [21760/60000 (36%)] Loss: 1.51531 \nTraining set [22080/60000 (37%)] Loss: 1.53585 \nTraining set [22400/60000 (37%)] Loss: 1.56373 \nTraining set [22720/60000 (38%)] Loss: 1.55235 \nTraining set [23040/60000 (38%)] Loss: 1.52502 \nTraining set [23360/60000 (39%)] Loss: 1.62371 \nTraining set [23680/60000 (39%)] Loss: 1.52139 \nTraining set [24000/60000 (40%)] Loss: 1.53356 \nTraining set [24320/60000 (41%)] Loss: 1.58911 \nTraining set [24640/60000 (41%)] Loss: 1.55185 \nTraining set [24960/60000 (42%)] Loss: 1.52640 \nTraining set [25280/60000 (42%)] Loss: 1.57607 \nTraining set [25600/60000 (43%)] Loss: 1.53213 \nTraining set [25920/60000 (43%)] Loss: 1.51476 \nTraining set [26240/60000 (44%)] Loss: 1.57133 \nTraining set [26560/60000 (44%)] Loss: 1.53246 \nTraining set [26880/60000 (45%)] Loss: 1.61293 \nTraining set [27200/60000 (45%)] Loss: 1.55815 \nTraining set [27520/60000 (46%)] Loss: 1.53002 \nTraining set [27840/60000 (46%)] Loss: 1.54407 \nTraining set [28160/60000 (47%)] Loss: 1.58092 \nTraining set [28480/60000 (47%)] Loss: 1.56492 \nTraining set [28800/60000 (48%)] Loss: 1.52931 \nTraining set [29120/60000 (49%)] Loss: 1.58000 \nTraining set [29440/60000 (49%)] Loss: 1.61305 \nTraining set [29760/60000 (50%)] Loss: 1.52565 \nTraining set [30080/60000 (50%)] Loss: 1.59008 \nTraining set [30400/60000 (51%)] Loss: 1.62377 \nTraining set [30720/60000 (51%)] Loss: 1.56419 \nTraining set [31040/60000 (52%)] Loss: 1.57303 \nTraining set [31360/60000 (52%)] Loss: 1.53658 \nTraining set [31680/60000 (53%)] Loss: 1.53770 \nTraining set [32000/60000 (53%)] Loss: 1.56936 \nTraining set [32320/60000 (54%)] Loss: 1.63098 \nTraining set [32640/60000 (54%)] Loss: 1.58963 \nTraining set [32960/60000 (55%)] Loss: 1.59638 \nTraining set [33280/60000 (55%)] Loss: 1.57825 \nTraining set [33600/60000 (56%)] Loss: 1.59805 \nTraining set [33920/60000 (57%)] Loss: 1.57169 \nTraining set [34240/60000 (57%)] Loss: 1.50239 \nTraining set [34560/60000 (58%)] Loss: 1.55952 \nTraining set [34880/60000 (58%)] Loss: 1.60169 \nTraining set [35200/60000 (59%)] Loss: 1.49205 \nTraining set [35520/60000 (59%)] Loss: 1.55687 \nTraining set [35840/60000 (60%)] Loss: 1.58963 \nTraining set [36160/60000 (60%)] Loss: 1.59297 \nTraining set [36480/60000 (61%)] Loss: 1.56209 \nTraining set [36800/60000 (61%)] Loss: 1.57537 \nTraining set [37120/60000 (62%)] Loss: 1.52142 \nTraining set [37440/60000 (62%)] Loss: 1.51018 \nTraining set [37760/60000 (63%)] Loss: 1.52430 \nTraining set [38080/60000 (63%)] Loss: 1.57732 \nTraining set [38400/60000 (64%)] Loss: 1.49933 \nTraining set [38720/60000 (64%)] Loss: 1.51016 \nTraining set [39040/60000 (65%)] Loss: 1.52575 \nTraining set [39360/60000 (66%)] Loss: 1.57320 \nTraining set [39680/60000 (66%)] Loss: 1.56080 \nTraining set [40000/60000 (67%)] Loss: 1.52417 \nTraining set [40320/60000 (67%)] Loss: 1.57353 \nTraining set [40640/60000 (68%)] Loss: 1.58041 \nTraining set [40960/60000 (68%)] Loss: 1.51785 \nTraining set [41280/60000 (69%)] Loss: 1.49975 \nTraining set [41600/60000 (69%)] Loss: 1.52360 \nTraining set [41920/60000 (70%)] Loss: 1.55876 \nTraining set [42240/60000 (70%)] Loss: 1.57868 \nTraining set [42560/60000 (71%)] Loss: 1.51421 \nTraining set [42880/60000 (71%)] Loss: 1.56961 \nTraining set [43200/60000 (72%)] Loss: 1.52732 \nTraining set [43520/60000 (72%)] Loss: 1.52097 \nTraining set [43840/60000 (73%)] Loss: 1.53712 \nTraining set [44160/60000 (74%)] Loss: 1.55961 \nTraining set [44480/60000 (74%)] Loss: 1.53394 \nTraining set [44800/60000 (75%)] Loss: 1.53887 \nTraining set [45120/60000 (75%)] Loss: 1.60513 \nTraining set [45440/60000 (76%)] Loss: 1.57276 \nTraining set [45760/60000 (76%)] Loss: 1.53020 \nTraining set [46080/60000 (77%)] Loss: 1.57652 \nTraining set [46400/60000 (77%)] Loss: 1.56463 \nTraining set [46720/60000 (78%)] Loss: 1.62905 \nTraining set [47040/60000 (78%)] Loss: 1.58267 \nTraining set [47360/60000 (79%)] Loss: 1.55698 \nTraining set [47680/60000 (79%)] Loss: 1.50353 \nTraining set [48000/60000 (80%)] Loss: 1.60270 \nTraining set [48320/60000 (80%)] Loss: 1.51844 \nTraining set [48640/60000 (81%)] Loss: 1.55503 \nTraining set [48960/60000 (82%)] Loss: 1.60426 \nTraining set [49280/60000 (82%)] Loss: 1.55781 \nTraining set [49600/60000 (83%)] Loss: 1.54718 \nTraining set [49920/60000 (83%)] Loss: 1.54826 \nTraining set [50240/60000 (84%)] Loss: 1.57829 \nTraining set [50560/60000 (84%)] Loss: 1.54487 \nTraining set [50880/60000 (85%)] Loss: 1.55458 \nTraining set [51200/60000 (85%)] Loss: 1.50391 \nTraining set [51520/60000 (86%)] Loss: 1.54447 \nTraining set [51840/60000 (86%)] Loss: 1.59386 \nTraining set [52160/60000 (87%)] Loss: 1.55884 \nTraining set [52480/60000 (87%)] Loss: 1.53966 \nTraining set [52800/60000 (88%)] Loss: 1.53362 \nTraining set [53120/60000 (88%)] Loss: 1.55286 \nTraining set [53440/60000 (89%)] Loss: 1.60717 \nTraining set [53760/60000 (90%)] Loss: 1.59284 \nTraining set [54080/60000 (90%)] Loss: 1.62069 \nTraining set [54400/60000 (91%)] Loss: 1.58818 \nTraining set [54720/60000 (91%)] Loss: 1.52246 \nTraining set [55040/60000 (92%)] Loss: 1.58895 \nTraining set [55360/60000 (92%)] Loss: 1.58805 \nTraining set [55680/60000 (93%)] Loss: 1.50061 \nTraining set [56000/60000 (93%)] Loss: 1.53642 \nTraining set [56320/60000 (94%)] Loss: 1.53905 \nTraining set [56640/60000 (94%)] Loss: 1.51282 \nTraining set [56960/60000 (95%)] Loss: 1.57248 \nTraining set [57280/60000 (95%)] Loss: 1.56709 \nTraining set [57600/60000 (96%)] Loss: 1.53517 \nTraining set [57920/60000 (96%)] Loss: 1.49148 \nTraining set [58240/60000 (97%)] Loss: 1.51392 \nTraining set [58560/60000 (98%)] Loss: 1.57750 \nTraining set [58880/60000 (98%)] Loss: 1.56887 \nTraining set [59200/60000 (99%)] Loss: 1.49890 \nTraining set [59520/60000 (99%)] Loss: 1.53945 \nTraining set [59840/60000 (100%)] Loss: 1.54491 \nAverage Training Loss: 1.55230\nTraining Completed!\nModel Validation Starts!\nAverage Validation Loss: 1.56258, Accuracy: 9012/10000 (90%)\n\nEpoch: 20\nTraining set [0/60000 (0%)] Loss: 1.56944 \nTraining set [320/60000 (1%)] Loss: 1.54139 \nTraining set [640/60000 (1%)] Loss: 1.49835 \nTraining set [960/60000 (2%)] Loss: 1.57015 \nTraining set [1280/60000 (2%)] Loss: 1.60505 \nTraining set [1600/60000 (3%)] Loss: 1.56594 \nTraining set [1920/60000 (3%)] Loss: 1.50738 \nTraining set [2240/60000 (4%)] Loss: 1.59893 \nTraining set [2560/60000 (4%)] Loss: 1.55077 \nTraining set [2880/60000 (5%)] Loss: 1.56913 \nTraining set [3200/60000 (5%)] Loss: 1.58649 \nTraining set [3520/60000 (6%)] Loss: 1.52616 \nTraining set [3840/60000 (6%)] Loss: 1.56154 \nTraining set [4160/60000 (7%)] Loss: 1.51010 \nTraining set [4480/60000 (7%)] Loss: 1.50376 \nTraining set [4800/60000 (8%)] Loss: 1.53097 \nTraining set [5120/60000 (9%)] Loss: 1.53442 \nTraining set [5440/60000 (9%)] Loss: 1.54906 \nTraining set [5760/60000 (10%)] Loss: 1.54990 \nTraining set [6080/60000 (10%)] Loss: 1.61556 \nTraining set [6400/60000 (11%)] Loss: 1.54075 \nTraining set [6720/60000 (11%)] Loss: 1.58695 \nTraining set [7040/60000 (12%)] Loss: 1.57108 \nTraining set [7360/60000 (12%)] Loss: 1.54737 \nTraining set [7680/60000 (13%)] Loss: 1.64101 \nTraining set [8000/60000 (13%)] Loss: 1.51653 \nTraining set [8320/60000 (14%)] Loss: 1.55484 \nTraining set [8640/60000 (14%)] Loss: 1.56684 \nTraining set [8960/60000 (15%)] Loss: 1.55135 \nTraining set [9280/60000 (15%)] Loss: 1.52697 \nTraining set [9600/60000 (16%)] Loss: 1.55209 \nTraining set [9920/60000 (17%)] Loss: 1.52399 \nTraining set [10240/60000 (17%)] Loss: 1.52273 \nTraining set [10560/60000 (18%)] Loss: 1.53366 \nTraining set [10880/60000 (18%)] Loss: 1.53379 \nTraining set [11200/60000 (19%)] Loss: 1.54530 \nTraining set [11520/60000 (19%)] Loss: 1.53952 \nTraining set [11840/60000 (20%)] Loss: 1.56667 \nTraining set [12160/60000 (20%)] Loss: 1.54210 \nTraining set [12480/60000 (21%)] Loss: 1.52594 \nTraining set [12800/60000 (21%)] Loss: 1.54750 \nTraining set [13120/60000 (22%)] Loss: 1.57855 \nTraining set [13440/60000 (22%)] Loss: 1.52820 \nTraining set [13760/60000 (23%)] Loss: 1.54006 \nTraining set [14080/60000 (23%)] Loss: 1.54109 \nTraining set [14400/60000 (24%)] Loss: 1.55150 \nTraining set [14720/60000 (25%)] Loss: 1.55934 \nTraining set [15040/60000 (25%)] Loss: 1.52269 \nTraining set [15360/60000 (26%)] Loss: 1.53568 \nTraining set [15680/60000 (26%)] Loss: 1.52398 \nTraining set [16000/60000 (27%)] Loss: 1.51203 \nTraining set [16320/60000 (27%)] Loss: 1.51962 \nTraining set [16640/60000 (28%)] Loss: 1.52807 \nTraining set [16960/60000 (28%)] Loss: 1.51029 \nTraining set [17280/60000 (29%)] Loss: 1.58826 \nTraining set [17600/60000 (29%)] Loss: 1.51297 \nTraining set [17920/60000 (30%)] Loss: 1.53446 \nTraining set [18240/60000 (30%)] Loss: 1.57867 \nTraining set [18560/60000 (31%)] Loss: 1.55038 \nTraining set [18880/60000 (31%)] Loss: 1.59963 \nTraining set [19200/60000 (32%)] Loss: 1.56869 \nTraining set [19520/60000 (33%)] Loss: 1.52309 \nTraining set [19840/60000 (33%)] Loss: 1.53304 \nTraining set [20160/60000 (34%)] Loss: 1.52505 \nTraining set [20480/60000 (34%)] Loss: 1.56999 \nTraining set [20800/60000 (35%)] Loss: 1.51484 \nTraining set [21120/60000 (35%)] Loss: 1.56798 \nTraining set [21440/60000 (36%)] Loss: 1.50708 \nTraining set [21760/60000 (36%)] Loss: 1.53324 \nTraining set [22080/60000 (37%)] Loss: 1.56033 \nTraining set [22400/60000 (37%)] Loss: 1.51113 \nTraining set [22720/60000 (38%)] Loss: 1.51150 \nTraining set [23040/60000 (38%)] Loss: 1.52176 \nTraining set [23360/60000 (39%)] Loss: 1.55325 \nTraining set [23680/60000 (39%)] Loss: 1.55901 \nTraining set [24000/60000 (40%)] Loss: 1.50891 \nTraining set [24320/60000 (41%)] Loss: 1.52506 \nTraining set [24640/60000 (41%)] Loss: 1.50400 \nTraining set [24960/60000 (42%)] Loss: 1.49532 \nTraining set [25280/60000 (42%)] Loss: 1.52481 \nTraining set [25600/60000 (43%)] Loss: 1.53577 \nTraining set [25920/60000 (43%)] Loss: 1.57504 \nTraining set [26240/60000 (44%)] Loss: 1.54535 \nTraining set [26560/60000 (44%)] Loss: 1.55175 \nTraining set [26880/60000 (45%)] Loss: 1.56272 \nTraining set [27200/60000 (45%)] Loss: 1.54293 \nTraining set [27520/60000 (46%)] Loss: 1.64436 \nTraining set [27840/60000 (46%)] Loss: 1.52094 \nTraining set [28160/60000 (47%)] Loss: 1.55178 \nTraining set [28480/60000 (47%)] Loss: 1.61898 \nTraining set [28800/60000 (48%)] Loss: 1.59778 \nTraining set [29120/60000 (49%)] Loss: 1.57151 \nTraining set [29440/60000 (49%)] Loss: 1.51340 \nTraining set [29760/60000 (50%)] Loss: 1.56303 \nTraining set [30080/60000 (50%)] Loss: 1.52629 \nTraining set [30400/60000 (51%)] Loss: 1.50903 \nTraining set [30720/60000 (51%)] Loss: 1.53202 \nTraining set [31040/60000 (52%)] Loss: 1.54145 \nTraining set [31360/60000 (52%)] Loss: 1.56539 \nTraining set [31680/60000 (53%)] Loss: 1.60573 \nTraining set [32000/60000 (53%)] Loss: 1.58489 \nTraining set [32320/60000 (54%)] Loss: 1.55498 \nTraining set [32640/60000 (54%)] Loss: 1.50907 \nTraining set [32960/60000 (55%)] Loss: 1.53599 \nTraining set [33280/60000 (55%)] Loss: 1.49327 \nTraining set [33600/60000 (56%)] Loss: 1.52560 \nTraining set [33920/60000 (57%)] Loss: 1.51686 \nTraining set [34240/60000 (57%)] Loss: 1.55769 \nTraining set [34560/60000 (58%)] Loss: 1.57269 \nTraining set [34880/60000 (58%)] Loss: 1.55458 \nTraining set [35200/60000 (59%)] Loss: 1.58987 \nTraining set [35520/60000 (59%)] Loss: 1.58302 \nTraining set [35840/60000 (60%)] Loss: 1.47903 \nTraining set [36160/60000 (60%)] Loss: 1.53821 \nTraining set [36480/60000 (61%)] Loss: 1.52141 \nTraining set [36800/60000 (61%)] Loss: 1.56868 \nTraining set [37120/60000 (62%)] Loss: 1.57518 \nTraining set [37440/60000 (62%)] Loss: 1.58841 \nTraining set [37760/60000 (63%)] Loss: 1.52760 \nTraining set [38080/60000 (63%)] Loss: 1.53677 \nTraining set [38400/60000 (64%)] Loss: 1.58497 \nTraining set [38720/60000 (64%)] Loss: 1.55070 \nTraining set [39040/60000 (65%)] Loss: 1.58766 \nTraining set [39360/60000 (66%)] Loss: 1.54295 \nTraining set [39680/60000 (66%)] Loss: 1.51593 \nTraining set [40000/60000 (67%)] Loss: 1.54305 \nTraining set [40320/60000 (67%)] Loss: 1.53388 \nTraining set [40640/60000 (68%)] Loss: 1.56994 \nTraining set [40960/60000 (68%)] Loss: 1.49254 \nTraining set [41280/60000 (69%)] Loss: 1.59691 \nTraining set [41600/60000 (69%)] Loss: 1.55572 \nTraining set [41920/60000 (70%)] Loss: 1.55050 \nTraining set [42240/60000 (70%)] Loss: 1.57047 \nTraining set [42560/60000 (71%)] Loss: 1.50776 \nTraining set [42880/60000 (71%)] Loss: 1.53310 \nTraining set [43200/60000 (72%)] Loss: 1.49197 \nTraining set [43520/60000 (72%)] Loss: 1.54783 \nTraining set [43840/60000 (73%)] Loss: 1.54037 \nTraining set [44160/60000 (74%)] Loss: 1.48493 \nTraining set [44480/60000 (74%)] Loss: 1.51885 \nTraining set [44800/60000 (75%)] Loss: 1.47621 \nTraining set [45120/60000 (75%)] Loss: 1.49326 \nTraining set [45440/60000 (76%)] Loss: 1.57693 \nTraining set [45760/60000 (76%)] Loss: 1.52121 \nTraining set [46080/60000 (77%)] Loss: 1.58277 \nTraining set [46400/60000 (77%)] Loss: 1.56669 \nTraining set [46720/60000 (78%)] Loss: 1.54028 \nTraining set [47040/60000 (78%)] Loss: 1.55593 \nTraining set [47360/60000 (79%)] Loss: 1.53212 \nTraining set [47680/60000 (79%)] Loss: 1.51665 \nTraining set [48000/60000 (80%)] Loss: 1.52957 \nTraining set [48320/60000 (80%)] Loss: 1.57250 \nTraining set [48640/60000 (81%)] Loss: 1.59951 \nTraining set [48960/60000 (82%)] Loss: 1.58870 \nTraining set [49280/60000 (82%)] Loss: 1.55139 \nTraining set [49600/60000 (83%)] Loss: 1.58652 \nTraining set [49920/60000 (83%)] Loss: 1.55716 \nTraining set [50240/60000 (84%)] Loss: 1.56296 \nTraining set [50560/60000 (84%)] Loss: 1.55557 \nTraining set [50880/60000 (85%)] Loss: 1.49796 \nTraining set [51200/60000 (85%)] Loss: 1.53909 \nTraining set [51520/60000 (86%)] Loss: 1.54285 \nTraining set [51840/60000 (86%)] Loss: 1.52502 \nTraining set [52160/60000 (87%)] Loss: 1.56817 \nTraining set [52480/60000 (87%)] Loss: 1.55763 \nTraining set [52800/60000 (88%)] Loss: 1.60296 \nTraining set [53120/60000 (88%)] Loss: 1.51755 \nTraining set [53440/60000 (89%)] Loss: 1.54341 \nTraining set [53760/60000 (90%)] Loss: 1.57276 \nTraining set [54080/60000 (90%)] Loss: 1.50451 \nTraining set [54400/60000 (91%)] Loss: 1.59376 \nTraining set [54720/60000 (91%)] Loss: 1.57239 \nTraining set [55040/60000 (92%)] Loss: 1.56376 \nTraining set [55360/60000 (92%)] Loss: 1.50997 \nTraining set [55680/60000 (93%)] Loss: 1.50524 \nTraining set [56000/60000 (93%)] Loss: 1.51409 \nTraining set [56320/60000 (94%)] Loss: 1.54426 \nTraining set [56640/60000 (94%)] Loss: 1.58687 \nTraining set [56960/60000 (95%)] Loss: 1.54867 \nTraining set [57280/60000 (95%)] Loss: 1.56580 \nTraining set [57600/60000 (96%)] Loss: 1.60192 \nTraining set [57920/60000 (96%)] Loss: 1.52686 \nTraining set [58240/60000 (97%)] Loss: 1.55942 \nTraining set [58560/60000 (98%)] Loss: 1.64683 \nTraining set [58880/60000 (98%)] Loss: 1.56452 \nTraining set [59200/60000 (99%)] Loss: 1.60623 \nTraining set [59520/60000 (99%)] Loss: 1.55306 \nTraining set [59840/60000 (100%)] Loss: 1.57965 \nAverage Training Loss: 1.55006\nTraining Completed!\nModel Validation Starts!\nAverage Validation Loss: 1.56252, Accuracy: 9002/10000 (90%)\n\nEpoch: 21\nTraining set [0/60000 (0%)] Loss: 1.52578 \nTraining set [320/60000 (1%)] Loss: 1.58737 \nTraining set [640/60000 (1%)] Loss: 1.53513 \nTraining set [960/60000 (2%)] Loss: 1.54380 \nTraining set [1280/60000 (2%)] Loss: 1.55794 \nTraining set [1600/60000 (3%)] Loss: 1.47113 \nTraining set [1920/60000 (3%)] Loss: 1.55773 \nTraining set [2240/60000 (4%)] Loss: 1.59504 \nTraining set [2560/60000 (4%)] Loss: 1.52520 \nTraining set [2880/60000 (5%)] Loss: 1.57531 \nTraining set [3200/60000 (5%)] Loss: 1.52995 \nTraining set [3520/60000 (6%)] Loss: 1.57857 \nTraining set [3840/60000 (6%)] Loss: 1.52743 \nTraining set [4160/60000 (7%)] Loss: 1.52316 \nTraining set [4480/60000 (7%)] Loss: 1.54391 \nTraining set [4800/60000 (8%)] Loss: 1.51223 \nTraining set [5120/60000 (9%)] Loss: 1.50251 \nTraining set [5440/60000 (9%)] Loss: 1.60383 \nTraining set [5760/60000 (10%)] Loss: 1.55517 \nTraining set [6080/60000 (10%)] Loss: 1.55038 \nTraining set [6400/60000 (11%)] Loss: 1.49371 \nTraining set [6720/60000 (11%)] Loss: 1.59929 \nTraining set [7040/60000 (12%)] Loss: 1.54062 \nTraining set [7360/60000 (12%)] Loss: 1.51204 \nTraining set [7680/60000 (13%)] Loss: 1.62384 \nTraining set [8000/60000 (13%)] Loss: 1.51291 \nTraining set [8320/60000 (14%)] Loss: 1.51783 \nTraining set [8640/60000 (14%)] Loss: 1.50044 \nTraining set [8960/60000 (15%)] Loss: 1.60845 \nTraining set [9280/60000 (15%)] Loss: 1.54059 \nTraining set [9600/60000 (16%)] Loss: 1.53193 \nTraining set [9920/60000 (17%)] Loss: 1.53558 \nTraining set [10240/60000 (17%)] Loss: 1.56189 \nTraining set [10560/60000 (18%)] Loss: 1.60042 \nTraining set [10880/60000 (18%)] Loss: 1.51786 \nTraining set [11200/60000 (19%)] Loss: 1.55876 \nTraining set [11520/60000 (19%)] Loss: 1.57732 \nTraining set [11840/60000 (20%)] Loss: 1.53735 \nTraining set [12160/60000 (20%)] Loss: 1.54953 \nTraining set [12480/60000 (21%)] Loss: 1.56804 \nTraining set [12800/60000 (21%)] Loss: 1.52833 \nTraining set [13120/60000 (22%)] Loss: 1.54809 \nTraining set [13440/60000 (22%)] Loss: 1.57991 \nTraining set [13760/60000 (23%)] Loss: 1.52731 \nTraining set [14080/60000 (23%)] Loss: 1.54803 \nTraining set [14400/60000 (24%)] Loss: 1.52413 \nTraining set [14720/60000 (25%)] Loss: 1.57749 \nTraining set [15040/60000 (25%)] Loss: 1.52837 \nTraining set [15360/60000 (26%)] Loss: 1.57065 \nTraining set [15680/60000 (26%)] Loss: 1.61793 \nTraining set [16000/60000 (27%)] Loss: 1.52382 \nTraining set [16320/60000 (27%)] Loss: 1.46504 \nTraining set [16640/60000 (28%)] Loss: 1.56226 \nTraining set [16960/60000 (28%)] Loss: 1.47516 \nTraining set [17280/60000 (29%)] Loss: 1.56716 \nTraining set [17600/60000 (29%)] Loss: 1.53470 \nTraining set [17920/60000 (30%)] Loss: 1.63440 \nTraining set [18240/60000 (30%)] Loss: 1.56710 \nTraining set [18560/60000 (31%)] Loss: 1.56996 \nTraining set [18880/60000 (31%)] Loss: 1.48692 \nTraining set [19200/60000 (32%)] Loss: 1.61080 \nTraining set [19520/60000 (33%)] Loss: 1.56778 \nTraining set [19840/60000 (33%)] Loss: 1.57843 \nTraining set [20160/60000 (34%)] Loss: 1.54205 \nTraining set [20480/60000 (34%)] Loss: 1.53162 \nTraining set [20800/60000 (35%)] Loss: 1.54479 \nTraining set [21120/60000 (35%)] Loss: 1.60128 \nTraining set [21440/60000 (36%)] Loss: 1.56183 \nTraining set [21760/60000 (36%)] Loss: 1.54608 \nTraining set [22080/60000 (37%)] Loss: 1.55883 \nTraining set [22400/60000 (37%)] Loss: 1.57552 \nTraining set [22720/60000 (38%)] Loss: 1.54835 \nTraining set [23040/60000 (38%)] Loss: 1.53552 \nTraining set [23360/60000 (39%)] Loss: 1.57626 \nTraining set [23680/60000 (39%)] Loss: 1.57844 \nTraining set [24000/60000 (40%)] Loss: 1.53451 \nTraining set [24320/60000 (41%)] Loss: 1.56026 \nTraining set [24640/60000 (41%)] Loss: 1.51600 \nTraining set [24960/60000 (42%)] Loss: 1.53656 \nTraining set [25280/60000 (42%)] Loss: 1.55843 \nTraining set [25600/60000 (43%)] Loss: 1.55709 \nTraining set [25920/60000 (43%)] Loss: 1.55541 \nTraining set [26240/60000 (44%)] Loss: 1.55377 \nTraining set [26560/60000 (44%)] Loss: 1.53774 \nTraining set [26880/60000 (45%)] Loss: 1.50042 \nTraining set [27200/60000 (45%)] Loss: 1.56566 \nTraining set [27520/60000 (46%)] Loss: 1.51536 \nTraining set [27840/60000 (46%)] Loss: 1.51341 \nTraining set [28160/60000 (47%)] Loss: 1.55341 \nTraining set [28480/60000 (47%)] Loss: 1.57659 \nTraining set [28800/60000 (48%)] Loss: 1.61361 \nTraining set [29120/60000 (49%)] Loss: 1.55114 \nTraining set [29440/60000 (49%)] Loss: 1.51904 \nTraining set [29760/60000 (50%)] Loss: 1.56084 \nTraining set [30080/60000 (50%)] Loss: 1.53424 \nTraining set [30400/60000 (51%)] Loss: 1.51604 \nTraining set [30720/60000 (51%)] Loss: 1.58772 \nTraining set [31040/60000 (52%)] Loss: 1.47599 \nTraining set [31360/60000 (52%)] Loss: 1.56796 \nTraining set [31680/60000 (53%)] Loss: 1.50790 \nTraining set [32000/60000 (53%)] Loss: 1.53858 \nTraining set [32320/60000 (54%)] Loss: 1.49309 \nTraining set [32640/60000 (54%)] Loss: 1.58222 \nTraining set [32960/60000 (55%)] Loss: 1.54468 \nTraining set [33280/60000 (55%)] Loss: 1.57288 \nTraining set [33600/60000 (56%)] Loss: 1.55879 \nTraining set [33920/60000 (57%)] Loss: 1.53529 \nTraining set [34240/60000 (57%)] Loss: 1.56269 \nTraining set [34560/60000 (58%)] Loss: 1.53168 \nTraining set [34880/60000 (58%)] Loss: 1.54341 \nTraining set [35200/60000 (59%)] Loss: 1.56007 \nTraining set [35520/60000 (59%)] Loss: 1.53894 \nTraining set [35840/60000 (60%)] Loss: 1.53622 \nTraining set [36160/60000 (60%)] Loss: 1.56653 \nTraining set [36480/60000 (61%)] Loss: 1.55491 \nTraining set [36800/60000 (61%)] Loss: 1.53460 \nTraining set [37120/60000 (62%)] Loss: 1.53776 \nTraining set [37440/60000 (62%)] Loss: 1.58992 \nTraining set [37760/60000 (63%)] Loss: 1.51777 \nTraining set [38080/60000 (63%)] Loss: 1.58153 \nTraining set [38400/60000 (64%)] Loss: 1.50797 \nTraining set [38720/60000 (64%)] Loss: 1.53459 \nTraining set [39040/60000 (65%)] Loss: 1.60039 \nTraining set [39360/60000 (66%)] Loss: 1.56733 \nTraining set [39680/60000 (66%)] Loss: 1.53710 \nTraining set [40000/60000 (67%)] Loss: 1.54289 \nTraining set [40320/60000 (67%)] Loss: 1.52984 \nTraining set [40640/60000 (68%)] Loss: 1.53691 \nTraining set [40960/60000 (68%)] Loss: 1.54928 \nTraining set [41280/60000 (69%)] Loss: 1.52288 \nTraining set [41600/60000 (69%)] Loss: 1.55858 \nTraining set [41920/60000 (70%)] Loss: 1.57497 \nTraining set [42240/60000 (70%)] Loss: 1.59640 \nTraining set [42560/60000 (71%)] Loss: 1.55856 \nTraining set [42880/60000 (71%)] Loss: 1.57528 \nTraining set [43200/60000 (72%)] Loss: 1.50945 \nTraining set [43520/60000 (72%)] Loss: 1.54617 \nTraining set [43840/60000 (73%)] Loss: 1.55792 \nTraining set [44160/60000 (74%)] Loss: 1.56975 \nTraining set [44480/60000 (74%)] Loss: 1.59181 \nTraining set [44800/60000 (75%)] Loss: 1.55620 \nTraining set [45120/60000 (75%)] Loss: 1.55146 \nTraining set [45440/60000 (76%)] Loss: 1.59341 \nTraining set [45760/60000 (76%)] Loss: 1.52856 \nTraining set [46080/60000 (77%)] Loss: 1.56620 \nTraining set [46400/60000 (77%)] Loss: 1.50803 \nTraining set [46720/60000 (78%)] Loss: 1.52664 \nTraining set [47040/60000 (78%)] Loss: 1.54059 \nTraining set [47360/60000 (79%)] Loss: 1.52820 \nTraining set [47680/60000 (79%)] Loss: 1.52597 \nTraining set [48000/60000 (80%)] Loss: 1.53915 \nTraining set [48320/60000 (80%)] Loss: 1.56988 \nTraining set [48640/60000 (81%)] Loss: 1.56358 \nTraining set [48960/60000 (82%)] Loss: 1.54264 \nTraining set [49280/60000 (82%)] Loss: 1.55140 \nTraining set [49600/60000 (83%)] Loss: 1.49369 \nTraining set [49920/60000 (83%)] Loss: 1.54377 \nTraining set [50240/60000 (84%)] Loss: 1.54932 \nTraining set [50560/60000 (84%)] Loss: 1.53406 \nTraining set [50880/60000 (85%)] Loss: 1.57981 \nTraining set [51200/60000 (85%)] Loss: 1.56779 \nTraining set [51520/60000 (86%)] Loss: 1.59147 \nTraining set [51840/60000 (86%)] Loss: 1.56531 \nTraining set [52160/60000 (87%)] Loss: 1.55773 \nTraining set [52480/60000 (87%)] Loss: 1.55633 \nTraining set [52800/60000 (88%)] Loss: 1.57808 \nTraining set [53120/60000 (88%)] Loss: 1.56027 \nTraining set [53440/60000 (89%)] Loss: 1.53706 \nTraining set [53760/60000 (90%)] Loss: 1.60114 \nTraining set [54080/60000 (90%)] Loss: 1.57427 \nTraining set [54400/60000 (91%)] Loss: 1.51618 \nTraining set [54720/60000 (91%)] Loss: 1.58410 \nTraining set [55040/60000 (92%)] Loss: 1.53883 \nTraining set [55360/60000 (92%)] Loss: 1.56236 \nTraining set [55680/60000 (93%)] Loss: 1.59181 \nTraining set [56000/60000 (93%)] Loss: 1.56015 \nTraining set [56320/60000 (94%)] Loss: 1.58827 \nTraining set [56640/60000 (94%)] Loss: 1.58172 \nTraining set [56960/60000 (95%)] Loss: 1.56991 \nTraining set [57280/60000 (95%)] Loss: 1.49694 \nTraining set [57600/60000 (96%)] Loss: 1.52085 \nTraining set [57920/60000 (96%)] Loss: 1.52723 \nTraining set [58240/60000 (97%)] Loss: 1.56405 \nTraining set [58560/60000 (98%)] Loss: 1.51490 \nTraining set [58880/60000 (98%)] Loss: 1.53923 \nTraining set [59200/60000 (99%)] Loss: 1.57194 \nTraining set [59520/60000 (99%)] Loss: 1.54786 \nTraining set [59840/60000 (100%)] Loss: 1.62008 \nAverage Training Loss: 1.54876\nTraining Completed!\nModel Validation Starts!\nAverage Validation Loss: 1.56776, Accuracy: 8940/10000 (89%)\n\nEpoch: 22\nTraining set [0/60000 (0%)] Loss: 1.56227 \nTraining set [320/60000 (1%)] Loss: 1.55191 \nTraining set [640/60000 (1%)] Loss: 1.59266 \nTraining set [960/60000 (2%)] Loss: 1.53318 \nTraining set [1280/60000 (2%)] Loss: 1.55810 \nTraining set [1600/60000 (3%)] Loss: 1.57383 \nTraining set [1920/60000 (3%)] Loss: 1.56700 \nTraining set [2240/60000 (4%)] Loss: 1.57281 \nTraining set [2560/60000 (4%)] Loss: 1.57551 \nTraining set [2880/60000 (5%)] Loss: 1.56914 \nTraining set [3200/60000 (5%)] Loss: 1.54003 \nTraining set [3520/60000 (6%)] Loss: 1.50097 \nTraining set [3840/60000 (6%)] Loss: 1.53832 \nTraining set [4160/60000 (7%)] Loss: 1.57891 \nTraining set [4480/60000 (7%)] Loss: 1.54057 \nTraining set [4800/60000 (8%)] Loss: 1.55614 \nTraining set [5120/60000 (9%)] Loss: 1.49761 \nTraining set [5440/60000 (9%)] Loss: 1.51555 \nTraining set [5760/60000 (10%)] Loss: 1.51006 \nTraining set [6080/60000 (10%)] Loss: 1.58203 \nTraining set [6400/60000 (11%)] Loss: 1.51222 \nTraining set [6720/60000 (11%)] Loss: 1.54266 \nTraining set [7040/60000 (12%)] Loss: 1.52017 \nTraining set [7360/60000 (12%)] Loss: 1.52056 \nTraining set [7680/60000 (13%)] Loss: 1.60541 \nTraining set [8000/60000 (13%)] Loss: 1.50259 \nTraining set [8320/60000 (14%)] Loss: 1.53304 \nTraining set [8640/60000 (14%)] Loss: 1.54458 \nTraining set [8960/60000 (15%)] Loss: 1.54196 \nTraining set [9280/60000 (15%)] Loss: 1.56184 \nTraining set [9600/60000 (16%)] Loss: 1.54091 \nTraining set [9920/60000 (17%)] Loss: 1.50862 \nTraining set [10240/60000 (17%)] Loss: 1.51022 \nTraining set [10560/60000 (18%)] Loss: 1.56669 \nTraining set [10880/60000 (18%)] Loss: 1.57800 \nTraining set [11200/60000 (19%)] Loss: 1.51161 \nTraining set [11520/60000 (19%)] Loss: 1.55398 \nTraining set [11840/60000 (20%)] Loss: 1.57485 \nTraining set [12160/60000 (20%)] Loss: 1.53667 \nTraining set [12480/60000 (21%)] Loss: 1.62257 \nTraining set [12800/60000 (21%)] Loss: 1.56022 \nTraining set [13120/60000 (22%)] Loss: 1.52549 \nTraining set [13440/60000 (22%)] Loss: 1.54702 \nTraining set [13760/60000 (23%)] Loss: 1.59441 \nTraining set [14080/60000 (23%)] Loss: 1.59948 \nTraining set [14400/60000 (24%)] Loss: 1.53345 \nTraining set [14720/60000 (25%)] Loss: 1.49300 \nTraining set [15040/60000 (25%)] Loss: 1.53461 \nTraining set [15360/60000 (26%)] Loss: 1.48977 \nTraining set [15680/60000 (26%)] Loss: 1.60953 \nTraining set [16000/60000 (27%)] Loss: 1.58001 \nTraining set [16320/60000 (27%)] Loss: 1.52118 \nTraining set [16640/60000 (28%)] Loss: 1.53132 \nTraining set [16960/60000 (28%)] Loss: 1.56773 \nTraining set [17280/60000 (29%)] Loss: 1.50340 \nTraining set [17600/60000 (29%)] Loss: 1.54106 \nTraining set [17920/60000 (30%)] Loss: 1.48885 \nTraining set [18240/60000 (30%)] Loss: 1.58312 \nTraining set [18560/60000 (31%)] Loss: 1.52699 \nTraining set [18880/60000 (31%)] Loss: 1.54379 \nTraining set [19200/60000 (32%)] Loss: 1.51805 \nTraining set [19520/60000 (33%)] Loss: 1.53166 \nTraining set [19840/60000 (33%)] Loss: 1.51773 \nTraining set [20160/60000 (34%)] Loss: 1.52331 \nTraining set [20480/60000 (34%)] Loss: 1.55529 \nTraining set [20800/60000 (35%)] Loss: 1.52027 \nTraining set [21120/60000 (35%)] Loss: 1.54563 \nTraining set [21440/60000 (36%)] Loss: 1.56829 \nTraining set [21760/60000 (36%)] Loss: 1.54606 \nTraining set [22080/60000 (37%)] Loss: 1.55206 \nTraining set [22400/60000 (37%)] Loss: 1.49480 \nTraining set [22720/60000 (38%)] Loss: 1.59333 \nTraining set [23040/60000 (38%)] Loss: 1.60065 \nTraining set [23360/60000 (39%)] Loss: 1.56216 \nTraining set [23680/60000 (39%)] Loss: 1.55144 \nTraining set [24000/60000 (40%)] Loss: 1.52582 \nTraining set [24320/60000 (41%)] Loss: 1.55730 \nTraining set [24640/60000 (41%)] Loss: 1.52800 \nTraining set [24960/60000 (42%)] Loss: 1.52224 \nTraining set [25280/60000 (42%)] Loss: 1.51935 \nTraining set [25600/60000 (43%)] Loss: 1.58858 \nTraining set [25920/60000 (43%)] Loss: 1.60628 \nTraining set [26240/60000 (44%)] Loss: 1.51164 \nTraining set [26560/60000 (44%)] Loss: 1.48426 \nTraining set [26880/60000 (45%)] Loss: 1.55197 \nTraining set [27200/60000 (45%)] Loss: 1.52686 \nTraining set [27520/60000 (46%)] Loss: 1.52250 \nTraining set [27840/60000 (46%)] Loss: 1.56468 \nTraining set [28160/60000 (47%)] Loss: 1.57277 \nTraining set [28480/60000 (47%)] Loss: 1.54109 \nTraining set [28800/60000 (48%)] Loss: 1.52664 \nTraining set [29120/60000 (49%)] Loss: 1.54022 \nTraining set [29440/60000 (49%)] Loss: 1.52791 \nTraining set [29760/60000 (50%)] Loss: 1.58751 \nTraining set [30080/60000 (50%)] Loss: 1.56380 \nTraining set [30400/60000 (51%)] Loss: 1.61209 \nTraining set [30720/60000 (51%)] Loss: 1.53644 \nTraining set [31040/60000 (52%)] Loss: 1.58994 \nTraining set [31360/60000 (52%)] Loss: 1.52122 \nTraining set [31680/60000 (53%)] Loss: 1.54020 \nTraining set [32000/60000 (53%)] Loss: 1.52819 \nTraining set [32320/60000 (54%)] Loss: 1.53403 \nTraining set [32640/60000 (54%)] Loss: 1.58558 \nTraining set [32960/60000 (55%)] Loss: 1.50788 \nTraining set [33280/60000 (55%)] Loss: 1.63526 \nTraining set [33600/60000 (56%)] Loss: 1.56185 \nTraining set [33920/60000 (57%)] Loss: 1.55772 \nTraining set [34240/60000 (57%)] Loss: 1.54172 \nTraining set [34560/60000 (58%)] Loss: 1.51544 \nTraining set [34880/60000 (58%)] Loss: 1.56631 \nTraining set [35200/60000 (59%)] Loss: 1.54224 \nTraining set [35520/60000 (59%)] Loss: 1.51668 \nTraining set [35840/60000 (60%)] Loss: 1.49976 \nTraining set [36160/60000 (60%)] Loss: 1.52310 \nTraining set [36480/60000 (61%)] Loss: 1.48234 \nTraining set [36800/60000 (61%)] Loss: 1.52682 \nTraining set [37120/60000 (62%)] Loss: 1.55156 \nTraining set [37440/60000 (62%)] Loss: 1.52427 \nTraining set [37760/60000 (63%)] Loss: 1.52046 \nTraining set [38080/60000 (63%)] Loss: 1.52818 \nTraining set [38400/60000 (64%)] Loss: 1.52339 \nTraining set [38720/60000 (64%)] Loss: 1.52814 \nTraining set [39040/60000 (65%)] Loss: 1.50756 \nTraining set [39360/60000 (66%)] Loss: 1.50212 \nTraining set [39680/60000 (66%)] Loss: 1.62001 \nTraining set [40000/60000 (67%)] Loss: 1.57566 \nTraining set [40320/60000 (67%)] Loss: 1.57655 \nTraining set [40640/60000 (68%)] Loss: 1.52321 \nTraining set [40960/60000 (68%)] Loss: 1.52568 \nTraining set [41280/60000 (69%)] Loss: 1.57021 \nTraining set [41600/60000 (69%)] Loss: 1.53905 \nTraining set [41920/60000 (70%)] Loss: 1.57960 \nTraining set [42240/60000 (70%)] Loss: 1.57418 \nTraining set [42560/60000 (71%)] Loss: 1.53833 \nTraining set [42880/60000 (71%)] Loss: 1.52195 \nTraining set [43200/60000 (72%)] Loss: 1.55734 \nTraining set [43520/60000 (72%)] Loss: 1.53586 \nTraining set [43840/60000 (73%)] Loss: 1.54798 \nTraining set [44160/60000 (74%)] Loss: 1.50339 \nTraining set [44480/60000 (74%)] Loss: 1.52448 \nTraining set [44800/60000 (75%)] Loss: 1.53426 \nTraining set [45120/60000 (75%)] Loss: 1.53005 \nTraining set [45440/60000 (76%)] Loss: 1.53532 \nTraining set [45760/60000 (76%)] Loss: 1.54505 \nTraining set [46080/60000 (77%)] Loss: 1.55798 \nTraining set [46400/60000 (77%)] Loss: 1.57126 \nTraining set [46720/60000 (78%)] Loss: 1.57048 \nTraining set [47040/60000 (78%)] Loss: 1.56685 \nTraining set [47360/60000 (79%)] Loss: 1.52300 \nTraining set [47680/60000 (79%)] Loss: 1.51370 \nTraining set [48000/60000 (80%)] Loss: 1.53189 \nTraining set [48320/60000 (80%)] Loss: 1.53275 \nTraining set [48640/60000 (81%)] Loss: 1.60042 \nTraining set [48960/60000 (82%)] Loss: 1.56793 \nTraining set [49280/60000 (82%)] Loss: 1.53708 \nTraining set [49600/60000 (83%)] Loss: 1.53300 \nTraining set [49920/60000 (83%)] Loss: 1.50134 \nTraining set [50240/60000 (84%)] Loss: 1.46325 \nTraining set [50560/60000 (84%)] Loss: 1.53834 \nTraining set [50880/60000 (85%)] Loss: 1.57113 \nTraining set [51200/60000 (85%)] Loss: 1.60230 \nTraining set [51520/60000 (86%)] Loss: 1.57796 \nTraining set [51840/60000 (86%)] Loss: 1.55307 \nTraining set [52160/60000 (87%)] Loss: 1.55678 \nTraining set [52480/60000 (87%)] Loss: 1.61120 \nTraining set [52800/60000 (88%)] Loss: 1.52812 \nTraining set [53120/60000 (88%)] Loss: 1.52153 \nTraining set [53440/60000 (89%)] Loss: 1.54814 \nTraining set [53760/60000 (90%)] Loss: 1.50642 \nTraining set [54080/60000 (90%)] Loss: 1.57516 \nTraining set [54400/60000 (91%)] Loss: 1.53400 \nTraining set [54720/60000 (91%)] Loss: 1.53499 \nTraining set [55040/60000 (92%)] Loss: 1.53678 \nTraining set [55360/60000 (92%)] Loss: 1.52519 \nTraining set [55680/60000 (93%)] Loss: 1.51529 \nTraining set [56000/60000 (93%)] Loss: 1.54683 \nTraining set [56320/60000 (94%)] Loss: 1.61786 \nTraining set [56640/60000 (94%)] Loss: 1.58729 \nTraining set [56960/60000 (95%)] Loss: 1.59154 \nTraining set [57280/60000 (95%)] Loss: 1.49229 \nTraining set [57600/60000 (96%)] Loss: 1.55870 \nTraining set [57920/60000 (96%)] Loss: 1.49275 \nTraining set [58240/60000 (97%)] Loss: 1.48165 \nTraining set [58560/60000 (98%)] Loss: 1.59580 \nTraining set [58880/60000 (98%)] Loss: 1.55902 \nTraining set [59200/60000 (99%)] Loss: 1.51930 \nTraining set [59520/60000 (99%)] Loss: 1.54805 \nTraining set [59840/60000 (100%)] Loss: 1.55860 \nAverage Training Loss: 1.54800\nTraining Completed!\nModel Validation Starts!\nAverage Validation Loss: 1.55807, Accuracy: 9060/10000 (91%)\n\nEpoch: 23\nTraining set [0/60000 (0%)] Loss: 1.53898 \nTraining set [320/60000 (1%)] Loss: 1.49525 \nTraining set [640/60000 (1%)] Loss: 1.57089 \nTraining set [960/60000 (2%)] Loss: 1.49657 \nTraining set [1280/60000 (2%)] Loss: 1.59907 \nTraining set [1600/60000 (3%)] Loss: 1.57064 \nTraining set [1920/60000 (3%)] Loss: 1.53826 \nTraining set [2240/60000 (4%)] Loss: 1.61080 \nTraining set [2560/60000 (4%)] Loss: 1.58828 \nTraining set [2880/60000 (5%)] Loss: 1.49206 \nTraining set [3200/60000 (5%)] Loss: 1.51519 \nTraining set [3520/60000 (6%)] Loss: 1.58672 \nTraining set [3840/60000 (6%)] Loss: 1.48277 \nTraining set [4160/60000 (7%)] Loss: 1.55811 \nTraining set [4480/60000 (7%)] Loss: 1.57541 \nTraining set [4800/60000 (8%)] Loss: 1.54807 \nTraining set [5120/60000 (9%)] Loss: 1.54211 \nTraining set [5440/60000 (9%)] Loss: 1.54099 \nTraining set [5760/60000 (10%)] Loss: 1.61541 \nTraining set [6080/60000 (10%)] Loss: 1.49971 \nTraining set [6400/60000 (11%)] Loss: 1.51364 \nTraining set [6720/60000 (11%)] Loss: 1.49199 \nTraining set [7040/60000 (12%)] Loss: 1.52278 \nTraining set [7360/60000 (12%)] Loss: 1.54322 \nTraining set [7680/60000 (13%)] Loss: 1.53874 \nTraining set [8000/60000 (13%)] Loss: 1.52377 \nTraining set [8320/60000 (14%)] Loss: 1.57241 \nTraining set [8640/60000 (14%)] Loss: 1.52114 \nTraining set [8960/60000 (15%)] Loss: 1.56540 \nTraining set [9280/60000 (15%)] Loss: 1.53909 \nTraining set [9600/60000 (16%)] Loss: 1.60949 \nTraining set [9920/60000 (17%)] Loss: 1.55124 \nTraining set [10240/60000 (17%)] Loss: 1.53696 \nTraining set [10560/60000 (18%)] Loss: 1.59767 \nTraining set [10880/60000 (18%)] Loss: 1.51113 \nTraining set [11200/60000 (19%)] Loss: 1.52122 \nTraining set [11520/60000 (19%)] Loss: 1.51861 \nTraining set [11840/60000 (20%)] Loss: 1.57331 \nTraining set [12160/60000 (20%)] Loss: 1.50399 \nTraining set [12480/60000 (21%)] Loss: 1.51991 \nTraining set [12800/60000 (21%)] Loss: 1.52871 \nTraining set [13120/60000 (22%)] Loss: 1.56036 \nTraining set [13440/60000 (22%)] Loss: 1.57668 \nTraining set [13760/60000 (23%)] Loss: 1.65337 \nTraining set [14080/60000 (23%)] Loss: 1.53859 \nTraining set [14400/60000 (24%)] Loss: 1.59407 \nTraining set [14720/60000 (25%)] Loss: 1.53520 \nTraining set [15040/60000 (25%)] Loss: 1.51028 \nTraining set [15360/60000 (26%)] Loss: 1.54280 \nTraining set [15680/60000 (26%)] Loss: 1.51812 \nTraining set [16000/60000 (27%)] Loss: 1.50447 \nTraining set [16320/60000 (27%)] Loss: 1.58441 \nTraining set [16640/60000 (28%)] Loss: 1.53286 \nTraining set [16960/60000 (28%)] Loss: 1.54348 \nTraining set [17280/60000 (29%)] Loss: 1.55273 \nTraining set [17600/60000 (29%)] Loss: 1.48705 \nTraining set [17920/60000 (30%)] Loss: 1.53083 \nTraining set [18240/60000 (30%)] Loss: 1.52230 \nTraining set [18560/60000 (31%)] Loss: 1.56452 \nTraining set [18880/60000 (31%)] Loss: 1.56672 \nTraining set [19200/60000 (32%)] Loss: 1.55641 \nTraining set [19520/60000 (33%)] Loss: 1.55636 \nTraining set [19840/60000 (33%)] Loss: 1.54183 \nTraining set [20160/60000 (34%)] Loss: 1.57048 \nTraining set [20480/60000 (34%)] Loss: 1.49629 \nTraining set [20800/60000 (35%)] Loss: 1.54971 \nTraining set [21120/60000 (35%)] Loss: 1.56397 \nTraining set [21440/60000 (36%)] Loss: 1.58374 \nTraining set [21760/60000 (36%)] Loss: 1.58744 \nTraining set [22080/60000 (37%)] Loss: 1.56893 \nTraining set [22400/60000 (37%)] Loss: 1.57733 \nTraining set [22720/60000 (38%)] Loss: 1.52864 \nTraining set [23040/60000 (38%)] Loss: 1.53672 \nTraining set [23360/60000 (39%)] Loss: 1.54365 \nTraining set [23680/60000 (39%)] Loss: 1.47718 \nTraining set [24000/60000 (40%)] Loss: 1.59372 \nTraining set [24320/60000 (41%)] Loss: 1.62053 \nTraining set [24640/60000 (41%)] Loss: 1.52216 \nTraining set [24960/60000 (42%)] Loss: 1.51052 \nTraining set [25280/60000 (42%)] Loss: 1.53003 \nTraining set [25600/60000 (43%)] Loss: 1.53791 \nTraining set [25920/60000 (43%)] Loss: 1.56747 \nTraining set [26240/60000 (44%)] Loss: 1.51845 \nTraining set [26560/60000 (44%)] Loss: 1.53561 \nTraining set [26880/60000 (45%)] Loss: 1.62162 \nTraining set [27200/60000 (45%)] Loss: 1.60508 \nTraining set [27520/60000 (46%)] Loss: 1.54635 \nTraining set [27840/60000 (46%)] Loss: 1.61496 \nTraining set [28160/60000 (47%)] Loss: 1.54098 \nTraining set [28480/60000 (47%)] Loss: 1.53389 \nTraining set [28800/60000 (48%)] Loss: 1.60462 \nTraining set [29120/60000 (49%)] Loss: 1.53733 \nTraining set [29440/60000 (49%)] Loss: 1.57011 \nTraining set [29760/60000 (50%)] Loss: 1.54176 \nTraining set [30080/60000 (50%)] Loss: 1.56177 \nTraining set [30400/60000 (51%)] Loss: 1.57252 \nTraining set [30720/60000 (51%)] Loss: 1.57450 \nTraining set [31040/60000 (52%)] Loss: 1.49575 \nTraining set [31360/60000 (52%)] Loss: 1.56513 \nTraining set [31680/60000 (53%)] Loss: 1.55042 \nTraining set [32000/60000 (53%)] Loss: 1.54015 \nTraining set [32320/60000 (54%)] Loss: 1.53970 \nTraining set [32640/60000 (54%)] Loss: 1.51028 \nTraining set [32960/60000 (55%)] Loss: 1.52817 \nTraining set [33280/60000 (55%)] Loss: 1.55338 \nTraining set [33600/60000 (56%)] Loss: 1.59801 \nTraining set [33920/60000 (57%)] Loss: 1.59847 \nTraining set [34240/60000 (57%)] Loss: 1.57458 \nTraining set [34560/60000 (58%)] Loss: 1.54180 \nTraining set [34880/60000 (58%)] Loss: 1.57214 \nTraining set [35200/60000 (59%)] Loss: 1.54351 \nTraining set [35520/60000 (59%)] Loss: 1.53097 \nTraining set [35840/60000 (60%)] Loss: 1.52811 \nTraining set [36160/60000 (60%)] Loss: 1.54809 \nTraining set [36480/60000 (61%)] Loss: 1.52702 \nTraining set [36800/60000 (61%)] Loss: 1.52435 \nTraining set [37120/60000 (62%)] Loss: 1.52461 \nTraining set [37440/60000 (62%)] Loss: 1.57572 \nTraining set [37760/60000 (63%)] Loss: 1.52250 \nTraining set [38080/60000 (63%)] Loss: 1.48578 \nTraining set [38400/60000 (64%)] Loss: 1.56897 \nTraining set [38720/60000 (64%)] Loss: 1.59240 \nTraining set [39040/60000 (65%)] Loss: 1.51948 \nTraining set [39360/60000 (66%)] Loss: 1.56110 \nTraining set [39680/60000 (66%)] Loss: 1.62037 \nTraining set [40000/60000 (67%)] Loss: 1.52212 \nTraining set [40320/60000 (67%)] Loss: 1.55046 \nTraining set [40640/60000 (68%)] Loss: 1.53379 \nTraining set [40960/60000 (68%)] Loss: 1.56466 \nTraining set [41280/60000 (69%)] Loss: 1.50689 \nTraining set [41600/60000 (69%)] Loss: 1.54135 \nTraining set [41920/60000 (70%)] Loss: 1.53173 \nTraining set [42240/60000 (70%)] Loss: 1.55754 \nTraining set [42560/60000 (71%)] Loss: 1.55167 \nTraining set [42880/60000 (71%)] Loss: 1.50744 \nTraining set [43200/60000 (72%)] Loss: 1.54781 \nTraining set [43520/60000 (72%)] Loss: 1.56158 \nTraining set [43840/60000 (73%)] Loss: 1.54035 \nTraining set [44160/60000 (74%)] Loss: 1.49933 \nTraining set [44480/60000 (74%)] Loss: 1.54181 \nTraining set [44800/60000 (75%)] Loss: 1.52756 \nTraining set [45120/60000 (75%)] Loss: 1.60082 \nTraining set [45440/60000 (76%)] Loss: 1.54123 \nTraining set [45760/60000 (76%)] Loss: 1.58158 \nTraining set [46080/60000 (77%)] Loss: 1.55378 \nTraining set [46400/60000 (77%)] Loss: 1.48124 \nTraining set [46720/60000 (78%)] Loss: 1.54137 \nTraining set [47040/60000 (78%)] Loss: 1.54958 \nTraining set [47360/60000 (79%)] Loss: 1.55606 \nTraining set [47680/60000 (79%)] Loss: 1.53311 \nTraining set [48000/60000 (80%)] Loss: 1.58149 \nTraining set [48320/60000 (80%)] Loss: 1.58950 \nTraining set [48640/60000 (81%)] Loss: 1.52876 \nTraining set [48960/60000 (82%)] Loss: 1.55683 \nTraining set [49280/60000 (82%)] Loss: 1.50231 \nTraining set [49600/60000 (83%)] Loss: 1.55905 \nTraining set [49920/60000 (83%)] Loss: 1.54189 \nTraining set [50240/60000 (84%)] Loss: 1.53416 \nTraining set [50560/60000 (84%)] Loss: 1.54520 \nTraining set [50880/60000 (85%)] Loss: 1.59032 \nTraining set [51200/60000 (85%)] Loss: 1.58529 \nTraining set [51520/60000 (86%)] Loss: 1.49663 \nTraining set [51840/60000 (86%)] Loss: 1.51532 \nTraining set [52160/60000 (87%)] Loss: 1.51878 \nTraining set [52480/60000 (87%)] Loss: 1.54759 \nTraining set [52800/60000 (88%)] Loss: 1.52672 \nTraining set [53120/60000 (88%)] Loss: 1.50941 \nTraining set [53440/60000 (89%)] Loss: 1.53326 \nTraining set [53760/60000 (90%)] Loss: 1.61821 \nTraining set [54080/60000 (90%)] Loss: 1.63359 \nTraining set [54400/60000 (91%)] Loss: 1.53476 \nTraining set [54720/60000 (91%)] Loss: 1.53406 \nTraining set [55040/60000 (92%)] Loss: 1.61600 \nTraining set [55360/60000 (92%)] Loss: 1.61534 \nTraining set [55680/60000 (93%)] Loss: 1.55814 \nTraining set [56000/60000 (93%)] Loss: 1.56930 \nTraining set [56320/60000 (94%)] Loss: 1.60916 \nTraining set [56640/60000 (94%)] Loss: 1.54418 \nTraining set [56960/60000 (95%)] Loss: 1.57214 \nTraining set [57280/60000 (95%)] Loss: 1.56724 \nTraining set [57600/60000 (96%)] Loss: 1.59527 \nTraining set [57920/60000 (96%)] Loss: 1.58099 \nTraining set [58240/60000 (97%)] Loss: 1.53143 \nTraining set [58560/60000 (98%)] Loss: 1.53367 \nTraining set [58880/60000 (98%)] Loss: 1.60613 \nTraining set [59200/60000 (99%)] Loss: 1.57433 \nTraining set [59520/60000 (99%)] Loss: 1.53921 \nTraining set [59840/60000 (100%)] Loss: 1.51461 \nAverage Training Loss: 1.54579\nTraining Completed!\nModel Validation Starts!\nAverage Validation Loss: 1.56255, Accuracy: 8997/10000 (90%)\n\nEpoch: 24\nTraining set [0/60000 (0%)] Loss: 1.51187 \nTraining set [320/60000 (1%)] Loss: 1.60525 \nTraining set [640/60000 (1%)] Loss: 1.57623 \nTraining set [960/60000 (2%)] Loss: 1.53337 \nTraining set [1280/60000 (2%)] Loss: 1.52745 \nTraining set [1600/60000 (3%)] Loss: 1.51796 \nTraining set [1920/60000 (3%)] Loss: 1.51675 \nTraining set [2240/60000 (4%)] Loss: 1.55803 \nTraining set [2560/60000 (4%)] Loss: 1.56585 \nTraining set [2880/60000 (5%)] Loss: 1.58593 \nTraining set [3200/60000 (5%)] Loss: 1.55300 \nTraining set [3520/60000 (6%)] Loss: 1.54231 \nTraining set [3840/60000 (6%)] Loss: 1.57684 \nTraining set [4160/60000 (7%)] Loss: 1.56799 \nTraining set [4480/60000 (7%)] Loss: 1.52513 \nTraining set [4800/60000 (8%)] Loss: 1.50304 \nTraining set [5120/60000 (9%)] Loss: 1.58085 \nTraining set [5440/60000 (9%)] Loss: 1.53840 \nTraining set [5760/60000 (10%)] Loss: 1.49085 \nTraining set [6080/60000 (10%)] Loss: 1.51387 \nTraining set [6400/60000 (11%)] Loss: 1.49714 \nTraining set [6720/60000 (11%)] Loss: 1.59801 \nTraining set [7040/60000 (12%)] Loss: 1.52794 \nTraining set [7360/60000 (12%)] Loss: 1.57140 \nTraining set [7680/60000 (13%)] Loss: 1.50080 \nTraining set [8000/60000 (13%)] Loss: 1.53660 \nTraining set [8320/60000 (14%)] Loss: 1.56862 \nTraining set [8640/60000 (14%)] Loss: 1.49285 \nTraining set [8960/60000 (15%)] Loss: 1.58088 \nTraining set [9280/60000 (15%)] Loss: 1.55165 \nTraining set [9600/60000 (16%)] Loss: 1.56319 \nTraining set [9920/60000 (17%)] Loss: 1.50293 \nTraining set [10240/60000 (17%)] Loss: 1.56596 \nTraining set [10560/60000 (18%)] Loss: 1.65029 \nTraining set [10880/60000 (18%)] Loss: 1.54832 \nTraining set [11200/60000 (19%)] Loss: 1.57008 \nTraining set [11520/60000 (19%)] Loss: 1.55565 \nTraining set [11840/60000 (20%)] Loss: 1.55270 \nTraining set [12160/60000 (20%)] Loss: 1.57869 \nTraining set [12480/60000 (21%)] Loss: 1.58478 \nTraining set [12800/60000 (21%)] Loss: 1.56590 \nTraining set [13120/60000 (22%)] Loss: 1.54684 \nTraining set [13440/60000 (22%)] Loss: 1.56409 \nTraining set [13760/60000 (23%)] Loss: 1.58327 \nTraining set [14080/60000 (23%)] Loss: 1.54588 \nTraining set [14400/60000 (24%)] Loss: 1.60364 \nTraining set [14720/60000 (25%)] Loss: 1.54648 \nTraining set [15040/60000 (25%)] Loss: 1.52872 \nTraining set [15360/60000 (26%)] Loss: 1.56305 \nTraining set [15680/60000 (26%)] Loss: 1.55346 \nTraining set [16000/60000 (27%)] Loss: 1.51460 \nTraining set [16320/60000 (27%)] Loss: 1.58326 \nTraining set [16640/60000 (28%)] Loss: 1.58788 \nTraining set [16960/60000 (28%)] Loss: 1.54519 \nTraining set [17280/60000 (29%)] Loss: 1.62220 \nTraining set [17600/60000 (29%)] Loss: 1.49536 \nTraining set [17920/60000 (30%)] Loss: 1.51669 \nTraining set [18240/60000 (30%)] Loss: 1.58445 \nTraining set [18560/60000 (31%)] Loss: 1.53789 \nTraining set [18880/60000 (31%)] Loss: 1.51556 \nTraining set [19200/60000 (32%)] Loss: 1.53053 \nTraining set [19520/60000 (33%)] Loss: 1.56382 \nTraining set [19840/60000 (33%)] Loss: 1.49175 \nTraining set [20160/60000 (34%)] Loss: 1.54181 \nTraining set [20480/60000 (34%)] Loss: 1.49783 \nTraining set [20800/60000 (35%)] Loss: 1.50708 \nTraining set [21120/60000 (35%)] Loss: 1.55588 \nTraining set [21440/60000 (36%)] Loss: 1.52463 \nTraining set [21760/60000 (36%)] Loss: 1.50817 \nTraining set [22080/60000 (37%)] Loss: 1.51862 \nTraining set [22400/60000 (37%)] Loss: 1.54512 \nTraining set [22720/60000 (38%)] Loss: 1.56173 \nTraining set [23040/60000 (38%)] Loss: 1.52098 \nTraining set [23360/60000 (39%)] Loss: 1.50797 \nTraining set [23680/60000 (39%)] Loss: 1.55538 \nTraining set [24000/60000 (40%)] Loss: 1.55488 \nTraining set [24320/60000 (41%)] Loss: 1.57936 \nTraining set [24640/60000 (41%)] Loss: 1.53314 \nTraining set [24960/60000 (42%)] Loss: 1.49418 \nTraining set [25280/60000 (42%)] Loss: 1.53821 \nTraining set [25600/60000 (43%)] Loss: 1.52136 \nTraining set [25920/60000 (43%)] Loss: 1.55878 \nTraining set [26240/60000 (44%)] Loss: 1.53253 \nTraining set [26560/60000 (44%)] Loss: 1.53951 \nTraining set [26880/60000 (45%)] Loss: 1.55134 \nTraining set [27200/60000 (45%)] Loss: 1.54599 \nTraining set [27520/60000 (46%)] Loss: 1.54800 \nTraining set [27840/60000 (46%)] Loss: 1.51873 \nTraining set [28160/60000 (47%)] Loss: 1.57697 \nTraining set [28480/60000 (47%)] Loss: 1.53866 \nTraining set [28800/60000 (48%)] Loss: 1.57422 \nTraining set [29120/60000 (49%)] Loss: 1.55552 \nTraining set [29440/60000 (49%)] Loss: 1.58387 \nTraining set [29760/60000 (50%)] Loss: 1.59537 \nTraining set [30080/60000 (50%)] Loss: 1.54000 \nTraining set [30400/60000 (51%)] Loss: 1.49841 \nTraining set [30720/60000 (51%)] Loss: 1.55843 \nTraining set [31040/60000 (52%)] Loss: 1.57809 \nTraining set [31360/60000 (52%)] Loss: 1.57234 \nTraining set [31680/60000 (53%)] Loss: 1.57597 \nTraining set [32000/60000 (53%)] Loss: 1.53294 \nTraining set [32320/60000 (54%)] Loss: 1.57893 \nTraining set [32640/60000 (54%)] Loss: 1.52812 \nTraining set [32960/60000 (55%)] Loss: 1.50972 \nTraining set [33280/60000 (55%)] Loss: 1.56556 \nTraining set [33600/60000 (56%)] Loss: 1.53437 \nTraining set [33920/60000 (57%)] Loss: 1.58616 \nTraining set [34240/60000 (57%)] Loss: 1.54340 \nTraining set [34560/60000 (58%)] Loss: 1.50444 \nTraining set [34880/60000 (58%)] Loss: 1.52775 \nTraining set [35200/60000 (59%)] Loss: 1.58539 \nTraining set [35520/60000 (59%)] Loss: 1.62257 \nTraining set [35840/60000 (60%)] Loss: 1.59200 \nTraining set [36160/60000 (60%)] Loss: 1.50750 \nTraining set [36480/60000 (61%)] Loss: 1.50187 \nTraining set [36800/60000 (61%)] Loss: 1.56673 \nTraining set [37120/60000 (62%)] Loss: 1.55020 \nTraining set [37440/60000 (62%)] Loss: 1.56093 \nTraining set [37760/60000 (63%)] Loss: 1.53906 \nTraining set [38080/60000 (63%)] Loss: 1.52273 \nTraining set [38400/60000 (64%)] Loss: 1.58114 \nTraining set [38720/60000 (64%)] Loss: 1.52052 \nTraining set [39040/60000 (65%)] Loss: 1.61530 \nTraining set [39360/60000 (66%)] Loss: 1.54723 \nTraining set [39680/60000 (66%)] Loss: 1.56507 \nTraining set [40000/60000 (67%)] Loss: 1.54969 \nTraining set [40320/60000 (67%)] Loss: 1.56174 \nTraining set [40640/60000 (68%)] Loss: 1.56252 \nTraining set [40960/60000 (68%)] Loss: 1.55881 \nTraining set [41280/60000 (69%)] Loss: 1.51884 \nTraining set [41600/60000 (69%)] Loss: 1.57929 \nTraining set [41920/60000 (70%)] Loss: 1.55149 \nTraining set [42240/60000 (70%)] Loss: 1.54180 \nTraining set [42560/60000 (71%)] Loss: 1.49129 \nTraining set [42880/60000 (71%)] Loss: 1.53485 \nTraining set [43200/60000 (72%)] Loss: 1.54631 \nTraining set [43520/60000 (72%)] Loss: 1.53894 \nTraining set [43840/60000 (73%)] Loss: 1.52931 \nTraining set [44160/60000 (74%)] Loss: 1.48798 \nTraining set [44480/60000 (74%)] Loss: 1.55153 \nTraining set [44800/60000 (75%)] Loss: 1.56273 \nTraining set [45120/60000 (75%)] Loss: 1.59258 \nTraining set [45440/60000 (76%)] Loss: 1.56187 \nTraining set [45760/60000 (76%)] Loss: 1.61208 \nTraining set [46080/60000 (77%)] Loss: 1.54113 \nTraining set [46400/60000 (77%)] Loss: 1.52714 \nTraining set [46720/60000 (78%)] Loss: 1.53708 \nTraining set [47040/60000 (78%)] Loss: 1.53799 \nTraining set [47360/60000 (79%)] Loss: 1.52267 \nTraining set [47680/60000 (79%)] Loss: 1.53203 \nTraining set [48000/60000 (80%)] Loss: 1.52299 \nTraining set [48320/60000 (80%)] Loss: 1.58720 \nTraining set [48640/60000 (81%)] Loss: 1.57359 \nTraining set [48960/60000 (82%)] Loss: 1.50314 \nTraining set [49280/60000 (82%)] Loss: 1.51916 \nTraining set [49600/60000 (83%)] Loss: 1.51184 \nTraining set [49920/60000 (83%)] Loss: 1.50826 \nTraining set [50240/60000 (84%)] Loss: 1.54125 \nTraining set [50560/60000 (84%)] Loss: 1.56900 \nTraining set [50880/60000 (85%)] Loss: 1.53952 \nTraining set [51200/60000 (85%)] Loss: 1.54212 \nTraining set [51520/60000 (86%)] Loss: 1.56793 \nTraining set [51840/60000 (86%)] Loss: 1.53626 \nTraining set [52160/60000 (87%)] Loss: 1.57492 \nTraining set [52480/60000 (87%)] Loss: 1.53591 \nTraining set [52800/60000 (88%)] Loss: 1.54337 \nTraining set [53120/60000 (88%)] Loss: 1.58170 \nTraining set [53440/60000 (89%)] Loss: 1.55510 \nTraining set [53760/60000 (90%)] Loss: 1.49514 \nTraining set [54080/60000 (90%)] Loss: 1.55778 \nTraining set [54400/60000 (91%)] Loss: 1.52833 \nTraining set [54720/60000 (91%)] Loss: 1.51601 \nTraining set [55040/60000 (92%)] Loss: 1.51861 \nTraining set [55360/60000 (92%)] Loss: 1.60930 \nTraining set [55680/60000 (93%)] Loss: 1.59391 \nTraining set [56000/60000 (93%)] Loss: 1.56186 \nTraining set [56320/60000 (94%)] Loss: 1.54197 \nTraining set [56640/60000 (94%)] Loss: 1.50310 \nTraining set [56960/60000 (95%)] Loss: 1.52133 \nTraining set [57280/60000 (95%)] Loss: 1.50537 \nTraining set [57600/60000 (96%)] Loss: 1.54870 \nTraining set [57920/60000 (96%)] Loss: 1.55961 \nTraining set [58240/60000 (97%)] Loss: 1.55294 \nTraining set [58560/60000 (98%)] Loss: 1.53597 \nTraining set [58880/60000 (98%)] Loss: 1.58405 \nTraining set [59200/60000 (99%)] Loss: 1.52971 \nTraining set [59520/60000 (99%)] Loss: 1.51079 \nTraining set [59840/60000 (100%)] Loss: 1.51034 \nAverage Training Loss: 1.54461\nTraining Completed!\nModel Validation Starts!\nAverage Validation Loss: 1.55983, Accuracy: 9030/10000 (90%)\n\nEpoch: 25\nTraining set [0/60000 (0%)] Loss: 1.51420 \nTraining set [320/60000 (1%)] Loss: 1.56060 \nTraining set [640/60000 (1%)] Loss: 1.58434 \nTraining set [960/60000 (2%)] Loss: 1.49422 \nTraining set [1280/60000 (2%)] Loss: 1.51876 \nTraining set [1600/60000 (3%)] Loss: 1.54582 \nTraining set [1920/60000 (3%)] Loss: 1.49047 \nTraining set [2240/60000 (4%)] Loss: 1.52682 \nTraining set [2560/60000 (4%)] Loss: 1.51808 \nTraining set [2880/60000 (5%)] Loss: 1.55848 \nTraining set [3200/60000 (5%)] Loss: 1.50042 \nTraining set [3520/60000 (6%)] Loss: 1.56814 \nTraining set [3840/60000 (6%)] Loss: 1.57381 \nTraining set [4160/60000 (7%)] Loss: 1.51935 \nTraining set [4480/60000 (7%)] Loss: 1.59060 \nTraining set [4800/60000 (8%)] Loss: 1.51266 \nTraining set [5120/60000 (9%)] Loss: 1.56552 \nTraining set [5440/60000 (9%)] Loss: 1.54562 \nTraining set [5760/60000 (10%)] Loss: 1.49867 \nTraining set [6080/60000 (10%)] Loss: 1.52679 \nTraining set [6400/60000 (11%)] Loss: 1.53872 \nTraining set [6720/60000 (11%)] Loss: 1.54717 \nTraining set [7040/60000 (12%)] Loss: 1.59596 \nTraining set [7360/60000 (12%)] Loss: 1.53134 \nTraining set [7680/60000 (13%)] Loss: 1.57375 \nTraining set [8000/60000 (13%)] Loss: 1.50624 \nTraining set [8320/60000 (14%)] Loss: 1.56872 \nTraining set [8640/60000 (14%)] Loss: 1.55533 \nTraining set [8960/60000 (15%)] Loss: 1.59003 \nTraining set [9280/60000 (15%)] Loss: 1.52917 \nTraining set [9600/60000 (16%)] Loss: 1.54957 \nTraining set [9920/60000 (17%)] Loss: 1.52831 \nTraining set [10240/60000 (17%)] Loss: 1.48022 \nTraining set [10560/60000 (18%)] Loss: 1.55618 \nTraining set [10880/60000 (18%)] Loss: 1.58589 \nTraining set [11200/60000 (19%)] Loss: 1.53104 \nTraining set [11520/60000 (19%)] Loss: 1.57280 \nTraining set [11840/60000 (20%)] Loss: 1.51796 \nTraining set [12160/60000 (20%)] Loss: 1.53906 \nTraining set [12480/60000 (21%)] Loss: 1.58344 \nTraining set [12800/60000 (21%)] Loss: 1.56741 \nTraining set [13120/60000 (22%)] Loss: 1.52989 \nTraining set [13440/60000 (22%)] Loss: 1.54403 \nTraining set [13760/60000 (23%)] Loss: 1.52437 \nTraining set [14080/60000 (23%)] Loss: 1.56088 \nTraining set [14400/60000 (24%)] Loss: 1.55736 \nTraining set [14720/60000 (25%)] Loss: 1.46755 \nTraining set [15040/60000 (25%)] Loss: 1.58860 \nTraining set [15360/60000 (26%)] Loss: 1.59841 \nTraining set [15680/60000 (26%)] Loss: 1.57809 \nTraining set [16000/60000 (27%)] Loss: 1.55565 \nTraining set [16320/60000 (27%)] Loss: 1.55178 \nTraining set [16640/60000 (28%)] Loss: 1.54918 \nTraining set [16960/60000 (28%)] Loss: 1.48087 \nTraining set [17280/60000 (29%)] Loss: 1.58891 \nTraining set [17600/60000 (29%)] Loss: 1.53282 \nTraining set [17920/60000 (30%)] Loss: 1.56737 \nTraining set [18240/60000 (30%)] Loss: 1.53724 \nTraining set [18560/60000 (31%)] Loss: 1.54912 \nTraining set [18880/60000 (31%)] Loss: 1.46629 \nTraining set [19200/60000 (32%)] Loss: 1.55225 \nTraining set [19520/60000 (33%)] Loss: 1.52562 \nTraining set [19840/60000 (33%)] Loss: 1.60567 \nTraining set [20160/60000 (34%)] Loss: 1.51876 \nTraining set [20480/60000 (34%)] Loss: 1.50703 \nTraining set [20800/60000 (35%)] Loss: 1.51246 \nTraining set [21120/60000 (35%)] Loss: 1.57726 \nTraining set [21440/60000 (36%)] Loss: 1.53050 \nTraining set [21760/60000 (36%)] Loss: 1.52477 \nTraining set [22080/60000 (37%)] Loss: 1.53803 \nTraining set [22400/60000 (37%)] Loss: 1.49463 \nTraining set [22720/60000 (38%)] Loss: 1.49292 \nTraining set [23040/60000 (38%)] Loss: 1.54881 \nTraining set [23360/60000 (39%)] Loss: 1.56731 \nTraining set [23680/60000 (39%)] Loss: 1.53910 \nTraining set [24000/60000 (40%)] Loss: 1.53473 \nTraining set [24320/60000 (41%)] Loss: 1.54251 \nTraining set [24640/60000 (41%)] Loss: 1.55252 \nTraining set [24960/60000 (42%)] Loss: 1.54172 \nTraining set [25280/60000 (42%)] Loss: 1.50866 \nTraining set [25600/60000 (43%)] Loss: 1.50165 \nTraining set [25920/60000 (43%)] Loss: 1.58612 \nTraining set [26240/60000 (44%)] Loss: 1.58878 \nTraining set [26560/60000 (44%)] Loss: 1.61419 \nTraining set [26880/60000 (45%)] Loss: 1.55324 \nTraining set [27200/60000 (45%)] Loss: 1.53428 \nTraining set [27520/60000 (46%)] Loss: 1.55904 \nTraining set [27840/60000 (46%)] Loss: 1.55967 \nTraining set [28160/60000 (47%)] Loss: 1.53371 \nTraining set [28480/60000 (47%)] Loss: 1.55480 \nTraining set [28800/60000 (48%)] Loss: 1.58891 \nTraining set [29120/60000 (49%)] Loss: 1.51601 \nTraining set [29440/60000 (49%)] Loss: 1.57293 \nTraining set [29760/60000 (50%)] Loss: 1.51413 \nTraining set [30080/60000 (50%)] Loss: 1.50876 \nTraining set [30400/60000 (51%)] Loss: 1.48131 \nTraining set [30720/60000 (51%)] Loss: 1.55093 \nTraining set [31040/60000 (52%)] Loss: 1.56346 \nTraining set [31360/60000 (52%)] Loss: 1.62198 \nTraining set [31680/60000 (53%)] Loss: 1.57945 \nTraining set [32000/60000 (53%)] Loss: 1.57316 \nTraining set [32320/60000 (54%)] Loss: 1.53918 \nTraining set [32640/60000 (54%)] Loss: 1.56655 \nTraining set [32960/60000 (55%)] Loss: 1.59756 \nTraining set [33280/60000 (55%)] Loss: 1.58320 \nTraining set [33600/60000 (56%)] Loss: 1.49281 \nTraining set [33920/60000 (57%)] Loss: 1.49201 \nTraining set [34240/60000 (57%)] Loss: 1.53059 \nTraining set [34560/60000 (58%)] Loss: 1.54203 \nTraining set [34880/60000 (58%)] Loss: 1.59270 \nTraining set [35200/60000 (59%)] Loss: 1.53445 \nTraining set [35520/60000 (59%)] Loss: 1.56911 \nTraining set [35840/60000 (60%)] Loss: 1.52310 \nTraining set [36160/60000 (60%)] Loss: 1.56026 \nTraining set [36480/60000 (61%)] Loss: 1.54599 \nTraining set [36800/60000 (61%)] Loss: 1.58208 \nTraining set [37120/60000 (62%)] Loss: 1.56074 \nTraining set [37440/60000 (62%)] Loss: 1.60069 \nTraining set [37760/60000 (63%)] Loss: 1.54637 \nTraining set [38080/60000 (63%)] Loss: 1.56418 \nTraining set [38400/60000 (64%)] Loss: 1.59626 \nTraining set [38720/60000 (64%)] Loss: 1.51961 \nTraining set [39040/60000 (65%)] Loss: 1.56828 \nTraining set [39360/60000 (66%)] Loss: 1.57416 \nTraining set [39680/60000 (66%)] Loss: 1.58259 \nTraining set [40000/60000 (67%)] Loss: 1.51830 \nTraining set [40320/60000 (67%)] Loss: 1.58706 \nTraining set [40640/60000 (68%)] Loss: 1.52278 \nTraining set [40960/60000 (68%)] Loss: 1.52063 \nTraining set [41280/60000 (69%)] Loss: 1.53516 \nTraining set [41600/60000 (69%)] Loss: 1.59229 \nTraining set [41920/60000 (70%)] Loss: 1.55246 \nTraining set [42240/60000 (70%)] Loss: 1.55349 \nTraining set [42560/60000 (71%)] Loss: 1.51167 \nTraining set [42880/60000 (71%)] Loss: 1.56982 \nTraining set [43200/60000 (72%)] Loss: 1.56951 \nTraining set [43520/60000 (72%)] Loss: 1.56014 \nTraining set [43840/60000 (73%)] Loss: 1.53060 \nTraining set [44160/60000 (74%)] Loss: 1.55212 \nTraining set [44480/60000 (74%)] Loss: 1.50881 \nTraining set [44800/60000 (75%)] Loss: 1.53234 \nTraining set [45120/60000 (75%)] Loss: 1.55108 \nTraining set [45440/60000 (76%)] Loss: 1.56435 \nTraining set [45760/60000 (76%)] Loss: 1.59901 \nTraining set [46080/60000 (77%)] Loss: 1.58748 \nTraining set [46400/60000 (77%)] Loss: 1.54876 \nTraining set [46720/60000 (78%)] Loss: 1.55592 \nTraining set [47040/60000 (78%)] Loss: 1.58960 \nTraining set [47360/60000 (79%)] Loss: 1.53263 \nTraining set [47680/60000 (79%)] Loss: 1.54489 \nTraining set [48000/60000 (80%)] Loss: 1.53054 \nTraining set [48320/60000 (80%)] Loss: 1.58786 \nTraining set [48640/60000 (81%)] Loss: 1.54962 \nTraining set [48960/60000 (82%)] Loss: 1.53770 \nTraining set [49280/60000 (82%)] Loss: 1.52941 \nTraining set [49600/60000 (83%)] Loss: 1.53359 \nTraining set [49920/60000 (83%)] Loss: 1.57155 \nTraining set [50240/60000 (84%)] Loss: 1.52014 \nTraining set [50560/60000 (84%)] Loss: 1.53626 \nTraining set [50880/60000 (85%)] Loss: 1.54713 \nTraining set [51200/60000 (85%)] Loss: 1.53088 \nTraining set [51520/60000 (86%)] Loss: 1.48766 \nTraining set [51840/60000 (86%)] Loss: 1.55082 \nTraining set [52160/60000 (87%)] Loss: 1.56069 \nTraining set [52480/60000 (87%)] Loss: 1.51458 \nTraining set [52800/60000 (88%)] Loss: 1.54712 \nTraining set [53120/60000 (88%)] Loss: 1.48994 \nTraining set [53440/60000 (89%)] Loss: 1.55859 \nTraining set [53760/60000 (90%)] Loss: 1.53883 \nTraining set [54080/60000 (90%)] Loss: 1.59334 \nTraining set [54400/60000 (91%)] Loss: 1.49309 \nTraining set [54720/60000 (91%)] Loss: 1.53193 \nTraining set [55040/60000 (92%)] Loss: 1.57703 \nTraining set [55360/60000 (92%)] Loss: 1.56411 \nTraining set [55680/60000 (93%)] Loss: 1.53092 \nTraining set [56000/60000 (93%)] Loss: 1.51440 \nTraining set [56320/60000 (94%)] Loss: 1.52538 \nTraining set [56640/60000 (94%)] Loss: 1.59195 \nTraining set [56960/60000 (95%)] Loss: 1.53950 \nTraining set [57280/60000 (95%)] Loss: 1.55213 \nTraining set [57600/60000 (96%)] Loss: 1.54206 \nTraining set [57920/60000 (96%)] Loss: 1.53730 \nTraining set [58240/60000 (97%)] Loss: 1.54197 \nTraining set [58560/60000 (98%)] Loss: 1.53611 \nTraining set [58880/60000 (98%)] Loss: 1.54922 \nTraining set [59200/60000 (99%)] Loss: 1.53366 \nTraining set [59520/60000 (99%)] Loss: 1.56225 \nTraining set [59840/60000 (100%)] Loss: 1.53852 \nAverage Training Loss: 1.54363\nTraining Completed!\nModel Validation Starts!\nAverage Validation Loss: 1.57274, Accuracy: 8902/10000 (89%)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Training for more epochs can lead to higher accuracy. I stopped at 20 epochs for learning and implementation.**","metadata":{}},{"cell_type":"code","source":"# Plot the training and validation loss over epochs\nplt.plot(epoch_nums, training_loss, label=\"Training\")  # Plot training loss\nplt.plot(epoch_nums, validation_loss, label=\"Validation\")  # Plot validation loss\n\n# Label the x-axis as 'Epoch'\nplt.xlabel(\"Epoch\")\n\n# Label the y-axis as 'Loss'\nplt.ylabel(\"Loss\")\n\n# Add a legend to differentiate between training and validation loss\nplt.legend(loc=\"upper right\")\n\n# Set the title of the plot\nplt.title(\"Learning Curve\")","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:17:56.342957Z","iopub.execute_input":"2024-08-26T23:17:56.343619Z","iopub.status.idle":"2024-08-26T23:17:56.684644Z","shell.execute_reply.started":"2024-08-26T23:17:56.343580Z","shell.execute_reply":"2024-08-26T23:17:56.683780Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"Text(0.5, 1.0, 'Learning Curve')"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]}]}